{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Indoor Navigation System (NLOS) : Trajectory.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AIPvjm5Cf1Em",
        "I2y7Elsws4St",
        "mR1aksXnEqYm",
        "UCea7ZMKGABp",
        "FOldJYL7LGf7",
        "mB3x1O6akdim",
        "aWin3YbZ1Hry",
        "V12y5iSsILjs",
        "jKxfUHJ7ITpY"
      ],
      "authorship_tag": "ABX9TyPKOU7SxXc1YRDYJplRrXrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipayandas97/Indoor-Navigation-System/blob/master/Indoor_Navigation_System_(NLOS)_Trajectory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SntdNgusSTP",
        "colab_type": "code",
        "outputId": "ad20d55c-538e-4272-8541-5b964f2573d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive\n",
        "import seaborn as sns\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIPvjm5Cf1Em",
        "colab_type": "text"
      },
      "source": [
        "##Topology / Graph\n",
        "- node_to_coordinates[]\n",
        "- coordinates_to_node()\n",
        "- nodes_connected_to[]\n",
        "- adj_matrix[]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbHcOkNlyPe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dictionaries\n",
        "\n",
        "node_to_coordinates = {\n",
        "    'C01':[5,0],\n",
        "    'C02':[5,1],\n",
        "    'C03':[5,2],\n",
        "    'C04':[5,3],\n",
        "    'C05':[5,4],\n",
        "    'C06':[5,5],\n",
        "    'C07':[5,6],\n",
        "    'C08':[5,7],\n",
        "    'C09':[5,8],\n",
        "    'C10':[5,9],\n",
        "    'C11':[5,10],\n",
        "    'C12':[5,11],\n",
        "    'C13':[5,12],\n",
        "    'C14':[5,13],\n",
        "    'C15':[5,14],\n",
        "    'C16':[5,15],\n",
        "    'C17':[5,16],\n",
        "    'C18':[5,17],\n",
        "    'C19':[5,18],\n",
        "    'C20':[5,19],\n",
        "    'C21':[5,20],\n",
        "    'C22':[5,21],\n",
        "    'C23':[5,22],\n",
        "\n",
        "    'R11':[1,1],\n",
        "    'R12':[2,1],\n",
        "    'R13':[3,1],\n",
        "    'R14':[1,2],\n",
        "    'R15':[2,2],\n",
        "    'R16':[3,2],\n",
        "    'R17':[4,2],\n",
        "\n",
        "    'R21':[1,7],\n",
        "    'R22':[2,7],\n",
        "    'R23':[3,7],\n",
        "    'R24':[1,8],\n",
        "    'R25':[2,8],\n",
        "    'R26':[3,8],\n",
        "    'R27':[4,7],\n",
        "\n",
        "    'R31':[1,15],\n",
        "    'R32':[2,15],\n",
        "    'R33':[3,15],\n",
        "    'R34':[1,16],\n",
        "    'R35':[2,16],\n",
        "    'R36':[3,16],\n",
        "    'R37':[4,15],\n",
        "\n",
        "    'R41':[1,20],\n",
        "    'R42':[2,20],\n",
        "    'R43':[3,20],\n",
        "    'R44':[1,21],\n",
        "    'R45':[2,21],\n",
        "    'R46':[3,21],\n",
        "    'R47':[4,20]                               \n",
        "}\n",
        "\n",
        "def coordinates_to_node(c):\n",
        "    x, y = c[0], c[1]       #c can be nd.array or list\n",
        "    for node, pos in node_to_coordinates.items():\n",
        "        if x == pos[0] and y == pos[1]:\n",
        "            return node\n",
        "    raise Exception('Invalid coordinates!')\n",
        "\n",
        "nodes_connected_to = {\n",
        "    'C01': ['C01', 'C02'],\n",
        "    'C02': ['C02', 'C01', 'C03'],\n",
        "    'C03': ['C03', 'C02', 'C04', 'R17'],\n",
        "    'C04': ['C04', 'C03', 'C05'],\n",
        "    'C05': ['C05', 'C04', 'C06'],\n",
        "    'C06': ['C06', 'C05', 'C07'],\n",
        "    'C07': ['C07', 'C06', 'C08'],\n",
        "    'C08': ['C08', 'C07', 'C09', 'R27'],\n",
        "    'C09': ['C09', 'C08', 'C10'],\n",
        "    'C10': ['C10', 'C09', 'C11'],\n",
        "    'C11': ['C11', 'C10', 'C12'],\n",
        "    'C12': ['C12', 'C11', 'C13'],\n",
        "    'C13': ['C13', 'C12', 'C14'],\n",
        "    'C14': ['C14', 'C13', 'C15'],\n",
        "    'C15': ['C15', 'C14', 'C16'],\n",
        "    'C16': ['C16', 'C15', 'C17', 'R37'],\n",
        "    'C17': ['C17', 'C16', 'C18'],\n",
        "    'C18': ['C18', 'C17', 'C19'],\n",
        "    'C19': ['C19', 'C18', 'C20'],\n",
        "    'C20': ['C20', 'C19', 'C21'],\n",
        "    'C21': ['C21', 'C20', 'C22', 'R47'],\n",
        "    'C22': ['C22', 'C21', 'C23'],\n",
        "    'C23': ['C23', 'C22'],\n",
        "\n",
        "    'R11': ['R11', 'R12', 'R15', 'R14'],\n",
        "    'R12': ['R12', 'R11', 'R15', 'R13', 'R14', 'R16'],\n",
        "    'R13': ['R13', 'R12', 'R16', 'R15', 'R17'],\n",
        "    'R14': ['R14', 'R11', 'R12', 'R15'],\n",
        "    'R15': ['R12', 'R11', 'R15', 'R13', 'R14', 'R16'],\n",
        "    'R16': ['R16', 'R15', 'R12', 'R13', 'R17'],\n",
        "    'R17': ['R17', 'R16', 'R13', 'C03'],\n",
        "\n",
        "    'R21': ['R21', 'R22', 'R25', 'R24'],\n",
        "    'R22': ['R22', 'R21', 'R25', 'R23', 'R24', 'R26'],\n",
        "    'R23': ['R23', 'R22', 'R26', 'R25', 'R27'],\n",
        "    'R24': ['R24', 'R21', 'R22', 'R25'],\n",
        "    'R25': ['R22', 'R21', 'R25', 'R23', 'R24', 'R26'],\n",
        "    'R26': ['R26', 'R25', 'R22', 'R23', 'R27'],\n",
        "    'R27': ['R27', 'R26', 'R23', 'C08'],\n",
        "\n",
        "    'R31': ['R31', 'R32', 'R35', 'R34'],\n",
        "    'R32': ['R32', 'R31', 'R35', 'R33', 'R34', 'R36'],\n",
        "    'R33': ['R33', 'R32', 'R36', 'R35', 'R37'],\n",
        "    'R34': ['R34', 'R31', 'R32', 'R35'],\n",
        "    'R35': ['R32', 'R31', 'R35', 'R33', 'R34', 'R36'],\n",
        "    'R36': ['R36', 'R35', 'R32', 'R33', 'R37'],\n",
        "    'R37': ['R37', 'R36', 'R33', 'C16'],\n",
        "\n",
        "    'R41': ['R41', 'R42', 'R45', 'R44'],\n",
        "    'R42': ['R42', 'R41', 'R45', 'R43', 'R44', 'R46'],\n",
        "    'R43': ['R43', 'R42', 'R46', 'R45', 'R47'],\n",
        "    'R44': ['R44', 'R41', 'R42', 'R45'],\n",
        "    'R45': ['R42', 'R41', 'R45', 'R43', 'R44', 'R46'],\n",
        "    'R46': ['R46', 'R45', 'R42', 'R43', 'R47'],\n",
        "    'R47': ['R47', 'R46', 'R43', 'C21'],\n",
        "}\n",
        "\n",
        "nodes = list(node_to_coordinates.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAcPndr4Bedc",
        "colab_type": "code",
        "outputId": "5c819696-1791-4886-ade5-debd6b27f6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "#create adjacency matrix\n",
        "num_of_nodes = len(nodes)\n",
        "adj_matrix = np.zeros((num_of_nodes, num_of_nodes))\n",
        "\n",
        "for x, node_x in enumerate(nodes):\n",
        "    for y, node_y in enumerate(nodes):\n",
        "        connected_nodes = nodes_connected_to[node_x]\n",
        "        if node_y in connected_nodes:\n",
        "            adj_matrix[x][y] = 1\n",
        "\n",
        "print(nodes)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(adj_matrix, cmap='gray') \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['C01', 'C02', 'C03', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'R11', 'R12', 'R13', 'R14', 'R15', 'R16', 'R17', 'R21', 'R22', 'R23', 'R24', 'R25', 'R26', 'R27', 'R31', 'R32', 'R33', 'R34', 'R35', 'R36', 'R37', 'R41', 'R42', 'R43', 'R44', 'R45', 'R46', 'R47']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_C5HuNFAQDU",
        "colab_type": "code",
        "outputId": "d567c797-abf7-4818-f03b-62729ee35a9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "plt.figure(figsize=(12,3))\n",
        "for node in list(node_to_coordinates.keys()):\n",
        "    plt.scatter(node_to_coordinates[node][1], 5-node_to_coordinates[node][0], c='b')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2y7Elsws4St",
        "colab_type": "text"
      },
      "source": [
        "#Data Extraction and Heat_maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aI96JzIs2dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = '/content/drive/My Drive/Final_year_project/NLOS_data/'\n",
        "\n",
        "\n",
        "os.chdir(filepath+'AP_1/')\n",
        "AP_1_file_names = os.listdir()\n",
        "\n",
        "os.chdir(filepath+'AP_2/')\n",
        "AP_2_file_names = os.listdir()\n",
        "\n",
        "os.chdir(filepath+'AP_3/')\n",
        "AP_3_file_names = os.listdir()\n",
        "\n",
        "os.chdir(filepath+'AP_4/')\n",
        "AP_4_file_names = os.listdir()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuWUmNcK2ZRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_normal_noise(loc, scale, size):\n",
        "    randomNums = np.random.normal(loc=loc, scale=scale, size=size)\n",
        "    return np.round(randomNums).astype(np.int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHt4a6CFyJ-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_data(noise_std):\n",
        "    nodes = list(node_to_coordinates.keys())\n",
        "    data_dict = {}\n",
        "    n_data_dict = {}\n",
        "\n",
        "    for i,node in enumerate(nodes):\n",
        "        f1 = pd.read_csv(filepath+'AP_1/'+node+'.txt', header=None).to_numpy()\n",
        "        n_f1 = f1 + get_normal_noise(loc=0, scale=noise_std, size=f1.shape)\n",
        "        \n",
        "        f2 = pd.read_csv(filepath+'AP_2/'+node+'.txt', header=None).to_numpy()\n",
        "        n_f2 = f2 + get_normal_noise(loc=0, scale=noise_std, size=f2.shape)\n",
        "        \n",
        "        f3 = pd.read_csv(filepath+'AP_3/'+node+'.txt', header=None).to_numpy()\n",
        "        n_f3 = f3 + get_normal_noise(loc=0, scale=noise_std, size=f3.shape)    \n",
        "        \n",
        "        f4 = pd.read_csv(filepath+'AP_4/'+node+'.txt', header=None).to_numpy()\n",
        "        n_f4 = f4 + get_normal_noise(loc=0, scale=noise_std, size=f4.shape)    \n",
        "\n",
        "        data_file = np.hstack((f1,f2,f3,f4))\n",
        "        data_dict[node] = data_file\n",
        "\n",
        "        n_data_file = np.hstack((n_f1,n_f2,n_f3, n_f4))\n",
        "        n_data_dict[node] = n_data_file\n",
        "    return data_dict, n_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xyClwBvFKNM",
        "colab_type": "code",
        "outputId": "a05831d3-6b0b-4af0-97ea-4851573bef26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        }
      },
      "source": [
        "#Heatmaps for each AP\n",
        "scale = 2\n",
        "for ap in range(4):\n",
        "    heat_map = np.zeros((7*scale,25*scale))\n",
        "    for n in nodes:\n",
        "        pos = node_to_coordinates[n]\n",
        "        avg_power = np.mean(data_dict[n][:,ap])\n",
        "        heat_map[pos[0]*scale,(pos[1]+1)*scale] = 110+avg_power #offset\n",
        "    print('Access Point: ',ap+1)\n",
        "    plt.figure(figsize=(7*scale*0.65,3*scale*0.65))\n",
        "    plt.imshow(heat_map,cmap='gray')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Access Point:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAACtCAYAAAAUNc27AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANdklEQVR4nO3df4wdVRnG8eexhSBa5YemwRYtBoJpGllgY5aWqIGa1FpaSQihsYQKYQlRRKLZtEpKaDBgNYJBgyxYS4AUGwQpTVXKjwShtrpbFi1UpSJKsbQI/qiaiNXXP3YS1+22nNyZPXN37veTNHvn7NuZNzmwfXbmzIwjQgAAAOPtTXU3AAAAOgOhAwAAZEHoAAAAWRA6AABAFoQOAACQBaEDAABkMTnnwWxzfy4AAM33x4h45+jBUmc6bM+z/SvbO20vK7MvAADQGL8ba7Dl0GF7kqRvSvqopJmSFtue2er+AABAs5U50/EBSTsj4vmIeF3SPZIWVdMWAABomjKhY5qkF0ds7yrGAAAADjDuC0lt90rqHe/jAACA9lYmdLwk6fgR29OLsf8TEf2S+iXuXgEAoJOVubzyM0kn2T7B9uGSLpC0vpq2AABA07R8piMi9tv+tKQfSZokaXVEPFNZZwAAoFEcke+KB5dXAADoCIMR0T16kMegAwCALAgdAAAgC0IHAADIgtABAACyIHQAAIAsCB0AACALQgcAAMiC0AEAALIgdAAAgCzG/S2zE82sWbOS6rZv355U19XVlVQ3NDSUVIdqdHcf8KC8MQ0MDCTVnXbaaUl127ZtS6pDZ1i5cmVS3YoVK5Lqbr755uRjX3HFFcm1nWbjxo1JdfPnz0+q27lzZ1LdiSeemFQ3kXGmAwAAZEHoAAAAWbQcOmwfb/sx28/afsb2lVU2BgAAmqXMmo79kj4XEdtsT5E0aHtTRDxbUW8AAKBBWj7TERG7I2Jb8XmfpB2SplXVGAAAaJZK7l6xPUPSqZK2jvG9Xkm9VRwHAABMXKVDh+23SvqepM9GxF9Hfz8i+iX1F7VR9ngAAGBiKnX3iu3DNBw47o6I+6ppCQAANFGZu1cs6duSdkTE16prCQAANFGZMx1zJF0o6SzbQ8WftMezAQCAjuOIfMssWNMBAEBHGIyIA943wRNJAQBAFoQOAACQBaEDAABkQegAAABZEDoAAEAWhA4AAJAFoQMAAGRB6AAAAFkQOgAAQBaVvNq+SU455ZSkuqeffjqprqurK6luaGgoqQ7V6OnpSarbsmVLUt3s2bOT6jZv3pxUh86wcuXKpLoVK1Yk1d1yyy3Jx7788suTazvNnj17kuqmTp2aVDc4OJhUd/rppyfVTWSc6QAAAFmUDh22J9l+yvaGKhoCAADNVMWZjisl7ahgPwAAoMFKhQ7b0yV9TNLt1bQDAACaquyZjpsk9Un6z8EKbPfaHrA9UPJYAABgAms5dNheIGlvRBxyWW5E9EdEd0R0t3osAAAw8ZU50zFH0kLbL0i6R9JZtu+qpCsAANA4LYeOiFgeEdMjYoakCyQ9GhFLKusMAAA0Cs/pAAAAWTgi8h3MzncwAABQl8Gx1nJypgMAAGRB6AAAAFkQOgAAQBaEDgAAkAWhAwAAZEHoAAAAWRA6AABAFoQOAACQBaEDAABkMbnuBtpNV1dXUt3Q0FAt+0M1enp6kuq2bNmSVHfmmWcm1T3xxBNJdegMfX19SXWrVq1KqrvhhhuSj71s2bLk2k6zb9++pLopU6Yk1aU++dt2Ut1ExpkOAACQRanQYfso2/fa/qXtHbbPqKoxAADQLGUvr3xd0g8j4jzbh0s6soKeAABAA7UcOmy/XdIHJS2VpIh4XdLr1bQFAACapszllRMkvSLpO7afsn277bdU1BcAAGiYMqFjsqTTJN0SEadK+rukA5ZD2+61PWB7oMSxAADABFcmdOyStCsithbb92o4hPyfiOiPiO6I6C5xLAAAMMG1HDoi4mVJL9o+uRg6W9KzlXQFAAAap+zdK1dIuru4c+V5SZ8s3xIAAGgipz4prZKD2fkOBgAA6jI41rIKnkgKAACyIHQAAIAsCB0AACALQgcAAMiC0AEAALIgdAAAgCwIHQAAIAtCBwAAyILQAQAAsiB0AACALMq+ewVvYM6cOUl1Tz755Dh3gpFmz56dVLd58+Zx7gSd7Prrr0+qW758+Th3gpFeeOGFpLoZM2aMax9NxJkOAACQRanQYfsq28/Y3m57re0jqmoMAAA0S8uhw/Y0SZ+R1B0RsyRNknRBVY0BAIBmKXt5ZbKkN9ueLOlISX8o3xIAAGiilkNHRLwk6auSfi9pt6S/RMRDo+ts99oesD3QepsAAGCiK3N55WhJiySdIOldkt5ie8nouojoj4juiOhuvU0AADDRlbm8MlfSbyPilYj4l6T7JKXdhwgAADpOmdDxe0k9to+0bUlnS9pRTVsAAKBpyqzp2CrpXknbJP2i2Fd/RX0BAICGcUTkO5id72AAAKAug2Ot5eSJpAAAIAtCBwAAyILQAQAAsiB0AACALAgdAAAgC0IHAADIgtABAACyIHQAAIAsCB0AACCLyXU3UMaFF16YVHfnnXcm73PBggVJdRs2bEiqO/fcc5Pq7r///qS6c845J6nuwQcfTKqTpCVLDng58JjuuuuupLrzzz8/qW7dunW17E+Sli5dmlS3Zs2apLqFCxcm1a1fvz6prur/bi699NKkOkm67bbbKt1n6v6uuuqqpLobb7wxqW7x4sVJdZK0du3apLrLLrssqe7WW2+tZX9XX311Up0kXXfddUl1q1atSqrr6+tLqrvmmmuS6q699tqkuo0bNybVSdL8+fOT6h5++OGkurlz5ybVvfrqq0l1xx57bFLd6tWrk+ouvvjipLqcONMBAACyeMPQYXu17b22t48YO8b2JtvPFV+PHt82AQDARJdypmONpHmjxpZJeiQiTpL0SLENAABwUG8YOiLicUmvjRpeJOmO4vMdkj5ecV8AAKBhWl1IOjUidhefX5Y09WCFtnsl9bZ4HAAA0BCl716JiLAdh/h+v6R+STpUHQAAaLZW717ZY/s4SSq+7q2uJQAA0EStho71ki4qPl8k6YFq2gEAAE2VcsvsWkk/kXSy7V22L5F0g6SP2H5O0txiGwAA4KAckW+ZBWs6AADoCIMR0T16kCeSAgCALAgdAAAgC0IHAADIgtABAACyIHQAAIAsCB0AACALQgcAAMiC0AEAALIgdAAAgCwIHQAAIAtCBwAAyCLlhW+rbe+1vX3E2Fds/9L2z23fb/uo8W0TAABMdClnOtZImjdqbJOkWRHxfkm/lrS84r4AAEDDvGHoiIjHJb02auyhiNhfbG6RNH0cegMAAA1SxZqOiyX9oIL9AACABptc5i/b/qKk/ZLuPkRNr6TeMscBAAATX8uhw/ZSSQsknR0RcbC6iOiX1F/8nYPWAQCAZmspdNieJ6lP0oci4h/VtgQAAJoo5ZbZtZJ+Iulk27tsXyLpG5KmSNpke8j2t8a5TwAAMMH5EFdGqj8Yl1cAAOgEgxHRPXqQJ5ICAIAsCB0AACALQgcAAMiC0AEAALIgdAAAgCwIHQAAIAtCBwAAyILQAQAAsiB0AACALAgdAAAgi1Kvtm/BHyX9btTYO4pxtBfmpf0wJ+2JeWk/zEn93jPWYNZ3r4zZgD0w1vPZUS/mpf0wJ+2JeWk/zEn74vIKAADIgtABAACyaIfQ0V93AxgT89J+mJP2xLy0H+akTdW+pgMAAHSGdjjTAQAAOkCtocP2PNu/sr3T9rI6e+lUtlfb3mt7+4ixY2xvsv1c8fXoOnvsRLaPt/2Y7WdtP2P7ymKcuamJ7SNs/9T208WcXFuMn2B7a/Fz7Lu2D6+7105je5Ltp2xvKLaZkzZVW+iwPUnSNyV9VNJMSYttz6yrnw62RtK8UWPLJD0SESdJeqTYRl77JX0uImZK6pH0qeL/D+amPv+UdFZEnCKpS9I82z2Svizpxog4UdKfJF1SY4+d6kpJO0ZsMydtqs4zHR+QtDMino+I1yXdI2lRjf10pIh4XNJro4YXSbqj+HyHpI9nbQqKiN0Rsa34vE/DP1CnibmpTQz7W7F5WPEnJJ0l6d5inDnJzPZ0SR+TdHuxbTEnbavO0DFN0osjtncVY6jf1IjYXXx+WdLUOpvpdLZnSDpV0lYxN7UqTuMPSdoraZOk30j6c0TsL0r4OZbfTZL6JP2n2D5WzEnbYiEpDimGb2/iFqea2H6rpO9J+mxE/HXk95ib/CLi3xHRJWm6hs/Wvq/mljqa7QWS9kbEYN29IE3ud6+M9JKk40dsTy/GUL89to+LiN22j9Pwb3XIzPZhGg4cd0fEfcUwc9MGIuLPth+TdIako2xPLn6z5udYXnMkLbQ9X9IRkt4m6etiTtpWnWc6fibppGKV8eGSLpC0vsZ+8D/rJV1UfL5I0gM19tKRiuvS35a0IyK+NuJbzE1NbL/T9lHF5zdL+oiG19o8Jum8oow5ySgilkfE9IiYoeF/Qx6NiE+IOWlbtT4crEinN0maJGl1RHyptmY6lO21kj6s4bcy7pF0jaTvS1on6d0afivw+RExerEpxpHtMyX9WNIv9L9r1V/Q8LoO5qYGtt+v4UWJkzT8C9u6iFhp+70aXgh/jKSnJC2JiH/W12lnsv1hSZ+PiAXMSfviiaQAACALFpICAIAsCB0AACALQgcAAMiC0AEAALIgdAAAgCwIHQAAIAtCBwAAyILQAQAAsvgvZQhV53gzjQIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Access Point:  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAACtCAYAAAAUNc27AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANpElEQVR4nO3dfYxddZ3H8c9nWwii7kLXprAtKywLbBqjZTMx40KsASVVwUoipIUCYmEa4i64FNqy0hAsJpSHikTDdmjLU0u7BWEFk91tg6RI4lRnoCq0ytOuUrZPpuv6lIjVr3/MSRgv0/LznjO/c+fc9ytpes+ZT+d8k18YPnMe7nVECAAAYKz9Wd0DAACA7kDpAAAAWVA6AABAFpQOAACQBaUDAABkQekAAABZTMx5MNs8nwsAQPP9NCImt+4sdabD9izbP7L9ku0lZb4XAABojB+PtrPt0mF7gqSvSvqopOmS5tqe3u73AwAAzVbmTMf7Jb0UEa9ExOuSNkiaXc1YAACgacqUjqmSXh2xvbPYBwAA8CZjfiOp7T5JfWN9HAAA0NnKlI7XJB03Yntase+PRES/pH6Jp1cAAOhmZS6vfFfSSbZPsH24pDmSHqtmLAAA0DRtn+mIiAO2/1HSf0maIGlNRDxf2WQAAKBRHJHvigeXVwAA6ApDEdHTupO3QQcAAFlQOgAAQBaUDgAAkAWlAwAAZEHpAAAAWVA6AABAFpQOAACQBaUDAABkQekAAABZjPmnzI43Z511VlJu06ZNSbne3t6k3MDAQFIO1ZgzZ05SbsOGDUm5c889Nyn36KOPJuXQHfbv35+UmzRpUlJuz549yceeMmVKcrbb3HzzzUm5JUuWJOWWLVuWlFu6dGlSbjzjTAcAAMiC0gEAALJou3TYPs72k7a3237e9lVVDgYAAJqlzD0dByQtjIhnbL9T0pDtzRGxvaLZAABAg7R9piMidkXEM8XrX0jaIWlqVYMBAIBmqeTpFdvHSzpV0tZRvtYnqa+K4wAAgPGrdOmw/Q5JX5P0uYj4eevXI6JfUn+RjbLHAwAA41Opp1dsH6bhwrEuIh6pZiQAANBEZZ5esaTVknZExIrqRgIAAE1U5kzHaZIuknSG7W3Fn49VNBcAAGgYR+S7zYJ7OgAA6ApDEdHTupN3JAUAAFlQOgAAQBaUDgAAkAWlAwAAZEHpAAAAWVA6AABAFpQOAACQBaUDAABkQekAAABZVPLR9k3S0/OmN1Ab1eDgYFKut7c3KTcwMJCUQzUuuuiipNwDDzyQlLvsssuScqtWrUrKoTvs27cvKTd58uSk3AsvvJB87JNPPjk5222WL1+elFu8eHFS7sEHH0zKXXDBBUm58YwzHQAAIIvSpcP2BNvP2v5GFQMBAIBmquJMx1WSdlTwfQAAQIOVKh22p0n6uCQuVAMAgEMqe6bjDkmLJP3+YAHbfbYHbafdeQkAABqp7dJh+2xJeyNi6FC5iOiPiJ6ISHssBAAANFKZMx2nSfqE7f+RtEHSGbbXVjIVAABonLZLR0RcFxHTIuJ4SXMkfTMi5lU2GQAAaBTepwMAAGThiMh3MDvfwQAAQF2GRruXkzMdAAAgC0oHAADIgtIBAACyoHQAAIAsKB0AACALSgcAAMiC0gEAALKgdAAAgCwoHQAAIIuJdQ/QaU4//fSk3NNPP52U6+3tTcoNDAwk5VCNuXPnJuXWr1+flJs3L+1jh9au5TMR8YZt27Yl5WbMmJGU+1PeYdp2crbbrFy5Mim3YMGCpNy6deuSchdeeGFSbjzjTAcAAMiiVOmwfZTth23/0PYO2x+oajAAANAsZS+vfFnSf0bEp2wfLunICmYCAAAN1HbpsP0Xkj4o6dOSFBGvS3q9mrEAAEDTlLm8coKkfZLusf2s7VW2317RXAAAoGHKlI6Jkv5e0l0RcaqkX0la0hqy3Wd70PZgiWMBAIBxrkzp2ClpZ0RsLbYf1nAJ+SMR0R8RPRHRU+JYAABgnGu7dETEbkmv2j6l2HWmpO2VTAUAABqn7NMr/yRpXfHkyiuSLi0/EgAAaCL/Ke9gV/pgdr6DAQCAugyNdlsF70gKAACyoHQAAIAsKB0AACALSgcAAMiC0gEAALKgdAAAgCwoHQAAIAtKBwAAyILSAQAAsqB0AACALMp+9grewsyZM5NyW7ZsGeNJMNLFF1+clLv//vvHeBJ0s4ceeigpd955543xJBhp2bJlSbmlS5eO8STNw5kOAACQRanSYfufbT9v+znb620fUdVgAACgWdouHbanSrpSUk9EvEfSBElzqhoMAAA0S9nLKxMlvc32RElHSvrf8iMBAIAmart0RMRrkm6T9BNJuyT9f0Rsas3Z7rM9aHuw/TEBAMB4V+byytGSZks6QdJfSXq77XmtuYjoj4ieiOhpf0wAADDelbm88mFJ/x0R+yLit5IekfQP1YwFAACapkzp+ImkXttH2rakMyXtqGYsAADQNGXu6dgq6WFJz0j6QfG9+iuaCwAANIwjIt/B7HwHAwAAdRka7V5O3pEUAABkQekAAABZUDoAAEAWlA4AAJAFpQMAAGRB6QAAAFlQOgAAQBaUDgAAkAWlAwAAZDGx7gHKmDlzZlJuy5Ytyd/ziiuuSMrdddddSbkFCxYk5VauXFnL95OkK6+8Mil35513JuUuv/zypNzdd9+dlLv++uuTcjfddFNSTpJuuOGGpNyNN96YlJs3700fsDyqtWvXJuWuvfbapNytt96alFuxYkVSTpKuvvrqpNztt9+elFu4cGFSbvny5Um5xYsXJ+VWr16dlJOk+fPnJ+U2btyYlDv//POTctu3b0/KTZ8+PSm3e/fupJwkHXPMMUm5l19+OSl34oknJuUef/zxpNw555yTlLvllluScpK0aNGipNw999yTlLv00kuTcv39aZ8Q0tfXl5S75pprknK33XZbUi4nznQAAIAs3rJ02F5je6/t50bsm2R7s+0Xi7+PHtsxAQDAeJdypuNeSbNa9i2R9EREnCTpiWIbAADgoN6ydETEU5L2t+yeLem+4vV9kj5Z8VwAAKBh2r2RdEpE7Cpe75Y05WBB232S0u6OAQAAjVX66ZWICNtxiK/3S+qXpEPlAABAs7X79Moe28dKUvH33upGAgAATdRu6XhM0iXF60skfb2acQAAQFOlPDK7XtK3JZ1ie6ft+ZJulvQR2y9K+nCxDQAAcFCOyHebBfd0AADQFYYioqd1J+9ICgAAsqB0AACALCgdAAAgC0oHAADIgtIBAACyoHQAAIAsKB0AACALSgcAAMiC0gEAALKgdAAAgCwoHQAAIIuUD3xbY3uv7edG7LvV9g9tf9/2o7aPGtsxAQDAeJdypuNeSbNa9m2W9J6IeK+kFyRdV/FcAACgYd6ydETEU5L2t+zbFBEHis0BSdPGYDYAANAgVdzT8RlJ/1HB9wEAAA02scw/tv15SQckrTtEpk9SX5njAACA8a/t0mH705LOlnRmRMTBchHRL6m/+DcHzQEAgGZrq3TYniVpkaSZEfHrakcCAABNlPLI7HpJ35Z0iu2dtudL+oqkd0rabHub7X8d4zkBAMA450NcGan+YFxeAQCgGwxFRE/rTt6RFAAAZEHpAAAAWVA6AABAFpQOAACQBaUDAABkQekAAABZUDoAAEAWlA4AAJAFpQMAAGRB6QAAAFmU+mj7NvxU0o9b9r2r2I/Owrp0HtakM7EunYc1qd+7R9uZ9bNXRh3AHhzt/dlRL9al87AmnYl16TysSefi8goAAMiC0gEAALLohNLRX/cAGBXr0nlYk87EunQe1qRD1X5PBwAA6A6dcKYDAAB0gVpLh+1Ztn9k+yXbS+qcpVvZXmN7r+3nRuybZHuz7ReLv4+uc8ZuZPs420/a3m77edtXFftZm5rYPsL2d2x/r1iTG4v9J9jeWvwc+zfbh9c9a7exPcH2s7a/UWyzJh2qttJhe4Kkr0r6qKTpkubanl7XPF3sXkmzWvYtkfRERJwk6YliG3kdkLQwIqZL6pX02eK/D9amPr+RdEZEvE/SDEmzbPdKWi7pSxHxt5L+T9L8GmfsVldJ2jFimzXpUHWe6Xi/pJci4pWIeF3SBkmza5ynK0XEU5L2t+yeLem+4vV9kj6ZdSgoInZFxDPF619o+AfqVLE2tYlhvyw2Dyv+hKQzJD1c7GdNMrM9TdLHJa0qti3WpGPVWTqmSnp1xPbOYh/qNyUidhWvd0uaUucw3c728ZJOlbRVrE2titP42yTtlbRZ0suSfhYRB4oIP8fyu0PSIkm/L7b/UqxJx+JGUhxSDD/exCNONbH9Dklfk/S5iPj5yK+xNvlFxO8iYoakaRo+W/t3NY/U1WyfLWlvRAzVPQvS5P7slZFek3TciO1pxT7Ub4/tYyNil+1jNfxbHTKzfZiGC8e6iHik2M3adICI+JntJyV9QNJRticWv1nzcyyv0yR9wvbHJB0h6c8lfVmsSceq80zHdyWdVNxlfLikOZIeq3EevOExSZcUry+R9PUaZ+lKxXXp1ZJ2RMSKEV9ibWpie7Lto4rXb5P0EQ3fa/OkpE8VMdYko4i4LiKmRcTxGv5/yDcj4kKxJh2r1jcHK9rpHZImSFoTEV+sbZguZXu9pA9p+FMZ90i6QdK/S9oo6a81/KnA50dE682mGEO2T5f0LUk/0BvXqv9Fw/d1sDY1sP1eDd+UOEHDv7BtjIgv2P4bDd8IP0nSs5LmRcRv6pu0O9n+kKRrIuJs1qRz8Y6kAAAgC24kBQAAWVA6AABAFpQOAACQBaUDAABkQekAAABZUDoAAEAWlA4AAJAFpQMAAGTxB0XaVef/EYFzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Access Point:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Access Point:  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR1aksXnEqYm",
        "colab_type": "text"
      },
      "source": [
        "#BaseLine SISO location prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqO5tUcauHBd",
        "colab_type": "text"
      },
      "source": [
        "##General function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuZKED4guFlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
        "\n",
        "def SISO(data_dict):\n",
        "    X = np.concatenate([data_dict[node] for node in nodes])\n",
        "    y = [node_to_coordinates[node] for node in nodes]\n",
        "\n",
        "    labels = []\n",
        "    for label in y:\n",
        "        for i in range(100):\n",
        "            labels.append(label) \n",
        "    labels = np.asarray(labels)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, labels, shuffle = True, test_size=0.3)\n",
        "\n",
        "    sc_X = StandardScaler()\n",
        "    X_train = sc_X.fit_transform(X_train)\n",
        "    X_test = sc_X.transform(X_test)\n",
        "\n",
        "    inp = Input(shape=(4,))\n",
        "    x = Dense(128, activation='relu')(inp)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dense(8, activation='relu')(x)\n",
        "    x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "    model = Model(inp, x)\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=0)\n",
        "    history = model.fit(X_train, y_train, epochs = 200, batch_size=16, verbose=0, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    error_nn = []\n",
        "    for e in range(y_pred.shape[0]):\n",
        "        error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "    #KNN\n",
        "    knn = KNNR(n_neighbors = 2)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    error_knnr = []\n",
        "    for e in range(y_pred.shape[0]):\n",
        "        error_knnr.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "    return np.mean(error_nn), np.std(error_nn), np.mean(error_knnr), np.std(error_knnr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVtYQRA9ZSlK",
        "colab_type": "text"
      },
      "source": [
        "##Noise : 0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jf2T8kiEuad",
        "colab_type": "code",
        "outputId": "2a783e1a-ff0c-4519-b2e7-3787a04baa7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = np.concatenate([data_dict[node] for node in nodes])\n",
        "y = [node_to_coordinates[node] for node in nodes]\n",
        "\n",
        "labels = []\n",
        "for label in y:\n",
        "    for i in range(100):\n",
        "        labels.append(label) \n",
        "labels = np.asarray(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3570, 4) (3570, 2)\n",
            "(1530, 4) (1530, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCea7ZMKGABp",
        "colab_type": "text"
      },
      "source": [
        "###DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GvsW25UHIQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(4,))\n",
        "x = Dense(128, activation='relu')(inp)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-h3MXlUHTzF",
        "colab_type": "code",
        "outputId": "4309c51e-d8ee-47f0-b3c2-b4204f462190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs = 200, batch_size=16, validation_data=(X_test, y_test), callbacks=[lr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3570 samples, validate on 1530 samples\n",
            "Epoch 1/200\n",
            "3570/3570 [==============================] - 1s 332us/step - loss: 21.4505 - val_loss: 2.6974\n",
            "Epoch 2/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 1.3904 - val_loss: 0.8239\n",
            "Epoch 3/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.6926 - val_loss: 0.6273\n",
            "Epoch 4/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.5798 - val_loss: 0.5361\n",
            "Epoch 5/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.4952 - val_loss: 0.4648\n",
            "Epoch 6/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.4167 - val_loss: 0.4016\n",
            "Epoch 7/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.3408 - val_loss: 0.3468\n",
            "Epoch 8/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.2863 - val_loss: 0.2682\n",
            "Epoch 9/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2353 - val_loss: 0.2662\n",
            "Epoch 10/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2043 - val_loss: 0.1963\n",
            "Epoch 11/200\n",
            "3570/3570 [==============================] - 1s 304us/step - loss: 0.1646 - val_loss: 0.1378\n",
            "Epoch 12/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.1393 - val_loss: 0.1788\n",
            "Epoch 13/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.1195 - val_loss: 0.1255\n",
            "Epoch 14/200\n",
            "3570/3570 [==============================] - 1s 302us/step - loss: 0.1203 - val_loss: 0.1050\n",
            "Epoch 15/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0999 - val_loss: 0.1075\n",
            "Epoch 16/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.0987 - val_loss: 0.0871\n",
            "Epoch 17/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0898 - val_loss: 0.1038\n",
            "Epoch 18/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0899 - val_loss: 0.0974\n",
            "Epoch 19/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0829 - val_loss: 0.1364\n",
            "Epoch 20/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0837 - val_loss: 0.0849\n",
            "Epoch 21/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0726 - val_loss: 0.1136\n",
            "Epoch 22/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0782 - val_loss: 0.0842\n",
            "Epoch 23/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0794 - val_loss: 0.0864\n",
            "Epoch 24/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0709 - val_loss: 0.0529\n",
            "Epoch 25/200\n",
            "3570/3570 [==============================] - 1s 284us/step - loss: 0.0675 - val_loss: 0.0750\n",
            "Epoch 26/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 0.0583 - val_loss: 0.0834\n",
            "Epoch 27/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0582 - val_loss: 0.0843\n",
            "Epoch 28/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0607 - val_loss: 0.1130\n",
            "Epoch 29/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0613 - val_loss: 0.0619\n",
            "Epoch 30/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0554 - val_loss: 0.0501\n",
            "Epoch 31/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.0532 - val_loss: 0.0622\n",
            "Epoch 32/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0518 - val_loss: 0.0666\n",
            "Epoch 33/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 0.0522 - val_loss: 0.0555\n",
            "Epoch 34/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 0.0458 - val_loss: 0.0624\n",
            "Epoch 35/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0479 - val_loss: 0.0641\n",
            "Epoch 36/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0466 - val_loss: 0.0488\n",
            "Epoch 37/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0501 - val_loss: 0.0396\n",
            "Epoch 38/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0470 - val_loss: 0.0538\n",
            "Epoch 39/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0474 - val_loss: 0.0335\n",
            "Epoch 40/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.0422 - val_loss: 0.0386\n",
            "Epoch 41/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0410 - val_loss: 0.0460\n",
            "Epoch 42/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.0429 - val_loss: 0.0327\n",
            "Epoch 43/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0477 - val_loss: 0.0490\n",
            "Epoch 44/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.0400 - val_loss: 0.0368\n",
            "Epoch 45/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0341 - val_loss: 0.0379\n",
            "Epoch 46/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0367 - val_loss: 0.0297\n",
            "Epoch 47/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.0349 - val_loss: 0.0283\n",
            "Epoch 48/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0378 - val_loss: 0.0312\n",
            "Epoch 49/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0423 - val_loss: 0.0344\n",
            "Epoch 50/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0378 - val_loss: 0.0339\n",
            "Epoch 51/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0340 - val_loss: 0.0291\n",
            "Epoch 52/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0372 - val_loss: 0.0272\n",
            "Epoch 53/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0368 - val_loss: 0.0640\n",
            "Epoch 54/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.0376 - val_loss: 0.0344\n",
            "Epoch 55/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.0319 - val_loss: 0.0390\n",
            "Epoch 56/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0320 - val_loss: 0.0232\n",
            "Epoch 57/200\n",
            "3570/3570 [==============================] - 1s 281us/step - loss: 0.0321 - val_loss: 0.0345\n",
            "Epoch 58/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0301 - val_loss: 0.0411\n",
            "Epoch 59/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0410 - val_loss: 0.0363\n",
            "Epoch 60/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0400 - val_loss: 0.0205\n",
            "Epoch 61/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0299 - val_loss: 0.0374\n",
            "Epoch 62/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 0.0303 - val_loss: 0.0230\n",
            "Epoch 63/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0310 - val_loss: 0.0420\n",
            "Epoch 64/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0266 - val_loss: 0.0335\n",
            "Epoch 65/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 0.0298 - val_loss: 0.0836\n",
            "Epoch 66/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.0412 - val_loss: 0.0201\n",
            "Epoch 67/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0259 - val_loss: 0.0184\n",
            "Epoch 68/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0273 - val_loss: 0.0180\n",
            "Epoch 69/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0259 - val_loss: 0.0309\n",
            "Epoch 70/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0279 - val_loss: 0.0173\n",
            "Epoch 71/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0290 - val_loss: 0.0437\n",
            "Epoch 72/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0232 - val_loss: 0.0226\n",
            "Epoch 73/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0419 - val_loss: 0.0343\n",
            "Epoch 74/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0226 - val_loss: 0.0294\n",
            "Epoch 75/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0283 - val_loss: 0.0597\n",
            "Epoch 76/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.0336 - val_loss: 0.0492\n",
            "Epoch 77/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0248 - val_loss: 0.0530\n",
            "Epoch 78/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0254 - val_loss: 0.0273\n",
            "Epoch 79/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0302 - val_loss: 0.0249\n",
            "Epoch 80/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.0242 - val_loss: 0.0363\n",
            "\n",
            "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "Epoch 81/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0216 - val_loss: 0.0138\n",
            "Epoch 82/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0183 - val_loss: 0.0362\n",
            "Epoch 83/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0184 - val_loss: 0.0275\n",
            "Epoch 84/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0287 - val_loss: 0.0250\n",
            "Epoch 85/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0191 - val_loss: 0.0182\n",
            "Epoch 86/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0229 - val_loss: 0.0245\n",
            "Epoch 87/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0213 - val_loss: 0.0184\n",
            "Epoch 88/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.0189 - val_loss: 0.0287\n",
            "Epoch 89/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0205 - val_loss: 0.0318\n",
            "Epoch 90/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0205 - val_loss: 0.0173\n",
            "Epoch 91/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0178 - val_loss: 0.0138\n",
            "\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "Epoch 92/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0153 - val_loss: 0.0140\n",
            "Epoch 93/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0147 - val_loss: 0.0162\n",
            "Epoch 94/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0155 - val_loss: 0.0216\n",
            "Epoch 95/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.0169 - val_loss: 0.0219\n",
            "Epoch 96/200\n",
            "3570/3570 [==============================] - 1s 305us/step - loss: 0.0155 - val_loss: 0.0117\n",
            "Epoch 97/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.0143 - val_loss: 0.0186\n",
            "Epoch 98/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0166 - val_loss: 0.0230\n",
            "Epoch 99/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0136 - val_loss: 0.0161\n",
            "Epoch 100/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0144 - val_loss: 0.0124\n",
            "Epoch 101/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0147 - val_loss: 0.0193\n",
            "Epoch 102/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.0162 - val_loss: 0.0480\n",
            "Epoch 103/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.0208 - val_loss: 0.0177\n",
            "Epoch 104/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0155 - val_loss: 0.0133\n",
            "Epoch 105/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0167 - val_loss: 0.0202\n",
            "Epoch 106/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 0.0169 - val_loss: 0.0121\n",
            "\n",
            "Epoch 00106: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "Epoch 107/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0114 - val_loss: 0.0104\n",
            "Epoch 108/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0121 - val_loss: 0.0112\n",
            "Epoch 109/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.0118 - val_loss: 0.0124\n",
            "Epoch 110/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0122 - val_loss: 0.0131\n",
            "Epoch 111/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0118 - val_loss: 0.0131\n",
            "Epoch 112/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.0130 - val_loss: 0.0138\n",
            "Epoch 113/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0107 - val_loss: 0.0172\n",
            "Epoch 114/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0126 - val_loss: 0.0165\n",
            "Epoch 115/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0117 - val_loss: 0.0127\n",
            "Epoch 116/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0123 - val_loss: 0.0177\n",
            "Epoch 117/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.0122 - val_loss: 0.0157\n",
            "\n",
            "Epoch 00117: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "Epoch 118/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.0108 - val_loss: 0.0089\n",
            "Epoch 119/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0096 - val_loss: 0.0108\n",
            "Epoch 120/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0098 - val_loss: 0.0139\n",
            "Epoch 121/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.0111 - val_loss: 0.0113\n",
            "Epoch 122/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0100 - val_loss: 0.0102\n",
            "Epoch 123/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.0104 - val_loss: 0.0094\n",
            "Epoch 124/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 0.0107 - val_loss: 0.0109\n",
            "Epoch 125/200\n",
            "3570/3570 [==============================] - 1s 317us/step - loss: 0.0098 - val_loss: 0.0088\n",
            "Epoch 126/200\n",
            "3570/3570 [==============================] - 1s 313us/step - loss: 0.0104 - val_loss: 0.0083\n",
            "Epoch 127/200\n",
            "3570/3570 [==============================] - 1s 314us/step - loss: 0.0101 - val_loss: 0.0101\n",
            "Epoch 128/200\n",
            "3570/3570 [==============================] - 1s 313us/step - loss: 0.0094 - val_loss: 0.0121\n",
            "Epoch 129/200\n",
            "3570/3570 [==============================] - 1s 318us/step - loss: 0.0102 - val_loss: 0.0156\n",
            "Epoch 130/200\n",
            "3570/3570 [==============================] - 1s 314us/step - loss: 0.0101 - val_loss: 0.0115\n",
            "Epoch 131/200\n",
            "3570/3570 [==============================] - 1s 310us/step - loss: 0.0092 - val_loss: 0.0179\n",
            "Epoch 132/200\n",
            "3570/3570 [==============================] - 1s 309us/step - loss: 0.0104 - val_loss: 0.0088\n",
            "Epoch 133/200\n",
            "3570/3570 [==============================] - 1s 303us/step - loss: 0.0096 - val_loss: 0.0145\n",
            "Epoch 134/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.0103 - val_loss: 0.0110\n",
            "Epoch 135/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0097 - val_loss: 0.0079\n",
            "Epoch 136/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0098 - val_loss: 0.0085\n",
            "Epoch 137/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.0097 - val_loss: 0.0134\n",
            "Epoch 138/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.0105 - val_loss: 0.0087\n",
            "Epoch 139/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.0090 - val_loss: 0.0102\n",
            "Epoch 140/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0103 - val_loss: 0.0091\n",
            "Epoch 141/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.0088 - val_loss: 0.0090\n",
            "Epoch 142/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0092 - val_loss: 0.0106\n",
            "Epoch 143/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.0097 - val_loss: 0.0098\n",
            "Epoch 144/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0092 - val_loss: 0.0087\n",
            "Epoch 145/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0088 - val_loss: 0.0091\n",
            "\n",
            "Epoch 00145: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "Epoch 146/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0082 - val_loss: 0.0070\n",
            "Epoch 147/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0081 - val_loss: 0.0069\n",
            "Epoch 148/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0077 - val_loss: 0.0094\n",
            "Epoch 149/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.0081 - val_loss: 0.0076\n",
            "Epoch 150/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0078 - val_loss: 0.0087\n",
            "Epoch 151/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 0.0083 - val_loss: 0.0083\n",
            "Epoch 152/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0079 - val_loss: 0.0096\n",
            "Epoch 153/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0077 - val_loss: 0.0094\n",
            "Epoch 154/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0081 - val_loss: 0.0089\n",
            "Epoch 155/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0083 - val_loss: 0.0077\n",
            "Epoch 156/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.0081 - val_loss: 0.0102\n",
            "Epoch 157/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0081 - val_loss: 0.0073\n",
            "\n",
            "Epoch 00157: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "Epoch 158/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0073 - val_loss: 0.0081\n",
            "Epoch 159/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0068 - val_loss: 0.0069\n",
            "Epoch 160/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0074 - val_loss: 0.0072\n",
            "Epoch 161/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.0069 - val_loss: 0.0079\n",
            "Epoch 162/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.0078 - val_loss: 0.0072\n",
            "Epoch 163/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0070 - val_loss: 0.0079\n",
            "Epoch 164/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0069 - val_loss: 0.0070\n",
            "Epoch 165/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0069 - val_loss: 0.0061\n",
            "Epoch 166/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0071 - val_loss: 0.0076\n",
            "Epoch 167/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.0068 - val_loss: 0.0066\n",
            "Epoch 168/200\n",
            "3570/3570 [==============================] - 1s 307us/step - loss: 0.0070 - val_loss: 0.0079\n",
            "Epoch 169/200\n",
            "3570/3570 [==============================] - 1s 310us/step - loss: 0.0074 - val_loss: 0.0087\n",
            "Epoch 170/200\n",
            "3570/3570 [==============================] - 1s 316us/step - loss: 0.0071 - val_loss: 0.0085\n",
            "Epoch 171/200\n",
            "3570/3570 [==============================] - 1s 308us/step - loss: 0.0070 - val_loss: 0.0101\n",
            "Epoch 172/200\n",
            "3570/3570 [==============================] - 1s 315us/step - loss: 0.0069 - val_loss: 0.0079\n",
            "Epoch 173/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0069 - val_loss: 0.0062\n",
            "Epoch 174/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.0070 - val_loss: 0.0076\n",
            "Epoch 175/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0066 - val_loss: 0.0074\n",
            "\n",
            "Epoch 00175: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "Epoch 176/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.0062 - val_loss: 0.0064\n",
            "Epoch 177/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.0063 - val_loss: 0.0065\n",
            "Epoch 178/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0061 - val_loss: 0.0067\n",
            "Epoch 179/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0061 - val_loss: 0.0057\n",
            "Epoch 180/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0063 - val_loss: 0.0058\n",
            "Epoch 181/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0062 - val_loss: 0.0066\n",
            "Epoch 182/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0062 - val_loss: 0.0062\n",
            "Epoch 183/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0060 - val_loss: 0.0059\n",
            "Epoch 184/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0061 - val_loss: 0.0063\n",
            "Epoch 185/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0062 - val_loss: 0.0058\n",
            "Epoch 186/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.0063 - val_loss: 0.0063\n",
            "Epoch 187/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.0059 - val_loss: 0.0055\n",
            "Epoch 188/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.0061 - val_loss: 0.0060\n",
            "Epoch 189/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 0.0061 - val_loss: 0.0078\n",
            "Epoch 190/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0061 - val_loss: 0.0063\n",
            "Epoch 191/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0059 - val_loss: 0.0066\n",
            "Epoch 192/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0061 - val_loss: 0.0058\n",
            "Epoch 193/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0061 - val_loss: 0.0075\n",
            "Epoch 194/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.0059 - val_loss: 0.0054\n",
            "Epoch 195/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.0058 - val_loss: 0.0064\n",
            "Epoch 196/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.0061 - val_loss: 0.0068\n",
            "Epoch 197/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.0058 - val_loss: 0.0067\n",
            "\n",
            "Epoch 00197: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "Epoch 198/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0056 - val_loss: 0.0054\n",
            "Epoch 199/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.0055 - val_loss: 0.0053\n",
            "Epoch 200/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.0055 - val_loss: 0.0071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmKG2Ll5JuAu",
        "colab_type": "code",
        "outputId": "ec958bd9-d1a2-452b-ceef-ba006392e908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(history.history['val_loss'][2:])\n",
        "plt.plot(history.history['loss'][2:])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltV1VpXdJD2b",
        "colab_type": "code",
        "outputId": "0867b071-b808-4165-fa1f-ec628d8276f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 0.09087233120096234\n",
            "Std of error: 0.07686327577814842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnjXu0PxOqJy",
        "colab_type": "text"
      },
      "source": [
        "###Classification using one hot encoded nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLF0yR6AZHJc",
        "colab_type": "code",
        "outputId": "d3b453d5-5fff-417b-e02e-b681c5fe5067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = np.concatenate([data_dict[node] for node in nodes])\n",
        "y = np.asarray([np.eye(len(nodes))[nodes.index(node)] for node in nodes])\n",
        "\n",
        "labels = []\n",
        "for label in y:\n",
        "    for i in range(100):\n",
        "        labels.append(label) \n",
        "labels = np.asarray(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3570, 4) (3570, 51)\n",
            "(1530, 4) (1530, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF1WIU0tOQ-q",
        "colab_type": "code",
        "outputId": "82eeba2a-9fdd-444b-9ec6-5b6c6c8df8c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(4,))\n",
        "x = Dense(128, activation='relu')(inp)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(51, activation = 'softmax')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQYaJFw8bkIT",
        "colab_type": "code",
        "outputId": "5a9e06de-8d5a-44d9-8f82-b55082cc08d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3570 samples, validate on 1530 samples\n",
            "Epoch 1/50\n",
            "3570/3570 [==============================] - 1s 365us/step - loss: 3.0156 - val_loss: 2.0396\n",
            "Epoch 2/50\n",
            "3570/3570 [==============================] - 1s 323us/step - loss: 1.5249 - val_loss: 1.1423\n",
            "Epoch 3/50\n",
            "3570/3570 [==============================] - 1s 325us/step - loss: 1.0193 - val_loss: 0.9071\n",
            "Epoch 4/50\n",
            "3570/3570 [==============================] - 1s 319us/step - loss: 0.8282 - val_loss: 0.7969\n",
            "Epoch 5/50\n",
            "3570/3570 [==============================] - 1s 320us/step - loss: 0.6937 - val_loss: 0.7219\n",
            "Epoch 6/50\n",
            "3570/3570 [==============================] - 1s 319us/step - loss: 0.6627 - val_loss: 0.5973\n",
            "Epoch 7/50\n",
            "3570/3570 [==============================] - 1s 320us/step - loss: 0.6004 - val_loss: 0.6469\n",
            "Epoch 8/50\n",
            "3570/3570 [==============================] - 1s 324us/step - loss: 0.5868 - val_loss: 0.5429\n",
            "Epoch 9/50\n",
            "3570/3570 [==============================] - 1s 315us/step - loss: 0.5507 - val_loss: 0.5715\n",
            "Epoch 10/50\n",
            "3570/3570 [==============================] - 1s 318us/step - loss: 0.5535 - val_loss: 0.5510\n",
            "Epoch 11/50\n",
            "3570/3570 [==============================] - 1s 323us/step - loss: 0.5193 - val_loss: 0.5396\n",
            "Epoch 12/50\n",
            "3570/3570 [==============================] - 1s 325us/step - loss: 0.5245 - val_loss: 0.5414\n",
            "Epoch 13/50\n",
            "3570/3570 [==============================] - 1s 320us/step - loss: 0.5051 - val_loss: 0.5296\n",
            "Epoch 14/50\n",
            "3570/3570 [==============================] - 1s 320us/step - loss: 0.4911 - val_loss: 0.4977\n",
            "Epoch 15/50\n",
            "3570/3570 [==============================] - 1s 321us/step - loss: 0.4694 - val_loss: 0.5792\n",
            "Epoch 16/50\n",
            "3570/3570 [==============================] - 1s 322us/step - loss: 0.4882 - val_loss: 0.4908\n",
            "Epoch 17/50\n",
            "3570/3570 [==============================] - 1s 320us/step - loss: 0.4546 - val_loss: 0.4988\n",
            "Epoch 18/50\n",
            "3570/3570 [==============================] - 1s 321us/step - loss: 0.4638 - val_loss: 0.4718\n",
            "Epoch 19/50\n",
            "3570/3570 [==============================] - 1s 319us/step - loss: 0.4586 - val_loss: 0.4667\n",
            "Epoch 20/50\n",
            "3570/3570 [==============================] - 1s 316us/step - loss: 0.4381 - val_loss: 0.5066\n",
            "Epoch 21/50\n",
            "3570/3570 [==============================] - 1s 317us/step - loss: 0.4519 - val_loss: 0.4660\n",
            "Epoch 22/50\n",
            "3570/3570 [==============================] - 1s 324us/step - loss: 0.4507 - val_loss: 0.5223\n",
            "Epoch 23/50\n",
            "3570/3570 [==============================] - 1s 318us/step - loss: 0.4356 - val_loss: 0.4973\n",
            "Epoch 24/50\n",
            "3570/3570 [==============================] - 1s 322us/step - loss: 0.4501 - val_loss: 0.4601\n",
            "Epoch 25/50\n",
            "3570/3570 [==============================] - 1s 322us/step - loss: 0.4421 - val_loss: 0.5002\n",
            "Epoch 26/50\n",
            "3570/3570 [==============================] - 1s 319us/step - loss: 0.4216 - val_loss: 0.4771\n",
            "Epoch 27/50\n",
            "3570/3570 [==============================] - 1s 325us/step - loss: 0.4325 - val_loss: 0.4763\n",
            "Epoch 28/50\n",
            "3570/3570 [==============================] - 1s 320us/step - loss: 0.4390 - val_loss: 0.4930\n",
            "Epoch 29/50\n",
            "3570/3570 [==============================] - 1s 321us/step - loss: 0.4368 - val_loss: 0.4764\n",
            "Epoch 30/50\n",
            "3570/3570 [==============================] - 1s 326us/step - loss: 0.4181 - val_loss: 0.4675\n",
            "Epoch 31/50\n",
            "3570/3570 [==============================] - 1s 321us/step - loss: 0.4199 - val_loss: 0.5108\n",
            "Epoch 32/50\n",
            "3570/3570 [==============================] - 1s 318us/step - loss: 0.4158 - val_loss: 0.4385\n",
            "Epoch 33/50\n",
            "3570/3570 [==============================] - 1s 320us/step - loss: 0.4114 - val_loss: 0.4962\n",
            "Epoch 34/50\n",
            "3570/3570 [==============================] - 1s 326us/step - loss: 0.4218 - val_loss: 0.5583\n",
            "Epoch 35/50\n",
            "3570/3570 [==============================] - 1s 318us/step - loss: 0.4272 - val_loss: 0.4598\n",
            "Epoch 36/50\n",
            "3570/3570 [==============================] - 1s 317us/step - loss: 0.4166 - val_loss: 0.4809\n",
            "Epoch 37/50\n",
            "3570/3570 [==============================] - 1s 314us/step - loss: 0.3991 - val_loss: 0.4676\n",
            "Epoch 38/50\n",
            "3570/3570 [==============================] - 1s 316us/step - loss: 0.3924 - val_loss: 0.4371\n",
            "Epoch 39/50\n",
            "3570/3570 [==============================] - 1s 316us/step - loss: 0.3952 - val_loss: 0.4266\n",
            "Epoch 40/50\n",
            "3570/3570 [==============================] - 1s 317us/step - loss: 0.4060 - val_loss: 0.4584\n",
            "Epoch 41/50\n",
            "3570/3570 [==============================] - 1s 315us/step - loss: 0.4052 - val_loss: 0.4583\n",
            "Epoch 42/50\n",
            "3570/3570 [==============================] - 1s 322us/step - loss: 0.3955 - val_loss: 0.4981\n",
            "Epoch 43/50\n",
            "3570/3570 [==============================] - 1s 321us/step - loss: 0.4065 - val_loss: 0.4225\n",
            "Epoch 44/50\n",
            "3570/3570 [==============================] - 1s 318us/step - loss: 0.3993 - val_loss: 0.4778\n",
            "Epoch 45/50\n",
            "3570/3570 [==============================] - 1s 322us/step - loss: 0.3851 - val_loss: 0.4516\n",
            "Epoch 46/50\n",
            "3570/3570 [==============================] - 1s 352us/step - loss: 0.4232 - val_loss: 0.4198\n",
            "Epoch 47/50\n",
            "3570/3570 [==============================] - 1s 357us/step - loss: 0.3848 - val_loss: 0.4481\n",
            "Epoch 48/50\n",
            "3570/3570 [==============================] - 1s 351us/step - loss: 0.3870 - val_loss: 0.4642\n",
            "Epoch 49/50\n",
            "3570/3570 [==============================] - 1s 346us/step - loss: 0.3991 - val_loss: 0.4868\n",
            "Epoch 50/50\n",
            "3570/3570 [==============================] - 1s 323us/step - loss: 0.3880 - val_loss: 0.4162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f9f2d7a2f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH_qtw-tbxmj",
        "colab_type": "code",
        "outputId": "0416f86d-fa7c-4cee-bf09-105e02358e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "#With added gaussian noise and categorical output\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of SE error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 0.006076994534169057\n",
            "Std of SE error: 0.06285789521772076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOldJYL7LGf7",
        "colab_type": "text"
      },
      "source": [
        "###KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJv33n-YgHfx",
        "colab_type": "code",
        "outputId": "1560c7a2-17d4-4f58-d73f-7e068085e56c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
        "\n",
        "knn = KNNR(n_neighbors = 2)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "error_knnr = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_knnr.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_knnr))\n",
        "print('Std of SE error:',np.std(error_knnr))\n",
        "\n",
        "plt.hist(np.asarray(error_knnr), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 0.0022875816993464053\n",
            "Std of SE error: 0.03374252242559851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mEg6dfk8Znq3"
      },
      "source": [
        "##Noise : 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f2f0da99-e205-4ae5-d5d5-51330b5ab1c8",
        "id": "L1yld3uoZnrI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = np.concatenate([n_data_dict[node] for node in nodes])\n",
        "y = [node_to_coordinates[node] for node in nodes]\n",
        "\n",
        "labels = []\n",
        "for label in y:\n",
        "    for i in range(100):\n",
        "        labels.append(label) \n",
        "labels = np.asarray(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3570, 4) (3570, 2)\n",
            "(1530, 4) (1530, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VmhCT8SZZnri"
      },
      "source": [
        "###DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o9mHj4IIZnrm",
        "colab": {}
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(4,))\n",
        "x = Dense(128, activation='relu')(inp)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2311c533-e201-42f3-c442-9644d83d7fa3",
        "id": "y2IKgXR4Znrz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs = 200, batch_size=16, validation_data=(X_test, y_test), callbacks=[lr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3570 samples, validate on 1530 samples\n",
            "Epoch 1/200\n",
            "3570/3570 [==============================] - 1s 337us/step - loss: 21.4359 - val_loss: 3.5815\n",
            "Epoch 2/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 2.0563 - val_loss: 1.2886\n",
            "Epoch 3/200\n",
            "3570/3570 [==============================] - 1s 314us/step - loss: 1.0113 - val_loss: 1.0537\n",
            "Epoch 4/200\n",
            "3570/3570 [==============================] - 1s 312us/step - loss: 0.8307 - val_loss: 1.0462\n",
            "Epoch 5/200\n",
            "3570/3570 [==============================] - 1s 317us/step - loss: 0.7611 - val_loss: 0.7080\n",
            "Epoch 6/200\n",
            "3570/3570 [==============================] - 1s 302us/step - loss: 0.6853 - val_loss: 0.6950\n",
            "Epoch 7/200\n",
            "3570/3570 [==============================] - 1s 310us/step - loss: 0.6406 - val_loss: 0.5735\n",
            "Epoch 8/200\n",
            "3570/3570 [==============================] - 1s 318us/step - loss: 0.5942 - val_loss: 0.7823\n",
            "Epoch 9/200\n",
            "3570/3570 [==============================] - 1s 320us/step - loss: 0.5770 - val_loss: 0.5501\n",
            "Epoch 10/200\n",
            "3570/3570 [==============================] - 1s 316us/step - loss: 0.5275 - val_loss: 0.5418\n",
            "Epoch 11/200\n",
            "3570/3570 [==============================] - 1s 318us/step - loss: 0.5332 - val_loss: 0.5209\n",
            "Epoch 12/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.4830 - val_loss: 0.5425\n",
            "Epoch 13/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.4820 - val_loss: 0.5830\n",
            "Epoch 14/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.4839 - val_loss: 0.4636\n",
            "Epoch 15/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.4477 - val_loss: 0.4618\n",
            "Epoch 16/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.4557 - val_loss: 0.5820\n",
            "Epoch 17/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.4423 - val_loss: 0.4521\n",
            "Epoch 18/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.4258 - val_loss: 0.4713\n",
            "Epoch 19/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.4367 - val_loss: 0.4666\n",
            "Epoch 20/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.4148 - val_loss: 0.3797\n",
            "Epoch 21/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.4102 - val_loss: 0.5034\n",
            "Epoch 22/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.4084 - val_loss: 0.4489\n",
            "Epoch 23/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.3970 - val_loss: 0.4336\n",
            "Epoch 24/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.3893 - val_loss: 0.4165\n",
            "Epoch 25/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.3980 - val_loss: 0.3944\n",
            "Epoch 26/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.3680 - val_loss: 0.3992\n",
            "Epoch 27/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.3734 - val_loss: 0.4798\n",
            "Epoch 28/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.3692 - val_loss: 0.5295\n",
            "Epoch 29/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.3768 - val_loss: 0.4633\n",
            "Epoch 30/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 0.3651 - val_loss: 0.3392\n",
            "Epoch 31/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 0.3635 - val_loss: 0.3614\n",
            "Epoch 32/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.3489 - val_loss: 0.3887\n",
            "Epoch 33/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.3810 - val_loss: 0.3899\n",
            "Epoch 34/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.3684 - val_loss: 0.3680\n",
            "Epoch 35/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.3414 - val_loss: 0.3832\n",
            "Epoch 36/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.3312 - val_loss: 0.3572\n",
            "Epoch 37/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.3538 - val_loss: 0.3760\n",
            "Epoch 38/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.3330 - val_loss: 0.3692\n",
            "Epoch 39/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.3248 - val_loss: 0.3897\n",
            "Epoch 40/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 0.3473 - val_loss: 0.3970\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "Epoch 41/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.3111 - val_loss: 0.3154\n",
            "Epoch 42/200\n",
            "3570/3570 [==============================] - 1s 310us/step - loss: 0.3109 - val_loss: 0.3115\n",
            "Epoch 43/200\n",
            "3570/3570 [==============================] - 1s 319us/step - loss: 0.3076 - val_loss: 0.3001\n",
            "Epoch 44/200\n",
            "3570/3570 [==============================] - 1s 320us/step - loss: 0.3024 - val_loss: 0.3265\n",
            "Epoch 45/200\n",
            "3570/3570 [==============================] - 1s 318us/step - loss: 0.3044 - val_loss: 0.3037\n",
            "Epoch 46/200\n",
            "3570/3570 [==============================] - 1s 303us/step - loss: 0.3094 - val_loss: 0.2906\n",
            "Epoch 47/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2974 - val_loss: 0.2901\n",
            "Epoch 48/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2934 - val_loss: 0.3474\n",
            "Epoch 49/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2992 - val_loss: 0.2819\n",
            "Epoch 50/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.2961 - val_loss: 0.2916\n",
            "Epoch 51/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.2937 - val_loss: 0.2737\n",
            "Epoch 52/200\n",
            "3570/3570 [==============================] - 1s 302us/step - loss: 0.2891 - val_loss: 0.3613\n",
            "Epoch 53/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 0.2900 - val_loss: 0.2856\n",
            "Epoch 54/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2979 - val_loss: 0.2675\n",
            "Epoch 55/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2924 - val_loss: 0.2921\n",
            "Epoch 56/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2858 - val_loss: 0.2812\n",
            "Epoch 57/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2892 - val_loss: 0.2873\n",
            "Epoch 58/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2850 - val_loss: 0.2825\n",
            "Epoch 59/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2904 - val_loss: 0.2756\n",
            "Epoch 60/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2868 - val_loss: 0.2921\n",
            "Epoch 61/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 0.2775 - val_loss: 0.2761\n",
            "Epoch 62/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2798 - val_loss: 0.3067\n",
            "Epoch 63/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2815 - val_loss: 0.3061\n",
            "Epoch 64/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2777 - val_loss: 0.2966\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "Epoch 65/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2628 - val_loss: 0.2789\n",
            "Epoch 66/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2647 - val_loss: 0.3002\n",
            "Epoch 67/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.2632 - val_loss: 0.2866\n",
            "Epoch 68/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.2546 - val_loss: 0.2590\n",
            "Epoch 69/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2606 - val_loss: 0.2817\n",
            "Epoch 70/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.2569 - val_loss: 0.2751\n",
            "Epoch 71/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2634 - val_loss: 0.2696\n",
            "Epoch 72/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.2586 - val_loss: 0.2508\n",
            "Epoch 73/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 0.2578 - val_loss: 0.2688\n",
            "Epoch 74/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2614 - val_loss: 0.2508\n",
            "Epoch 75/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.2618 - val_loss: 0.2714\n",
            "Epoch 76/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2526 - val_loss: 0.2569\n",
            "Epoch 77/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2530 - val_loss: 0.2498\n",
            "Epoch 78/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2470 - val_loss: 0.2716\n",
            "Epoch 79/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2558 - val_loss: 0.2749\n",
            "Epoch 80/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2532 - val_loss: 0.2454\n",
            "Epoch 81/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2504 - val_loss: 0.2526\n",
            "Epoch 82/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.2573 - val_loss: 0.2950\n",
            "Epoch 83/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.2502 - val_loss: 0.2782\n",
            "Epoch 84/200\n",
            "3570/3570 [==============================] - 1s 302us/step - loss: 0.2544 - val_loss: 0.2549\n",
            "Epoch 85/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2460 - val_loss: 0.2819\n",
            "Epoch 86/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.2459 - val_loss: 0.2596\n",
            "Epoch 87/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.2479 - val_loss: 0.2608\n",
            "Epoch 88/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2488 - val_loss: 0.2714\n",
            "Epoch 89/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.2451 - val_loss: 0.2714\n",
            "Epoch 90/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.2360 - val_loss: 0.2499\n",
            "\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "Epoch 91/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 0.2329 - val_loss: 0.2466\n",
            "Epoch 92/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2330 - val_loss: 0.2488\n",
            "Epoch 93/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2328 - val_loss: 0.2387\n",
            "Epoch 94/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2320 - val_loss: 0.2438\n",
            "Epoch 95/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2321 - val_loss: 0.2548\n",
            "Epoch 96/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.2342 - val_loss: 0.2395\n",
            "Epoch 97/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.2301 - val_loss: 0.2436\n",
            "Epoch 98/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.2305 - val_loss: 0.2411\n",
            "Epoch 99/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.2304 - val_loss: 0.2365\n",
            "Epoch 100/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2340 - val_loss: 0.2539\n",
            "Epoch 101/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2249 - val_loss: 0.2502\n",
            "Epoch 102/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.2278 - val_loss: 0.2368\n",
            "Epoch 103/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.2314 - val_loss: 0.2618\n",
            "Epoch 104/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2223 - val_loss: 0.2395\n",
            "Epoch 105/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2248 - val_loss: 0.2267\n",
            "Epoch 106/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2307 - val_loss: 0.2360\n",
            "Epoch 107/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2220 - val_loss: 0.2464\n",
            "Epoch 108/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2229 - val_loss: 0.2381\n",
            "Epoch 109/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2224 - val_loss: 0.2366\n",
            "Epoch 110/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2206 - val_loss: 0.2346\n",
            "Epoch 111/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.2184 - val_loss: 0.2481\n",
            "Epoch 112/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2222 - val_loss: 0.2463\n",
            "Epoch 113/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2233 - val_loss: 0.2475\n",
            "Epoch 114/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.2247 - val_loss: 0.2339\n",
            "Epoch 115/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.2266 - val_loss: 0.2522\n",
            "\n",
            "Epoch 00115: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "Epoch 116/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.2134 - val_loss: 0.2365\n",
            "Epoch 117/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2185 - val_loss: 0.2344\n",
            "Epoch 118/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2174 - val_loss: 0.2290\n",
            "Epoch 119/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.2132 - val_loss: 0.2256\n",
            "Epoch 120/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2137 - val_loss: 0.2520\n",
            "Epoch 121/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2138 - val_loss: 0.2341\n",
            "Epoch 122/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2098 - val_loss: 0.2518\n",
            "Epoch 123/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.2114 - val_loss: 0.2316\n",
            "Epoch 124/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2195 - val_loss: 0.2376\n",
            "Epoch 125/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.2130 - val_loss: 0.2400\n",
            "Epoch 126/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 0.2100 - val_loss: 0.2434\n",
            "Epoch 127/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2141 - val_loss: 0.2335\n",
            "Epoch 128/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.2121 - val_loss: 0.2222\n",
            "Epoch 129/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2098 - val_loss: 0.2293\n",
            "Epoch 130/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.2138 - val_loss: 0.2163\n",
            "Epoch 131/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2104 - val_loss: 0.2157\n",
            "Epoch 132/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2112 - val_loss: 0.2246\n",
            "Epoch 133/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.2068 - val_loss: 0.2335\n",
            "Epoch 134/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.2102 - val_loss: 0.2525\n",
            "Epoch 135/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.2084 - val_loss: 0.2148\n",
            "Epoch 136/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.2075 - val_loss: 0.2504\n",
            "Epoch 137/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2095 - val_loss: 0.2350\n",
            "Epoch 138/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2069 - val_loss: 0.2209\n",
            "Epoch 139/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.2093 - val_loss: 0.2287\n",
            "Epoch 140/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2078 - val_loss: 0.2208\n",
            "Epoch 141/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2086 - val_loss: 0.2252\n",
            "Epoch 142/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.2101 - val_loss: 0.2300\n",
            "Epoch 143/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.2115 - val_loss: 0.2221\n",
            "Epoch 144/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2063 - val_loss: 0.2212\n",
            "Epoch 145/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.2060 - val_loss: 0.2235\n",
            "\n",
            "Epoch 00145: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "Epoch 146/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2014 - val_loss: 0.2363\n",
            "Epoch 147/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2047 - val_loss: 0.2147\n",
            "Epoch 148/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.2010 - val_loss: 0.2161\n",
            "Epoch 149/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2011 - val_loss: 0.2157\n",
            "Epoch 150/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1998 - val_loss: 0.2208\n",
            "Epoch 151/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.1987 - val_loss: 0.2282\n",
            "Epoch 152/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2018 - val_loss: 0.2202\n",
            "Epoch 153/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.2044 - val_loss: 0.2158\n",
            "Epoch 154/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.2035 - val_loss: 0.2187\n",
            "Epoch 155/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.1992 - val_loss: 0.2234\n",
            "Epoch 156/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.2016 - val_loss: 0.2189\n",
            "Epoch 157/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.2001 - val_loss: 0.2261\n",
            "\n",
            "Epoch 00157: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "Epoch 158/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.1946 - val_loss: 0.2204\n",
            "Epoch 159/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.1946 - val_loss: 0.2160\n",
            "Epoch 160/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.1940 - val_loss: 0.2156\n",
            "Epoch 161/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.1923 - val_loss: 0.2220\n",
            "Epoch 162/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.1965 - val_loss: 0.2119\n",
            "Epoch 163/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.1955 - val_loss: 0.2167\n",
            "Epoch 164/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1942 - val_loss: 0.2267\n",
            "Epoch 165/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.1952 - val_loss: 0.2176\n",
            "Epoch 166/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1932 - val_loss: 0.2232\n",
            "Epoch 167/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.1931 - val_loss: 0.2105\n",
            "Epoch 168/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1925 - val_loss: 0.2186\n",
            "Epoch 169/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.1945 - val_loss: 0.2167\n",
            "Epoch 170/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1925 - val_loss: 0.2210\n",
            "Epoch 171/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.1949 - val_loss: 0.2173\n",
            "Epoch 172/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.1923 - val_loss: 0.2158\n",
            "Epoch 173/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.1920 - val_loss: 0.2147\n",
            "Epoch 174/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.1940 - val_loss: 0.2226\n",
            "Epoch 175/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.1928 - val_loss: 0.2121\n",
            "Epoch 176/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1914 - val_loss: 0.2105\n",
            "Epoch 177/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.1938 - val_loss: 0.2140\n",
            "\n",
            "Epoch 00177: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "Epoch 178/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.1886 - val_loss: 0.2132\n",
            "Epoch 179/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.1901 - val_loss: 0.2169\n",
            "Epoch 180/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.1907 - val_loss: 0.2099\n",
            "Epoch 181/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1897 - val_loss: 0.2209\n",
            "Epoch 182/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1885 - val_loss: 0.2193\n",
            "Epoch 183/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.1894 - val_loss: 0.2116\n",
            "Epoch 184/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.1872 - val_loss: 0.2096\n",
            "Epoch 185/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.1902 - val_loss: 0.2139\n",
            "Epoch 186/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1881 - val_loss: 0.2083\n",
            "Epoch 187/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.1873 - val_loss: 0.2098\n",
            "Epoch 188/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1884 - val_loss: 0.2187\n",
            "Epoch 189/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1889 - val_loss: 0.2109\n",
            "Epoch 190/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.1884 - val_loss: 0.2158\n",
            "Epoch 191/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.1879 - val_loss: 0.2101\n",
            "Epoch 192/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.1883 - val_loss: 0.2064\n",
            "Epoch 193/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.1858 - val_loss: 0.2119\n",
            "Epoch 194/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.1861 - val_loss: 0.2156\n",
            "Epoch 195/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.1870 - val_loss: 0.2111\n",
            "Epoch 196/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.1873 - val_loss: 0.2121\n",
            "Epoch 197/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.1887 - val_loss: 0.2084\n",
            "Epoch 198/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.1861 - val_loss: 0.2128\n",
            "Epoch 199/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.1860 - val_loss: 0.2146\n",
            "Epoch 200/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.1864 - val_loss: 0.2192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5510ec26-4f29-40aa-e082-eeaa6f12bcaf",
        "id": "hW7guqfzZnr7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(history.history['val_loss'][2:])\n",
        "plt.plot(history.history['loss'][2:])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3xUVd7H8c+Zmt4LIQkECF16ERQLii72hgjq2mXXVXctW9z1eVzXfXTXuqurrrqrq2tdXRsqChYUlV4FQgsBUknvyfTz/HEnyYQaICSZye/9evHKzJ07d05uhu+c+Z1z71Vaa4QQQgQ/U3c3QAghROeQQBdCiBAhgS6EECFCAl0IIUKEBLoQQoQIS3e9cFJSks7KyuqulxdCiKC0Zs2aCq118oEe67ZAz8rKYvXq1d318kIIEZSUUnsO9piUXIQQIkRIoAshRIiQQBdCiBAhgS6EECFCAl0IIUKEBLoQQoQICXQhhAgRQRfouyoa+fOnW/H55LS/QggRKOgC/fOcvTz3zU7+/NnW7m6KEEL0KN12pOjRuvmUgRRWN/PCkjyGpEYza0JGdzdJCCF6hKDroSul+P0FI4mPsLJmT1V3N0cIIXqMoAt0ALNJEW414/ZKHV0IIVoEZaADWMwmPF5fdzdDCCF6jCAOdIVbZroIIUSroA10q0l66EIIEShoA91iVlJDF0KIAEEc6Cbc0kMXQohWhw10pdRLSqkypdSmgzyulFJPKaVylVI/KKXGd34zA6x5Gf46mgjlxiM9dCGEaNWRHvrLwMxDPH4OMNj/bx7w92Nv1iG4mqBmDxEmFx6f9NCFEKLFYQNda70EONQRPBcB/9aG5UCcUiqtsxq4H2s4AJHKLTV0IYQI0Bk19HSgIOB+oX/ZfpRS85RSq5VSq8vLy4/u1fyBHmFySw9dCCECdOmgqNb6Ba31RK31xOTk5KPbiD/Qw3FKDV0IIQJ0RqAXAZkB9zP8y44PawRg9NBllosQQrTpjECfD1zjn+0yBajVWpd0wnYPzBIGQBguqaELIUSAw54+Vyn1JnA6kKSUKgR+D1gBtNbPAQuAc4FcoAm4/ng1FmjtoYfjkiNFhRAiwGEDXWs99zCPa+DWTmvR4Vj9PXTllHO5CCFEgOA7UrR1UFR66EIIESgIA90oudhxySwXIYQIEHyB7h8UtePELfPQhRCiVfAFeksPXcs8dCGECBR8gW62gjJh0y48Po0xJiuEECL4Al0psEZg1w4AmYsuhBB+wRfoANZwbNoFIOdzEUIIv+AMdEs4Vp/00IUQIlBwBro1HJt2AshcdCGE8AvaQLf6/IEuR4sKIQQQxIFuaS25SA9dCCEgqAO9peQiPXQhhIBgDXRLOBav0UOXWS5CCGEIzkC3tgW6yyM9dCGEgCAOdLNPeuhCCBEoeAPdK/PQhRAiUNAGuqmlhi6zXIQQAgjWQLeEY/Y6UfhkHroQQvgFZ6D7r1pkXChaeuhCCAFBG+jGOdHD5KpFQgjRKkgD3bhqUTgumeUihBB+QRro/h66cuGSHroQQgBBG+hGDT0cl8xyEUIIv+AMdP+FosOQ64oKIUSL4Az0gJKLW2roQggBBG2gBwyKSg9dCCGAoA30tmmLMg9dCCEMQRro/kFR5ZQjRYUQwi84A93SdqSozHIRQghDcAZ6wKH/Mg9dCCEMQR3okUp66EII0SI4A91kBrONCJNbauhCCOEXnIEOYA0nUjlllosQQvgFb6CHxRKrGmUeuhBC+AVvoEelkqJq5GyLQgjhF9SBnkSNXFNUCCH8gjfQo/uQpKtllosQQvh1KNCVUjOVUtuUUrlKqXsO8Hg/pdRipdQ6pdQPSqlzO7+p+4jqQwwNaLfzuL+UEEIEg8MGulLKDDwDnAOMAOYqpUbss9r/AG9rrccBc4BnO7uh+4lOBSDCVXHcX0oIIYJBR3rok4FcrXWe1toFvAVctM86Gojx344FijuviQcR1cf44a487i8lhBDBoCOBng4UBNwv9C8LdD9wtVKqEFgA3H6gDSml5imlViulVpeXlx9FcwP4e+hRbumhCyEEdN6g6FzgZa11BnAu8KpSar9ta61f0FpP1FpPTE5OPrZX9PfQoz3SQxdCCOhYoBcBmQH3M/zLAt0IvA2gtV4GhAFJndHAg4pMwoeJWAl0IYQAOhboq4DBSqkBSikbxqDn/H3WyQfOBFBKDccI9GOsqRyGyUytKY5Yb1W7xQs2lvDh+n0/b4QQIvRZDreC1tqjlLoNWAiYgZe01puVUg8Aq7XW84G7gX8ope7EGCC9Tmt93I/4qbUkEudt30N/ZeluHB4fF43dt8wvhBCh7bCBDqC1XoAx2Bm47L6A2znAyZ3btMOrsyQQ52j/RcDp8eHyyMFGQojeJ3iPFAXqLUkk6PYlFyPQvd3UIiGE6D5BHegN1kTidC14Pa3LnG4vTumhCyF6oeAOdFsSJjQ0tpVdpOQihOitgjrQm2yJxo3GstZlTo/00IUQvVOHBkV7quaWQG9o66E73D68clk6IUQvFNSB7rAnGDf26aFLoAsheqOgLrm47P6DURuMQPf6NG6vxqeR86QLIXqdoA50bY2gWdtaB0WdAdMVpY4uhOhtgjrQLWYzFToW7S+5ON1tIS4zXYQQvU1QB7rVrKggFl3f0kMPCHQpuQghepmgDnSL2USFjm0dFHW4A0oubgl0IUTvEtyBblKU65iAGnpgD10O/xdC9C5BHehWs4kKYlFNFeDzthsUdUgPXQjRywR1oFvMigodi9I+aKpqF+JSQxdC9DZBHejWlho6QGNZ+2mL0kMXQvQyQR3oo9Jj2wK9oaz9tEXpoQshepmgDvThaTGMGDIIgPrKYhzteugyKCqE6F2COtABrpkxCYCVm7dLD10I0asFfaAPzEjHjQVHdUm7aYtSQxdC9DZBH+goRZ05jnBXVbsDi6SHLoTobYI/0IEaSzLxntJ9euhSQxdC9C4hEehVYf3o6ylqN21ReuhCiN4mJAK9LqI/qVTidTRiUsYyqaELIXqbkAj0xugsACIa9hBps6CU9NCFEL1PSAS6M3YgAOH1u7BbzdjMJjkfuhCi1wmJQPfEZgEQ2bAbu8WE3WKSKxYJIXqdkAj0iKgYinUCcU35hFlN2CxmCXQhRK8TEoEeHWZhly+NVE8hdovZ30OXaYtCiN4lJAI9ym4lT6eR6SvGbjVKLlJDF0L0NiER6NFhFnbpNOJVA4mmBmxSQxdC9EIhEehRdgt5ug8AWbpYeuhCiF4pJAK9pYcOkOErwm4xSw1dCNHrhESgR9otFOpk3NpMX28xNumhCyF6oZAIdKvZhM1qI1+n0MdTKPPQhRC9UkgEOkBUmIU8nUaKq1B66EKIXilkAj3abtTRE50FhJmRHroQotfpUKArpWYqpbYppXKVUvccZJ3ZSqkcpdRmpdQbndvMwzMGRvtg1S6SdaX00IUQvY7lcCsopczAM8BZQCGwSik1X2udE7DOYOC3wMla62qlVMrxavDBRIVZyPP1BSDNW4TTk97VTRBCiG7VkR76ZCBXa52ntXYBbwEX7bPOzcAzWutqAK11Wec28/AC56Knugulhy6E6HU6EujpQEHA/UL/skBDgCFKqe+VUsuVUjM7q4EdFWW3Uk4cbnMEKa58OR+6EKLXOWzJ5Qi2Mxg4HcgAliilRmmtawJXUkrNA+YB9OvXr5Ne2hAdZgEU9ZH9SXbswe3V+HwaU8sljIQQIsR1pIdeBGQG3M/wLwtUCMzXWru11ruA7RgB347W+gWt9USt9cTk5OSjbfMBGYEOVcmTyahdQwJ10ksXQvQqHQn0VcBgpdQApZQNmAPM32edDzB65yilkjBKMHmd2M7DirIbgV4y8HLM2sNl5iUHvK7ozvIGml1yWgAhROg5bKBrrT3AbcBCYAvwttZ6s1LqAaXUhf7VFgKVSqkcYDHwK6115fFq9IFEh1kB8CQOpSxuHHPMi3F6PO3WaXZ5Oe+pb3l1+e7WZUt3VjD6/oXUNrm7srlCCNHpOjQPXWu9QGs9RGs9SGv9oH/ZfVrr+f7bWmt9l9Z6hNZ6lNb6rePZ6AOJ8pdc7BYTu/rPYpCpBF2wst06O8sbcLh9lNY5W5ftKG2gzuFhb52jS9srhBCdLWSOFE2KsgEQE26lps9JAJhKN7ZbZ3tpPQANjraee73D6Jk3OKWHLoQIbiET6FMHJvLWvCmM7BuDjkrFoa2Yqne3W2d7aQMA9QHhXe80wr3BKXV1IURwC5lAV0oxZWAiSilsVgv5OgVL7Z526+zw99Dr2/XQjduNzvb1diGECDYhE+iB7BYze3Qq1rrd7ZZvL9s/0FvKL4FlGCGECEYhGeg2i4k9OhV7fQFoDUCTy0NBVTPQVjcHaGgtuUigCyGCW0gGut0f6GZvMzSUArCzrBGAuAhru/BuGxSVQBdCBLeQDPTBKdE0RRkHt1YXbAXaZriMy4yTGroQIiSFZKCH28z87JKzAHj78yUA5JTUYTObOCE9liaXF6/PKMW0BLr00IUQwS4kAx1gUPZwfJhwlu+kqKaZr7aWceLABGLDjSNKG/YJcgl0IUSwC9lAx2LDG53OmaZ1mP8+lYTKtfxoZB9i/KcIqHe60Vq3BnlnlFzcXl9rz18IIbpa6AY6YE0ayEjTHvo4d3GqeSNnjUhtPStjvcNDs7ut9NIZPfRrXlzJAx9tPubtCCHE0eis86H3TGOvIqcphoS93zE2sprUmLDWc740OD3t56MfY6BrrVlfUIPHJ6fsFUJ0j5DuoTPmCmyX/Z2dvr4MtRsnf2w5K2O9w90a6CYFjcd46H95vZNmt7fdib+EEKIrhXagA9kp0QwbPopUbwnQdt70eoentVeeEh12zD30PVVNAJTWOdBa6uhCiK4X8oEOkJg5FNVYDs56YgJq6C0HFfWJDTvmQ//3VBqB7vT4qGuWGTNCiK7XKwKd+CzjZ/Xu1hp6vcPTGuJ9YsLaDZAGcnl8NLkOH9D5lY2tt+Xc6kKI7tA7Aj1hgPGzejfhVjNmk6LB2VZD7xMbBhx4YPShBVu48h8rDvsSLSUXMMouQgjR1XpHoMf7A71qF0opouwWo+TiD/A0f6AfaC56QVUTOSV1+A4zv3xPZRMZ8eGABLoQonv0jkAPj4OwOKjeBUB0mIWGfWrocOBAr3d6cHl8VDQcevZKQVUTk7MSACirl5kuQoiu1zsCHYyyi/8KRlF2C3X+GnqEzRxw9Oj+gd5SZy+obj7ophucHiobXQxOjSY23Co9dCFEt+g9gR6fBXs3wbrXuNX1Ly4v/QuNDhfRYZbWgdID9dBb6uqF1U37PdZij39AtH9iBH1iwthbK4EuhOh6oX2kaKABp0HOh/DhrVzgX7S0fg5R9mgibYcP9KKag/fQ8/1TFvsnRpASY6dUSi5CiG7Qe3roE6+He/fCbWt4st9TACQ1biMqzNruYKNAWuvWOnvhIUouLSWWtNhwUmPCKJOSixCiG/SeQAew2CEpm6rYkXgw0adpOzEBJZeckjpmP7+MzzbtBYyDhNxeY3bLoQK9stGFSUFcuJXUGDtl9c7DzooRQojO1ntKLgHCI6LYqdPJdO0kym4h0m4G4M2V+TjcPlbuquKus4Zw5Yn9Wp9zqBp6RYOLhEg7JpMiNSYMr09T0egkJTrsuP8uQgjRonf10P3iI6xs9vWnvzuP6DALdosZm9mEw+3j9KHJTOgfz2eb9rbOcEmKslNU3XzQc7RUNTpJjLQBtIZ4mZykSwjRxXploM+Z1I/UwRNJU1Wcm6WgbEtrL/2y8RkMSIqkusnVOiA6PC0ap8dH+UHmolc2uEiMMgI9LsKYAlnX7O6C30QIIdr0ykCPjbBy8rTpAJy+4mZ4dgrTLFuJtJmZMTyVhEgbVY2u1kHSoanRwMHr6FWNLhL8PfTWAVa5pJ0Qoov1ykAHoM9o42f5VjDb+IX5HeZOyiTcZiY+wtauRz4sLQYwjgY9kIoGJ0lRdoDWKyId69kbhRDiSPXeQI9IgIzJMOlmOPtBspt/4H9GlgOQEGmUTVoCfFR6LImRNj7aULzfZlweH3UOz349dLnotBCiq/XeQAe4cRGc9xhMuBZi0mHJYwDERxjh3HLAUEKkjaum9OfLrWXklTe020R1kwugtYYeeIk7IYToSr070JUyflrsMOlG2P0tlG9n4qq7ecz6HPn+Hnp0mIUfT+mP1WTiX9/vbreJlpN2tcxyaZkxs+9BSkIIcbz17kAPNO4aMFnhv9eTsPtjLjR9T0VlBRaTwm4xkRxt58KxffnvmsLWo0fBGBAFSPTX0MHopTc4ZZaLEKJrSaC3iEqGERdC6SZ8EcnYlJehDSuICrOg/D35q6f0p9nt5cP1bbX0ygYj0Ftq6GDU0WVQVAjR1STQA029FaLT0HPfolJHM8O0unWQE2BMRiwj0mJ4c2V+67JKfw89KTKgh263SA1dCNHlJNADpU+Au7dizpzId6aJnGFaT2xbxxulFHNP7Mfm4jo2FtYCUNngxGJSxIS3BX9UmEVq6EKILieBfhCr7VOJUU38xP0qeFytyy8a2xe7xcS7awsBo+SSEGlrLcsAREsPXQjRDToU6EqpmUqpbUqpXKXUPYdY7zKllFZKTey8JnaP7bEn8ZbndC5seg+eGgcf3QH1pcSEWZmWncQXW0rRWlPZ6Go3IAotg6IS6EKIrnXYQFdKmYFngHOAEcBcpdSIA6wXDfwCWNHZjewOsZHh3OOZx9/T/wx9x8L61+HjOwA4a0QqhdXNbN1bT2XAiblayKCoEKI7dKSHPhnI1Vrnaa1dwFvARQdY74/Aw0BIXN2hZdZKfuI0mPM6TP8dbFsA2xdxxvAUTMrHd+u3UtHgbD2oqEVUmEXO5SKE6HIdCfR0oCDgfqF/WSul1HggU2v9yaE2pJSap5RarZRaXV5efsSN7Urx/kBvOTcLU26FxMGw4JekmJt4PvZV5i6/gKqqKob7z/XSItpuweXx4fR4j+q1v8gp5YzHvpaLTQshjsgxD4oqpUzAE8Ddh1tXa/2C1nqi1npicnLysb70cRXvPw1u67RFiw0ufhbq98Lzp3KWYyFRysEjU9zMO2Vgu+e2PKfRaQS61pr8yia+2V5+2JBfkVfJz95YS15FI9tL6zv5txJChLKOBHoRkBlwP8O/rEU0cALwtVJqNzAFmB/sA6Mt53MJnIdO5mS49AWoLUT3m4pGcV58ISaTavfcqDDjw6Cljv7Axzmc+uhirn1pJa8u23PQ1/T5NHe9vYFo/2u2HIUqhBAd0ZFAXwUMVkoNUErZgDnA/JYHtda1WuskrXWW1joLWA5cqLVefVxa3EVaz54Yts9V+kZeDLcsRV39HiplOBTsPwYc5b9YRr3/8P9Fm0uZPCCBmYmlfL6pCI/Xxx1vrWN5XmW7560rqKGopplbp2cDxqXthBCiow4b6FprD3AbsBDYAryttd6slHpAKXXh8W5gd2k5v3lsuHX/B1NHgC0CMiZB4Urw+doe83roU58DaBocHgqqmiiqaWb2IC9/b7yLU4te4PUV+XywvpjXVxhHnDY4Pfh8mgUbS7CZTVw2PgOzSVHVKJexE0J0XIcuEq21XgAs2GfZfQdZ9/Rjb1b3G50Ry6OzRnP60EPU+jNPhLWvwPZPoSwHJlwPn9/H2PWvc7l5Hg3OSazYVQXAVHseCs0N5s8465OZQBzLdlbg9Hg5+4lvyEiIoLCqiVMGJxEbYSU+wiYlFyHEEelQoPdGSikun5h56JUyTzR+vnWl8XPJY+Bx4LXH8Wv9H1Y23MTy3Q7iI6z0bchBm+1YvF5+pt7h7cxfsb6ghn8syaO41kFxrTGj5e6zhwLG6XgrpeQihDgCcuj/sUgcBPEDIG0MXP0u9JsKU26l5rK3SFa1jF15N7YdHzM5Kx5VvBaVPp6NKRcyy/Itj18yGICnvsolMdLGk3PGcsrgJM4emQoYNfxK6aELIY6A9NCPhVJwy1KwhIHJBNkzAAhzevir51JuqfyUh3zfstxqgvwNMOkmxvQ/BfNb7zHIuZX+iRHsqWzigsl9uWhsOheNbZvenxBlI6e47pib+NJ3uwC4YdqAY96WEKJnkx76sbJFGGEeIMJm5knvLE7hZdb7BjE572nwOCB9POask0CZYNe3nDQoCYBLRye3H1gFkiJtVDYc+6Doe+sKW08kJoQIbRLox4FSiii7hbImL+9FXI7J7b8OafoECIuBtLGw+ztuOmUAv505hFEfng1fP9RuGwmRduocHlwe3wFeoeOqG91Sixeil5BAP05aDg4KO+F8SBgEEYkQ1994MGsaFK1mUKyJnwyqQVXvgvVvtPXSm2uYXvICdlytF6E+nH8v282Ggpr9llc1uqhsdKK17oxfSwjRg0mgHyctBySdOaIvzHoJLv1H20Wps04Br8uYw779U2NZXREUrzVur3+d0TtfYKppc4d6116f5oGPcnh9RfujUB1uL81uL26vprZZrnEqRKiTQdHjJMpuIS7CyoT+8WBObP9gvylgtsGaV6BiO/QZDWVbIOcDyJgI24yQH6H2dGguenm9E49PU1rXvuYe2LuvaHASF2Hb96lCiBAiPfTj5JqpWfzunOFYzAfYxWExcOqvYPN7ULoJRl0Og6ZDznxoroH8ZQCMMO2hsgNHixbVNAPsd3bGwA+D8nqpowsR6iTQj5OLx6Uze9IhDkyadif0GWXcHnoOnHAZ1OyBd64FnwdvTAYj1J4OlVyKDxLo1Y1tZZaKTpgxI4To2STQu4vZCrNfhfP/AkmDYdRsGHY+5H0NEYmYxl5Fliqlvq669SmNTg9XPL+MdfnV7TbV0kOvbnLjcLednrdqn5KLECK0SaB3p4QBMPEG47bJBJc8bwyYTrwBlT4Ok9LYKre0rr5sZyUrdlXxn1UF7TbT0kMHo57eokYCXYheRQZFexJ7FFz3sXG71jgYKKZma+vDS3cap9v9amsZPp9uPQ97YKDvrXOQmRABtNXQEyNtVEgNXYiQJz30niomnQZTDJlli/nXi0/T2FDH0p0V2MwmyuqdbCqubV21sLqZzIRwoH0dvbrRRWy4lZSYMOmhC9ELSKD3VEphH3gSp5o3cn3Bvfj+Moop5e9w9YmZKAW5Sz+Ev46CNS9TXNPM+H7xAOytbQv0qiY3CZE2kqJsRxXobq+PBRtL+GprKbVNMo9diJ5OSi49mHXOq1BbwD8//oahuS9yv/Xf1O7dxcwYD5NzloEyoxf9L2bHIwzrk81nlr2UBdTQqxtdxEdYSY6yk1feeMSv/9mmvdz+5joA+idG8PHt04gOO8AFP4QQPYL00Hsyiw0SB3HprKu5zXwfD/quJ6Z0JSPUbp7znM+Gmf8FVyN3Wt4lPT6c06PyOW3bg5C/HDAOLEqItJEUbaeiwYmu3gMlP3T45VfvriLcaubpK8dRUNXE7+dvBqDO4ebSZ79npf/iHUKInkF66EEgIdLGk3PHUdEwEjX2USxexcuPfcPna8N4Mnsu1+x4jdpVP2Om43tsDhe89BH0GcWPaidQnHoVSVE2Ejzl6BfPQjWW45vxAKaTbms7FcFBrCuoYUxmLOeP7sv20gae+nIHl47LoLTOwdr8Gh5csIUPfnYS6jDbORq7Khp5ZelufnfucGwW6XcI0RHyPyVInD40hVkTMsBsJcxm4fYzs1mzp5qfll7KE+5ZxJQsoygsm1n253kt8Xb2Nnj5ufdlfrnrZqaWvMYrtj+Dqwln1hmYPv8fcl/5GXjdkPsl1JWAuxm++AOb5j/JxD98yp7KRnKK61pr87dOH0RchJU3V+WzcPNelIINBTV8vb38uPy+H20o5uWlu1m4ee9x2b4QoUh66EHq8gmZrMirYmNRLcv73YTv6sd446sCVn9fwOraqfzJNo3xnnX83fYSo7Y8QYWKYcfpz7DJNp7KHb9h3u43cD+8AKurBqcpAmtCP0wVWzkBeMOXzkNvP4HHpxnnD3S7xczFY9N5Y0U+SsHcyf1Ysr2cv325g+lDUzr998stM045/PqKPVwwpm+nb1+IUCSBHqRsFhNPzR3XbllqnHEE6fC0GLaU1PEto/n0zM84KTOMk59az29cw9iaX8FXlmto9oZxsmMj75uu5mz3EiZU7CTvlOd45sttPGV9mjnFD/ENdzDZsxo+XwnRaVwx9lJeXrobgAtG9yU9LpxHF26josFJeb2TD9YVcceMIYTbzMf8++WWNaAULM+rIresgeyUKN5YkY/FrJh9uGu9CtFLSaCHkNOHJrO5uI77LxjJuU99S1FNM7FRkaT37cPE/nv475oCaprcnDkslaiM/+XhTSU8OWccuWUNTH1tJQ1fQJhlCtvHRjF9wx/ZYJ6H/T03mCzg8zA87lnO6HM/6+ujmZQV31rbXrmrik837eWjDcVsKKzhxWsnEWk/srdWo9NDuNWMyaTw+TR5FQ1cNKYvn2ws4a2V+dx73nAeX7QNpRSzxme0HlQlhGgjNfQQkp0SzV+uGEtshJXLJmQAEB9pnDJ31oQMdpY3Utno4tQhydw4bQDv/PQk+saFc+qQZP5x3RTCLGYum5DOyAvv4j37xWxK+JFx8evfFsKP34f6vfyl75e8eO1ELGYTozNiCbea+T63gm+2lTE0NZpVu6uZ/fyy1vPLBNJaszyvEp+v7WIbZXUO7np7PaP/sIg3V+UDxrlpHG4fJw5M5JTBySzKKWV7aQOVjS4qGpxsKKxp3d6KvEo+2lDcKddfFSLYSaCHqOtPyuKW0wcxOiMWgHNHpxFmNf7c0wYn7bf+lIGJLL3nDH5/wUhMZhMX/PpfjLvtNePC19ZwGHQGjJ1L7LZ3GJfgAcBqNjGhfzzvri2kzuHhzrOG8M9rJpJf2cRFT39PbZObwuomfvHWOmqb3Xy9vZw5LyxvDW6AP36yhY9/KCHCaubrbcYAa0v9PDsliulDk8mvauLNlcZzlIIvtpQCsGDjXq54YTm3v7mOK55f1u7EZEL0RhLoISo+0sZvZg7DbjHq2TFhVmZNyGDKwARSosMO+hyr//ztVrNp/7LG1NuNKy29cx28eDa8exNzon/A4fZhM5uYNjiJ6cNSeG12JjObP2bhhjxe/n43H2vyRdIAABkQSURBVK4v5r3Ve1iyZhPJVPP8N3l4vD6cHi+Lt5Zx2fh0zh7Zh7V7qtFatwV6chTThxkDrq+v2EN6XDiTsxL4cksZtU1ufj9/M6PSY3lk1mjqnR6W7qzo1H3Y6PTg9cml+0TwkEDvRf540Qm8NW/q0W8gKRtGXgIFy0H7YNcSzt/yS2aY1vD75K+I+vePYO2/Gb1oNv9n/RcnfXEZ7rWvc6X5S8756hx+v/0SVoXdytTaT/h0016W7aykwenhrBGpTMyKp7LRxa6KRnLLGkiMtBFv85IRH8GQ1CjcXs2JAxM4a0QqW/fWc97fvqW6ycWfLh3FRWP7EmW3sGhzaaftK7fXxxmPf82TX+7otG0KcbxJoPcinXIA0KUvwD0FcNMX8IsN+PqM5TnbX7mq5gWozIX5t6NcjSzOvgezp5E/+J7mIeuLlHkjuc99LVXJk3nA+gqfffohny9bS4TNxEkZds7feg/3WN5kS84Gcsvq+T/7v+HhLNjxOecNtHCx6TtOyorh/NF9GZMRy9BUY7zghPRY7BYz04el8HlO6UF71FWNLp79Ohen58BlmcoGJ+sDLrK9oaCG0jon89cXyQW2RdCQWS7iyJitxj8AazimOa9i+vfFMOw8OON/Yct86DuObJXGqY+MYFRkDS/NHcasf1Vis5i596rfop6bxjOOe2A3nB/7I8I+fRv7rk+5yQKWxR8xRGcwWBVCRCK8dRW3WcIw22ppKnERMflxPrxtmvH6rkaoyoO4LH40MpWPNhSzZEc504emUFLbTGKkvXUmzv3zNzN/QzEJETbmTO633691z3sb+TynlFOHJPPIZaP5dodRvtld2cTO8kayU6KOepd9tbWUPy3Yyn9/ehKxEfufC+dvX+6gpM7BQ5eM4q2V+azaXc3js8cc9esB1Da7OffJb/nf80cw84Q+vL+ukLGZ8QxIijym7YqeTQJdHJu4fvDztW33R80CIBOYO3UQ2SlRxGVnccvp27FZTNjj0uDmRWxbNp9lq1ZzXf3HsBnUmffxq63Dydj9LrOt31E97hbiZ/wK3rgcsy0KYjOJWPdP8DWBNQx2LTG+EQCkjeGMMx8kKcrGDS+vIisxkl0VjZw3Ko1nrhrPdzsqmL+hGKtZ8eJ3u7hiUma7bytldQ6+2lrG5KwEVu2q4oGPN7O31kFmQjgFVc18saW0XaB/sK4Iq9nEeaPT2u2KP3+6ldeX72Fc/3jqHW7sFhOv3DCZF7/bxY6yBv6zOp95pw5q9xytNa+vyKe03sEtpw3iyS93UFLr4NbpgxiYfPQfIp9uLKGoppnvcss5OTuRO/+zgZOzE3n9pilHvU3R80mgi+PmgYtOaL1951lD2h5IHMTQ8+8k+1wNm96Bshw4+U6mhhXxSKmNH13/BJnpxuwcbv7K+On1gKsBcj4EkxkyJ8OYuWCLhGXPEvHmpXxx5QKe3xHNpqJaRqTF8MnGEgYu2sbbqwvonxjBT08bxG/f28jtb65jeV4lYzLiuHpqf7aU1OH1aR6eNZr31xby1Fe5mBTcOj2br7aW8UVOKT89zQjiDQU13PX2egCiwiZz2pBkwLjIyEvf7SI7JYqyOgdhVjPL86r425e5LN1ZiUnBK0v3cMPJA9pdOHxXRSN7/eewv/udDZT4T3+8YGMJt50xGDBm/TyzOJcfCmt4/aYp9Ik98KB2oA/WFwGwpaSeLSX1AHyfW8nGwlpG+Wc+idAjgS66jdmkYPTs1vuzJ2Uya8JBDhoyW2D2Kwfe0KjZ8Nw04j6+md/MWwzhw/B4fRRWN/G3r3LplxDB36+aQHZKFH/5fDsf/1DCaUOMg7Cu/9cqwqwmJg9IYEBSJDdOG8i/lu6m3uFhWnYSJqV46qsdFFQ1kRoTxm/e/YGkKDsJkTZue2Mtl0/IZFJWPIu3laHRvHDNBDLiI9Bac+nfl/L0YuNbxO/OHc6DC7bw9upCrpiUafzuwLI84ypUA5IiWbmrivgIK/0SIvj4hxLG94/nkc+2sb6ghjCrCa3hN+/+wMvXTzrkeEhxTTPL86qwW0xs21vPpiLjYihhVhPPL9nJ01eOP6K/U73DTX5VEyP7ygdBTyeDoqJHOaojQCMTYdZLUJMPfx0DH96KZdFveS3jA97JXsjCk7YwonQ+thV/Y3HSo6wf/gavXJLKkl+dxh2n9yPWW80jEa/Bez8h1q74+RmDSY8LZ1xMPXPGp2A1m/jrFzv4yxfb2bG3hgcvGcU/rpnIyL4xvL5iD7e8vpa3Vxdy+cRMMuKNy/8ppbhjhvGtZHJWAtefnEV2ShS/e38jU/70JV9vKwOMywr2iQnj52dmA3DxuHQuHpfO1r31/PjFlVQ1uvjtOcP47jdncO95w/lmezlvriw48H7we2+tcfnC608eQIPTw6KcvSRF2bh2ahYLNpaQX9l0RLv3N+/+wCXPLqW2WS5y0tOp7hrBnzhxol69enW3vLYIUcXr4fsnYdc34POAzwceB/gCgihlJFTvMpZrX9tyZTLun/JLGD0bveRR1Mb/wuCzeDD29/zz+93cbX6Hm20Lsf/4bRhwKmBMb1y7p5o1+dXMmdSPBP+RuWDUxx9duI3TshM4McNOg4rk621lPP1VLttL67nl9EG8tbKA04Yk89Clo3h80TZumDYAk1Kc+shiJvSP57kfTyDGf1ERn09z1T9XsLm4li/vPp03VuSTGmNvN8i7ubiWS59dysnZSdx+RjaXPLsUgFOHJPPorNFMe/grrpzcj5+cNoj/rinkxmkDiLRbKKtzcP9Hm1FK8dcrxrYej7ChoIaLnvkegCfnjOWisen77falOyuIslsYnRF3yD+P1rpDM61cHh8+rQmzHvs5gUKRUmqN1nriAR+TQBchzeeDpkrwNIMlHKKSjQtwr3nZCHGzFVDG/Prv/gLrXjWeZwmHQdNh2wIc425gxdr1nKbWom1RKGs4zHnDmGVji4KEARAZcPRtQzkUr4W9P0DJBtj9Hbgd8LOlkDCQJpeHe9/fxPvrjDr3o7NGc/k+Jxwrq3eQGGlvLc20yC1r4Jwnl5AUZaek1oHNYmLJr6bTJzaMOoebC/72HQ63l09+fgrhVjMn3L8QreGW0wfxm5nD+PV/NzB/QzHpceHsLG9kxvAULhybzn0fbqLJ6cXl9TF3ciYPXHQCNU1ubn1jLbllDZiUYvKAeJ69akK79nyeU8pPX1uD3WLinZ9ObVeW2VneQGmtg6mDEvnDRzl8nlPK01eOaz2DZwu319f6AeL2+rj8uWXUOdx8eOvJ1Ds81DncDOsT0+E/+d5aR4fGGYKVBLoQHeFqgo/vgKTBMOF6Y9rkO9dBzgd47bG4ptxB+AnnwQvTwb3PJf36jofBZxsfHmv+ZXxDAEgYCJlTYPP7MPJiuPBpKF4HUSlsbophUU45PzltIBG2AwxnVe40rj419sp2FyP586dbee6bnVwyLp2Pfyhm9sRM/u/iE/jpa2v4YksZ/5k3hYlZCQCc/uhidlc28fSV4zh/dF9yy+qZ8cQSrGbF5RMzeWOFcUqFMZlxPH75GN5fV8gzi3diNim01vg0PHjJCWwuruODdUWs/d+zWnvOn24s4Rf/Wc+wPtGU1zvRGl68biLZKVH8e+keHl20DZfHx4i0GHJK6oi2W3B5fdx/4UjmTDI+wB5duI1Xlu7m9ZunMDYzjqe+3METn29HKThxQAI5xXV4fJpvfjUdu9XE1pJ6hqdF89J3u1m5u5IfTzGmrLb0/P/25Q4e/3w7f7liDJeMyzjkn3vx1jKe/HIHYzJi+fHUrENOTa1qdFHd5GLQMcw8AuPbx3Pf7OSKSZmkxhzdh44EuhBHy91shGq/qcZ0SYCyLbB3I0SnGY+XbIAdi6BwldHrn3CtMVCbOhLC/D3LRf8Dy56BtDFGoINR/pn5EESlGh8mZguknmDM4mkog3+cAbUFMOE6OO8JY3nVLtxasaY2mhMHJHDfB5v4ZtVaUjMGsSq/lnvPHc7Npw5sbf4tr63h0017+eru01qnQb6ydDf9EiKYPiyFf36bh0/r1tk3Pp9mwaYStpbUYzErzjkhjaF9ovlmeznXvrSSGcNTcHp8eH2apTsrGZMZx7+um0RpnYMfv7iS6iYXMWEWqpvczBieyoi+MTy7OJe5k/txx4zB/PytdXyfW8mo9Fgi7cZMIJvZRGZCOLdOz+bX//2Bc0alMaxPNI8u3EZ2ShS7KhqZPTGDLSX17Q7+So62U17vJDHSxoi+MfSNDec/qwuwW0yE28x8cddpJEUZl1/cXdFIuM3MC0vy2F3ZxBOzx3DlP5bj9mqaXV4i7Wbeu+Vk+iVG4PNpdpYbp2xWStHg9HDh099RVN3Mm/OmUFbn4IfCWn5+5uB2ZaGluRV8saWMm08dQFpseLu3kcPtZXleJQ9/to0tJXX8/oIRXH/ygKN6S0qgC9EVGiuNc93EpB3gsQp40n+w0Fl/AK2Nen/tPgOcaWONmT/r34SqnTDyUlj/mvFBkDISNrwJaEgdBcPOxb1zCdbCZew0D6Qo+VROybSixl0NKSMg9wsW7tE8tjGMhXedgclZC3VFxmM+rzGWEJ9llJ0ctVC0FgpXG6878QZjaqify+Pj1EcW0+j0MDA5EqfHx/RhKdw5Y0jrwVs1TS6e+Hw7lQ0u5k7ux8nZiSilaHR6Wk+n7PNp3lyVz9urC2lwuDl/dF/G94/n2pdWAjCuXxwvXTuJ2HArX28vY/KARB78JKd1IPjXM4dS7/Bw4oAEpmUn8eH6YpblVZJTXMeOsnqmDkrinpnDuPiZ7xmQFMnojFg++qEYh9sYLwmzmlAovFrj9vp475aTiA6zMuu5pUSHWbjupAEs2ryXFbuqmDUhgzvPGsL/fZzDopxSUqLtVDe5Wrc1JjOOZ68aT5Tdwq//u4GF/lNPRIdZmJSVQHWTsR/qHR4eX7SNJpeXpCg7D11yAmeP7HPUb7NjDnSl1EzgScAM/FNr/ed9Hr8LuAnwAOXADVrrPYfapgS66HVKc8AeDXH+ermrEbZ8BGabMZ++rhi+eRjqSyC6L5z/BAw9xwj3bx8zjoqddLPx/K2fGN8cwuONHnzOB1C1y9iWzwNRKcZ2wBgPiM0wAtzngcRscNRBYxlYI4yzaTZVtrXTFm2MOUyeBzF9jQ+V2HTczXWYrRGYLIeY7ezzGpc2tB5ZOeEfS/Lwas1N09rP0wcoqW3m0meXcv3JWfsdmBXI4/W1PvfD9UW8+N0utu2t55wT+nDe6L5UN7k4ZXASuyuauP7llcyZ1I/7LxwJwNr8an71zgZ2ljcSE2ZhxohU3ltb1Lrte84ZxtkjUvnJq2s4e2QqI/vG8st3NuD1aRIjbZQ3OLljxhBmDE/loQVb2Os/nmBbqXEMwPShyVxzUhZTBiQe8wVgjinQlVJmYDtwFlAIrALmaq1zAtaZDqzQWjcppW4BTtdaX3Go7UqgC3EA7mYjXGPS21/E2+cz6vb26LZlTVVgsRsfBlobYe1qgC//aIT3pJuM7RWthZo9RpDH9TPq+WGxkH0mlG8zZvzE9Ye+Y42xADR8eBts/dh4Hbt/3S3zIX4AnPxzox1RfdrKSvWlxtjB6pegodQoR510O5wwC/IWQ2QyZExqK0EdoY7OkOno8+ocbqLtlv0ey69sIjbcSmyElSXby9leWs/kAQkHnMFTWN3E44u2sza/mscuH8Mk/7hF4Gt/tsm4Ju7ME/p02sXUjzXQpwL3a61/5L//W39j/3SQ9ccBT2utTz7UdiXQhejhfF6j1//hrUbdf8wVkL8CKra1Xy+un3GhcZ8bss8ygjt/KeR93X49eyzMehEGn7X/a9UWGh9kSUOMbwzioA4V6B05UjQdCCz0FQInHmL9G4FPD9KQecA8gH799j9BkhCiBzGZjVMm37jQOPWC2WL8rNxhfCOoLTAGh0s3w7A0mHQjJPpLIlobPfqyrTDkbGiuhs/vg9cvN2YRmazGAWHJw40a/ornjQ8EZTLq+iarEfJ9x0L6eKOUFJ5glKl2LYHYdONbxfZPITIFRl1ufAso2wxFa4xvEmmjIS4Lcr8wfpeTbje+0YSwjvTQZwEztdY3+e//GDhRa33bAda9GrgNOE1r7TzUdqWHLkQv42qEr/9sfBB43UZpZu8mo14/eg4M+ZFRAirfYnw7iO7j/0aw3Qj7lgPB+ow2wr65ypgSWlsIdYVtrxPbzxg/aD2gTAHaGHCOSTfOHdRUaRw7kDQUkocYH1TOWhg43ShXbfsM0idA1snGB0ZtIdTmQ3ONUeKKSDTGL3we4wNp8NlGGUpr48OrqQocNcY3G1sUjLjQeB4Y62gNpqM7UP9Ye+hFGCfPa5HhX7bvi8wA7qUDYS6E6IVskXD2H9sv87iMYI4+zKwPrY2A1BoiEozAd9b5Q9UL5VvB2WD03GMzjO2WbzHOyJk5xei1f3KXMcbQd5zRm28oNT5Acr8wQtlih3WvGa+XNgbWvgIrn2/fDkuYMeawr5aD1pprjQ+GfX18J4THGR9kjlo473Fjemsn60igrwIGK6UGYAT5HODKwBX8dfPnMXryZZ3eSiFEaLLYDh/mYAwQhwccYWoyt903mY3B2X23mzbG+AdG0I+48MDb9nnbTv1QuNoI3uShxrEBlTuMI39j041SkDXc6M03Vxu9fLMVGsth03tGUIfFGOtFpoAtAvqMgtoi4yyhrgYwWYzt79veTtLRaYvnAn/FmLb4ktb6QaXUA8BqrfV8pdQXwCjAP0+KfK31QfaeQUouQghx5I615ILWegGwYJ9l9wXcnnFMLRRCCHHM5PS5QggRIiTQhRAiREigCyFEiJBAF0KIECGBLoQQIUICXQghQoQEuhBChIhuu8CFUqocOOQ50w8hCajoxOZ0Jmnbkeup7YKe27ae2i6Qth2NI2lXf6118oEe6LZAPxZKqdUHO1Kqu0nbjlxPbRf03Lb11HaBtO1odFa7pOQihBAhQgJdCCFCRLAG+gvd3YBDkLYduZ7aLui5beup7QJp29HolHYFZQ1dCCHE/oK1hy6EEGIfEuhCCBEigi7QlVIzlVLblFK5Sql7urEdmUqpxUqpHKXUZqXUL/zL71dKFSml1vv/ndtN7dutlNrob8Nq/7IEpdTnSqkd/p/xh9vOcWjX0IB9s14pVaeUuqM79ptS6iWlVJlSalPAsgPuI2V4yv+++0EpNb4b2vaoUmqr//XfV0rF+ZdnKaWaA/bdc93QtoP+/ZRSv/Xvt21KqR91cbv+E9Cm3Uqp9f7lXb3PDpYXnft+01oHzT+MKybtBAYCNmADMKKb2pIGjPffjga2AyOA+4Ff9oB9tRtI2mfZI8A9/tv3AA/3gL/nXqB/d+w34FRgPLDpcPsIOBf4FOOKw1OAFd3QtrMBi//2wwFtywpcr5v22wH/fv7/ExsAOzDA///X3FXt2ufxx4H7ummfHSwvOvX9Fmw99MlArtY6T2vtAt4CLuqOhmitS7TWa/2364EtQHp3tOUIXAS84r/9CnBxN7YF4Exgp9b6aI8YPiZa6yVA1T6LD7aPLgL+rQ3LgTilVFpXtk1rvUhr7fHfXY5xwfYud5D9djAXAW9prZ1a611ALsb/4y5tl1JKAbOBN4/Hax/OIfKiU99vwRbo6UBBwP1CekCIKqWygHHACv+i2/xfk17qjrKGnwYWKaXWKKXm+Zelaq1brvu6F0jtnqa1mkP7/2A9Yb8dbB/1tPfeDRg9uBYDlFLrlFLfKKVO6aY2Hejv11P22ylAqdZ6R8Cybtln++RFp77fgi3QexylVBTwLnCH1roO+DswCBiLcdHsx7upadO01uOBc4BblVKnBj6oje913TZnVSllAy4E3vEv6in7rVV376ODUUrdC3iA1/2LSoB+WutxwF3AG0qpmC5uVo/7++1jLu07D92yzw6QF6064/0WbIFeBGQG3M/wL+sWSikrxh/nda31ewBa61KttVdr7QP+wXH6enk4Wusi/88y4H1/O0pbvrb5f5Z1R9v8zgHWaq1LoefsNw6+j3rEe08pdR1wPnCVPwDwlzMq/bfXYNSph3Rluw7x9+v2/aaUsgCXAv9pWdYd++xAeUEnv9+CLdBXAYOVUgP8Pbw5wPzuaIi/JvcisEVr/UTA8sA61yXApn2f2wVti1RKRbfcxhhM24Sxr671r3Yt8GFXty1Aux5TT9hvfgfbR/OBa/yzD6YAtQFflbuEUmom8GvgQq11U8DyZKWU2X97IDAYyOvith3s7zcfmKOUsiulBvjbtrIr2wbMALZqrQtbFnT1PjtYXtDZ77euGuXtxNHiczFGiHcC93ZjO6ZhfD36AVjv/3cu8Cqw0b98PpDWDW0biDGzYAOwuWU/AYnAl8AO4AsgoZv2XSRQCcQGLOvy/YbxgVICuDFqlDcebB9hzDZ4xv++2whM7Ia25WLUVVveb8/5173M/3deD6wFLuiGth307wfc699v24BzurJd/uUvAz/dZ92u3mcHy4tOfb/Jof9CCBEigq3kIoQQ4iAk0IUQIkRIoAshRIiQQBdCiBAhgS6EECFCAl0IIUKEBLoQQoSI/wd0SDdnai5fIQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0fccb447-fb1f-4c07-9322-3fb2097bb5e5",
        "id": "6UJG4rCHZnsF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 0.4956579992915882\n",
            "Std of error: 0.43907955248922814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LDOpKBoAZns5"
      },
      "source": [
        "###KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "77b6b6fc-8cd9-4a9e-8335-cc85fd273fb8",
        "id": "k5IazGZZZns7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
        "\n",
        "knn = KNNR(n_neighbors = 2)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "error_knnr = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_knnr.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_knnr))\n",
        "print('Std of SE error:',np.std(error_knnr))\n",
        "\n",
        "plt.hist(np.asarray(error_knnr), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 0.2680127002389723\n",
            "Std of SE error: 0.5575382776749112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "igI6J8w3wIxN"
      },
      "source": [
        "##Noise : 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d28979b5-457f-4409-8410-1aa9fa0a1afa",
        "id": "n-tXr_2WwIxd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = np.concatenate([n_data_dict[node] for node in nodes])\n",
        "y = [node_to_coordinates[node] for node in nodes]\n",
        "\n",
        "labels = []\n",
        "for label in y:\n",
        "    for i in range(100):\n",
        "        labels.append(label) \n",
        "labels = np.asarray(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3570, 4) (3570, 2)\n",
            "(1530, 4) (1530, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XHN1J8LewIx4"
      },
      "source": [
        "###DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "52VAkgYCwIx7",
        "colab": {}
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(4,))\n",
        "x = Dense(128, activation='relu')(inp)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9b99a623-d03d-4474-ddf6-1ab35f3a8afa",
        "id": "0cRt3PJqwIyI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs = 200, batch_size=16, validation_data=(X_test, y_test), callbacks=[lr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3570 samples, validate on 1530 samples\n",
            "Epoch 1/200\n",
            "3570/3570 [==============================] - 1s 329us/step - loss: 21.3552 - val_loss: 3.4846\n",
            "Epoch 2/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 2.1123 - val_loss: 1.6386\n",
            "Epoch 3/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 1.3559 - val_loss: 1.2735\n",
            "Epoch 4/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 1.2167 - val_loss: 1.2212\n",
            "Epoch 5/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 1.1612 - val_loss: 1.1600\n",
            "Epoch 6/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 1.1006 - val_loss: 1.1401\n",
            "Epoch 7/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 1.0820 - val_loss: 1.0424\n",
            "Epoch 8/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 1.0334 - val_loss: 1.1330\n",
            "Epoch 9/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 1.0139 - val_loss: 1.0387\n",
            "Epoch 10/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.9978 - val_loss: 1.0658\n",
            "Epoch 11/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.9866 - val_loss: 1.0584\n",
            "Epoch 12/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.9507 - val_loss: 1.0185\n",
            "Epoch 13/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.9284 - val_loss: 0.9071\n",
            "Epoch 14/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.9029 - val_loss: 0.9882\n",
            "Epoch 15/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.9159 - val_loss: 1.0030\n",
            "Epoch 16/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.8860 - val_loss: 0.9266\n",
            "Epoch 17/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.8607 - val_loss: 0.8645\n",
            "Epoch 18/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.8736 - val_loss: 0.8540\n",
            "Epoch 19/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.8500 - val_loss: 0.8463\n",
            "Epoch 20/200\n",
            "3570/3570 [==============================] - 1s 303us/step - loss: 0.8578 - val_loss: 0.8144\n",
            "Epoch 21/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.8350 - val_loss: 0.8112\n",
            "Epoch 22/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.8189 - val_loss: 0.8677\n",
            "Epoch 23/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.8202 - val_loss: 0.8273\n",
            "Epoch 24/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.8455 - val_loss: 0.8470\n",
            "Epoch 25/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.8320 - val_loss: 0.8677\n",
            "Epoch 26/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.7916 - val_loss: 0.8378\n",
            "Epoch 27/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.8124 - val_loss: 0.8764\n",
            "Epoch 28/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.8054 - val_loss: 0.8271\n",
            "Epoch 29/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.7989 - val_loss: 0.8724\n",
            "Epoch 30/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.7875 - val_loss: 0.9104\n",
            "Epoch 31/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.8000 - val_loss: 0.8888\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "Epoch 32/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.7594 - val_loss: 0.7739\n",
            "Epoch 33/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.7590 - val_loss: 0.8305\n",
            "Epoch 34/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.7543 - val_loss: 0.8056\n",
            "Epoch 35/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.7625 - val_loss: 0.8156\n",
            "Epoch 36/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 0.7646 - val_loss: 0.7995\n",
            "Epoch 37/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.7621 - val_loss: 0.7737\n",
            "Epoch 38/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.7428 - val_loss: 0.7926\n",
            "Epoch 39/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.7537 - val_loss: 0.8140\n",
            "Epoch 40/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.7451 - val_loss: 0.8208\n",
            "Epoch 41/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.7546 - val_loss: 0.8378\n",
            "Epoch 42/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.7403 - val_loss: 0.8643\n",
            "Epoch 43/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.7567 - val_loss: 0.8305\n",
            "Epoch 44/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.7399 - val_loss: 0.7589\n",
            "Epoch 45/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.7421 - val_loss: 0.7728\n",
            "Epoch 46/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.7352 - val_loss: 0.7856\n",
            "Epoch 47/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.7466 - val_loss: 0.7883\n",
            "Epoch 48/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.7501 - val_loss: 0.8372\n",
            "Epoch 49/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.7314 - val_loss: 0.7876\n",
            "Epoch 50/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.7357 - val_loss: 0.8029\n",
            "Epoch 51/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.7281 - val_loss: 0.7536\n",
            "Epoch 52/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.7333 - val_loss: 0.7791\n",
            "Epoch 53/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.7183 - val_loss: 0.7767\n",
            "Epoch 54/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.7142 - val_loss: 0.8785\n",
            "Epoch 55/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.7133 - val_loss: 0.7807\n",
            "Epoch 56/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.7261 - val_loss: 0.8005\n",
            "Epoch 57/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.7073 - val_loss: 0.7667\n",
            "Epoch 58/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.7185 - val_loss: 0.7804\n",
            "Epoch 59/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.7029 - val_loss: 0.8397\n",
            "Epoch 60/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.7207 - val_loss: 0.7931\n",
            "Epoch 61/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.7038 - val_loss: 0.8006\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "Epoch 62/200\n",
            "3570/3570 [==============================] - 1s 303us/step - loss: 0.6990 - val_loss: 0.7704\n",
            "Epoch 63/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6960 - val_loss: 0.7350\n",
            "Epoch 64/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6988 - val_loss: 0.7416\n",
            "Epoch 65/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6876 - val_loss: 0.7576\n",
            "Epoch 66/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6905 - val_loss: 0.8076\n",
            "Epoch 67/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.7075 - val_loss: 0.7773\n",
            "Epoch 68/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.6871 - val_loss: 0.7753\n",
            "Epoch 69/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.6969 - val_loss: 0.7819\n",
            "Epoch 70/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.6839 - val_loss: 0.7510\n",
            "Epoch 71/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6857 - val_loss: 0.7671\n",
            "Epoch 72/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.6801 - val_loss: 0.8162\n",
            "Epoch 73/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.6811 - val_loss: 0.7674\n",
            "\n",
            "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "Epoch 74/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.6680 - val_loss: 0.7550\n",
            "Epoch 75/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6757 - val_loss: 0.7502\n",
            "Epoch 76/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 0.6689 - val_loss: 0.7422\n",
            "Epoch 77/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6649 - val_loss: 0.7882\n",
            "Epoch 78/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6651 - val_loss: 0.7736\n",
            "Epoch 79/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6654 - val_loss: 0.7370\n",
            "Epoch 80/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6644 - val_loss: 0.7464\n",
            "Epoch 81/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6687 - val_loss: 0.7393\n",
            "Epoch 82/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 0.6535 - val_loss: 0.7580\n",
            "Epoch 83/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.6600 - val_loss: 0.7726\n",
            "\n",
            "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "Epoch 84/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6564 - val_loss: 0.7345\n",
            "Epoch 85/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6490 - val_loss: 0.7314\n",
            "Epoch 86/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6511 - val_loss: 0.7359\n",
            "Epoch 87/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.6478 - val_loss: 0.7346\n",
            "Epoch 88/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6574 - val_loss: 0.7370\n",
            "Epoch 89/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6524 - val_loss: 0.7283\n",
            "Epoch 90/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6453 - val_loss: 0.7388\n",
            "Epoch 91/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.6473 - val_loss: 0.7325\n",
            "Epoch 92/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6441 - val_loss: 0.7503\n",
            "Epoch 93/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 0.6445 - val_loss: 0.7836\n",
            "Epoch 94/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.6472 - val_loss: 0.7527\n",
            "Epoch 95/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6428 - val_loss: 0.7308\n",
            "Epoch 96/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6460 - val_loss: 0.7645\n",
            "Epoch 97/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6405 - val_loss: 0.7370\n",
            "Epoch 98/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.6437 - val_loss: 0.7535\n",
            "Epoch 99/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6442 - val_loss: 0.7530\n",
            "\n",
            "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "Epoch 100/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6332 - val_loss: 0.7224\n",
            "Epoch 101/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6313 - val_loss: 0.7333\n",
            "Epoch 102/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.6305 - val_loss: 0.7493\n",
            "Epoch 103/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6349 - val_loss: 0.7350\n",
            "Epoch 104/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6308 - val_loss: 0.7289\n",
            "Epoch 105/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.6341 - val_loss: 0.7401\n",
            "Epoch 106/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.6393 - val_loss: 0.7318\n",
            "Epoch 107/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6324 - val_loss: 0.7285\n",
            "Epoch 108/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6314 - val_loss: 0.7362\n",
            "Epoch 109/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.6305 - val_loss: 0.7318\n",
            "Epoch 110/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.6310 - val_loss: 0.7532\n",
            "\n",
            "Epoch 00110: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "Epoch 111/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6264 - val_loss: 0.7308\n",
            "Epoch 112/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.6237 - val_loss: 0.7313\n",
            "Epoch 113/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6231 - val_loss: 0.7705\n",
            "Epoch 114/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6245 - val_loss: 0.7330\n",
            "Epoch 115/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6215 - val_loss: 0.7318\n",
            "Epoch 116/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.6235 - val_loss: 0.7290\n",
            "Epoch 117/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6256 - val_loss: 0.7374\n",
            "Epoch 118/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 0.6207 - val_loss: 0.7276\n",
            "Epoch 119/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6218 - val_loss: 0.7303\n",
            "Epoch 120/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6312 - val_loss: 0.7390\n",
            "\n",
            "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "Epoch 121/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6187 - val_loss: 0.7413\n",
            "Epoch 122/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6160 - val_loss: 0.7319\n",
            "Epoch 123/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6171 - val_loss: 0.7520\n",
            "Epoch 124/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6162 - val_loss: 0.7308\n",
            "Epoch 125/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6143 - val_loss: 0.7378\n",
            "Epoch 126/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6160 - val_loss: 0.7382\n",
            "Epoch 127/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6161 - val_loss: 0.7276\n",
            "Epoch 128/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6154 - val_loss: 0.7314\n",
            "Epoch 129/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 0.6167 - val_loss: 0.7276\n",
            "Epoch 130/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 0.6144 - val_loss: 0.7261\n",
            "\n",
            "Epoch 00130: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "Epoch 131/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 0.6132 - val_loss: 0.7289\n",
            "Epoch 132/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.6108 - val_loss: 0.7217\n",
            "Epoch 133/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6089 - val_loss: 0.7317\n",
            "Epoch 134/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6136 - val_loss: 0.7379\n",
            "Epoch 135/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6110 - val_loss: 0.7318\n",
            "Epoch 136/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.6088 - val_loss: 0.7352\n",
            "Epoch 137/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6104 - val_loss: 0.7258\n",
            "Epoch 138/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 0.6105 - val_loss: 0.7235\n",
            "Epoch 139/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6121 - val_loss: 0.7295\n",
            "Epoch 140/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6089 - val_loss: 0.7298\n",
            "Epoch 141/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.6107 - val_loss: 0.7323\n",
            "Epoch 142/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6151 - val_loss: 0.7337\n",
            "\n",
            "Epoch 00142: ReduceLROnPlateau reducing learning rate to 7.508467933803331e-05.\n",
            "Epoch 143/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6082 - val_loss: 0.7266\n",
            "Epoch 144/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6068 - val_loss: 0.7279\n",
            "Epoch 145/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6049 - val_loss: 0.7392\n",
            "Epoch 146/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.6058 - val_loss: 0.7229\n",
            "Epoch 147/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.6083 - val_loss: 0.7283\n",
            "Epoch 148/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6059 - val_loss: 0.7274\n",
            "Epoch 149/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 0.6056 - val_loss: 0.7230\n",
            "Epoch 150/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6052 - val_loss: 0.7253\n",
            "Epoch 151/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.6053 - val_loss: 0.7256\n",
            "Epoch 152/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6073 - val_loss: 0.7268\n",
            "\n",
            "Epoch 00152: ReduceLROnPlateau reducing learning rate to 5.6313510867767036e-05.\n",
            "Epoch 153/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6034 - val_loss: 0.7269\n",
            "Epoch 154/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.6039 - val_loss: 0.7279\n",
            "Epoch 155/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.6040 - val_loss: 0.7280\n",
            "Epoch 156/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6029 - val_loss: 0.7266\n",
            "Epoch 157/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6012 - val_loss: 0.7277\n",
            "Epoch 158/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6036 - val_loss: 0.7264\n",
            "Epoch 159/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6030 - val_loss: 0.7268\n",
            "Epoch 160/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6033 - val_loss: 0.7244\n",
            "Epoch 161/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6017 - val_loss: 0.7330\n",
            "Epoch 162/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.6019 - val_loss: 0.7354\n",
            "\n",
            "Epoch 00162: ReduceLROnPlateau reducing learning rate to 4.223513315082528e-05.\n",
            "Epoch 163/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6019 - val_loss: 0.7248\n",
            "Epoch 164/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.5999 - val_loss: 0.7277\n",
            "Epoch 165/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.5998 - val_loss: 0.7288\n",
            "Epoch 166/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.6008 - val_loss: 0.7298\n",
            "Epoch 167/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.6000 - val_loss: 0.7264\n",
            "Epoch 168/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6005 - val_loss: 0.7264\n",
            "Epoch 169/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.6014 - val_loss: 0.7245\n",
            "Epoch 170/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.5999 - val_loss: 0.7251\n",
            "Epoch 171/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.5996 - val_loss: 0.7257\n",
            "Epoch 172/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.6001 - val_loss: 0.7251\n",
            "\n",
            "Epoch 00172: ReduceLROnPlateau reducing learning rate to 3.167634986311896e-05.\n",
            "Epoch 173/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.5980 - val_loss: 0.7242\n",
            "Epoch 174/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.5985 - val_loss: 0.7240\n",
            "Epoch 175/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.5987 - val_loss: 0.7277\n",
            "Epoch 176/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.5984 - val_loss: 0.7242\n",
            "Epoch 177/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.5997 - val_loss: 0.7292\n",
            "Epoch 178/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 0.5983 - val_loss: 0.7278\n",
            "Epoch 179/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 0.6000 - val_loss: 0.7262\n",
            "Epoch 180/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.5992 - val_loss: 0.7258\n",
            "Epoch 181/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 0.5988 - val_loss: 0.7283\n",
            "Epoch 182/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.6011 - val_loss: 0.7251\n",
            "\n",
            "Epoch 00182: ReduceLROnPlateau reducing learning rate to 2.3757263079460245e-05.\n",
            "Epoch 183/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.5968 - val_loss: 0.7269\n",
            "Epoch 184/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.5980 - val_loss: 0.7272\n",
            "Epoch 185/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 0.5962 - val_loss: 0.7254\n",
            "Epoch 186/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.5971 - val_loss: 0.7236\n",
            "Epoch 187/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.5968 - val_loss: 0.7229\n",
            "Epoch 188/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.5973 - val_loss: 0.7254\n",
            "Epoch 189/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 0.5969 - val_loss: 0.7249\n",
            "Epoch 190/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.5963 - val_loss: 0.7259\n",
            "Epoch 191/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 0.5965 - val_loss: 0.7237\n",
            "Epoch 192/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 0.5973 - val_loss: 0.7268\n",
            "\n",
            "Epoch 00192: ReduceLROnPlateau reducing learning rate to 1.781794799171621e-05.\n",
            "Epoch 193/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.5962 - val_loss: 0.7256\n",
            "Epoch 194/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 0.5960 - val_loss: 0.7245\n",
            "Epoch 195/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.5956 - val_loss: 0.7243\n",
            "Epoch 196/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 0.5959 - val_loss: 0.7260\n",
            "Epoch 197/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.5958 - val_loss: 0.7237\n",
            "Epoch 198/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.5955 - val_loss: 0.7232\n",
            "Epoch 199/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 0.5962 - val_loss: 0.7245\n",
            "Epoch 200/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 0.5959 - val_loss: 0.7263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4a8a505e-b517-46b4-e735-3316c8f49c23",
        "id": "9RAWiG_qwIyS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(history.history['val_loss'][2:])\n",
        "plt.plot(history.history['loss'][2:])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "13f23980-2d65-4805-9775-8f8a57e1124b",
        "id": "6BNGcTxIwIyb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 0.9397703456175842\n",
            "Std of error: 0.7546592806414743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v-BhkjPKwIyo"
      },
      "source": [
        "###KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "201fe52d-e054-4cc3-da7e-d6de8467b2a5",
        "id": "yVeoyWpSwIyq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
        "\n",
        "knn = KNNR(n_neighbors = 2)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "error_knnr = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_knnr.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_knnr))\n",
        "print('Std of SE error:',np.std(error_knnr))\n",
        "\n",
        "plt.hist(np.asarray(error_knnr), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 0.9873302727499864\n",
            "Std of SE error: 1.0905937001404087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q3mskUa415Q1"
      },
      "source": [
        "##Noise : 6\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e984c252-5026-44b5-ce68-4268ec7df1c6",
        "id": "uYNz04xR15RE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = np.concatenate([n_data_dict[node] for node in nodes])\n",
        "y = [node_to_coordinates[node] for node in nodes]\n",
        "\n",
        "labels = []\n",
        "for label in y:\n",
        "    for i in range(100):\n",
        "        labels.append(label) \n",
        "labels = np.asarray(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3570, 4) (3570, 2)\n",
            "(1530, 4) (1530, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VtIaIQ1815Rd"
      },
      "source": [
        "###DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "adrDtc6a15Rg",
        "outputId": "f1fd3814-a44d-4061-b6f6-2ee355de2d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(4,))\n",
        "x = Dense(128, activation='relu')(inp)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "17e8a6d0-8e90-495f-d058-768b2292ee6f",
        "id": "0Taf_wpm15Rs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs = 200, batch_size=16, validation_data=(X_test, y_test), callbacks=[lr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3570 samples, validate on 1530 samples\n",
            "Epoch 1/200\n",
            "3570/3570 [==============================] - 2s 677us/step - loss: 20.4270 - val_loss: 4.2300\n",
            "Epoch 2/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 2.9906 - val_loss: 2.5313\n",
            "Epoch 3/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 2.1883 - val_loss: 2.0369\n",
            "Epoch 4/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 1.9405 - val_loss: 1.8710\n",
            "Epoch 5/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.7925 - val_loss: 1.7638\n",
            "Epoch 6/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.7255 - val_loss: 1.7649\n",
            "Epoch 7/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.6948 - val_loss: 1.6824\n",
            "Epoch 8/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.6855 - val_loss: 1.6517\n",
            "Epoch 9/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.6171 - val_loss: 1.8269\n",
            "Epoch 10/200\n",
            "3570/3570 [==============================] - 1s 201us/step - loss: 1.6313 - val_loss: 1.5914\n",
            "Epoch 11/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 1.6073 - val_loss: 1.5581\n",
            "Epoch 12/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 1.5544 - val_loss: 1.5593\n",
            "Epoch 13/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.5615 - val_loss: 1.5002\n",
            "Epoch 14/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 1.5122 - val_loss: 1.6196\n",
            "Epoch 15/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 1.5200 - val_loss: 1.5890\n",
            "Epoch 16/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.5103 - val_loss: 1.6263\n",
            "Epoch 17/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 1.5098 - val_loss: 1.4851\n",
            "Epoch 18/200\n",
            "3570/3570 [==============================] - 1s 224us/step - loss: 1.4902 - val_loss: 1.5513\n",
            "Epoch 19/200\n",
            "3570/3570 [==============================] - 1s 222us/step - loss: 1.4819 - val_loss: 1.4678\n",
            "Epoch 20/200\n",
            "3570/3570 [==============================] - 1s 222us/step - loss: 1.4723 - val_loss: 1.4760\n",
            "Epoch 21/200\n",
            "3570/3570 [==============================] - 1s 219us/step - loss: 1.4629 - val_loss: 1.5032\n",
            "Epoch 22/200\n",
            "3570/3570 [==============================] - 1s 224us/step - loss: 1.4746 - val_loss: 1.5398\n",
            "Epoch 23/200\n",
            "3570/3570 [==============================] - 1s 234us/step - loss: 1.4520 - val_loss: 1.5425\n",
            "Epoch 24/200\n",
            "3570/3570 [==============================] - 1s 229us/step - loss: 1.4503 - val_loss: 1.5028\n",
            "Epoch 25/200\n",
            "3570/3570 [==============================] - 1s 233us/step - loss: 1.4495 - val_loss: 1.4551\n",
            "Epoch 26/200\n",
            "3570/3570 [==============================] - 1s 222us/step - loss: 1.4454 - val_loss: 1.4957\n",
            "Epoch 27/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 1.4416 - val_loss: 1.5838\n",
            "Epoch 28/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.4618 - val_loss: 1.4781\n",
            "Epoch 29/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 1.4004 - val_loss: 1.6650\n",
            "Epoch 30/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.4254 - val_loss: 1.4427\n",
            "Epoch 31/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 1.4215 - val_loss: 1.4516\n",
            "Epoch 32/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.4204 - val_loss: 1.5709\n",
            "Epoch 33/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 1.4306 - val_loss: 1.4917\n",
            "Epoch 34/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.4014 - val_loss: 1.4476\n",
            "Epoch 35/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.3999 - val_loss: 1.5304\n",
            "Epoch 36/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.3906 - val_loss: 1.6756\n",
            "Epoch 37/200\n",
            "3570/3570 [==============================] - 1s 229us/step - loss: 1.4179 - val_loss: 1.4647\n",
            "Epoch 38/200\n",
            "3570/3570 [==============================] - 1s 223us/step - loss: 1.3995 - val_loss: 1.4344\n",
            "Epoch 39/200\n",
            "3570/3570 [==============================] - 1s 235us/step - loss: 1.3811 - val_loss: 1.4834\n",
            "Epoch 40/200\n",
            "3570/3570 [==============================] - 1s 234us/step - loss: 1.3996 - val_loss: 1.5213\n",
            "Epoch 41/200\n",
            "3570/3570 [==============================] - 1s 230us/step - loss: 1.3819 - val_loss: 1.4650\n",
            "Epoch 42/200\n",
            "3570/3570 [==============================] - 1s 217us/step - loss: 1.4192 - val_loss: 1.4838\n",
            "Epoch 43/200\n",
            "3570/3570 [==============================] - 1s 224us/step - loss: 1.3827 - val_loss: 1.6226\n",
            "Epoch 44/200\n",
            "3570/3570 [==============================] - 1s 222us/step - loss: 1.4100 - val_loss: 1.5006\n",
            "Epoch 45/200\n",
            "3570/3570 [==============================] - 1s 221us/step - loss: 1.3850 - val_loss: 1.4727\n",
            "Epoch 46/200\n",
            "3570/3570 [==============================] - 1s 227us/step - loss: 1.3843 - val_loss: 1.4379\n",
            "Epoch 47/200\n",
            "3570/3570 [==============================] - 1s 224us/step - loss: 1.3594 - val_loss: 1.4490\n",
            "Epoch 48/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.3548 - val_loss: 1.4292\n",
            "Epoch 49/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 1.3690 - val_loss: 1.5692\n",
            "Epoch 50/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.3521 - val_loss: 1.4294\n",
            "Epoch 51/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.3489 - val_loss: 1.5981\n",
            "Epoch 52/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 1.3691 - val_loss: 1.4794\n",
            "Epoch 53/200\n",
            "3570/3570 [==============================] - 1s 204us/step - loss: 1.3486 - val_loss: 1.4541\n",
            "Epoch 54/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 1.3526 - val_loss: 1.4074\n",
            "Epoch 55/200\n",
            "3570/3570 [==============================] - 1s 232us/step - loss: 1.3648 - val_loss: 1.4466\n",
            "Epoch 56/200\n",
            "3570/3570 [==============================] - 1s 215us/step - loss: 1.3348 - val_loss: 1.4089\n",
            "Epoch 57/200\n",
            "3570/3570 [==============================] - 1s 201us/step - loss: 1.3525 - val_loss: 1.5009\n",
            "Epoch 58/200\n",
            "3570/3570 [==============================] - 1s 209us/step - loss: 1.3634 - val_loss: 1.4788\n",
            "Epoch 59/200\n",
            "3570/3570 [==============================] - 1s 213us/step - loss: 1.3444 - val_loss: 1.4830\n",
            "Epoch 60/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 1.3557 - val_loss: 1.4609\n",
            "Epoch 61/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 1.3625 - val_loss: 1.4911\n",
            "Epoch 62/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 1.3343 - val_loss: 1.4449\n",
            "Epoch 63/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 1.3407 - val_loss: 1.4507\n",
            "Epoch 64/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 1.3299 - val_loss: 1.4545\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "Epoch 65/200\n",
            "3570/3570 [==============================] - 1s 249us/step - loss: 1.3050 - val_loss: 1.4542\n",
            "Epoch 66/200\n",
            "3570/3570 [==============================] - 1s 239us/step - loss: 1.2945 - val_loss: 1.4128\n",
            "Epoch 67/200\n",
            "3570/3570 [==============================] - 1s 238us/step - loss: 1.3062 - val_loss: 1.4022\n",
            "Epoch 68/200\n",
            "3570/3570 [==============================] - 1s 243us/step - loss: 1.2887 - val_loss: 1.4200\n",
            "Epoch 69/200\n",
            "3570/3570 [==============================] - 1s 240us/step - loss: 1.2975 - val_loss: 1.4287\n",
            "Epoch 70/200\n",
            "3570/3570 [==============================] - 1s 217us/step - loss: 1.3012 - val_loss: 1.4310\n",
            "Epoch 71/200\n",
            "3570/3570 [==============================] - 1s 213us/step - loss: 1.3065 - val_loss: 1.4300\n",
            "Epoch 72/200\n",
            "3570/3570 [==============================] - 1s 218us/step - loss: 1.3097 - val_loss: 1.4460\n",
            "Epoch 73/200\n",
            "3570/3570 [==============================] - 1s 208us/step - loss: 1.2855 - val_loss: 1.4102\n",
            "Epoch 74/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.3004 - val_loss: 1.4061\n",
            "Epoch 75/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.2831 - val_loss: 1.4109\n",
            "Epoch 76/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 1.2950 - val_loss: 1.4321\n",
            "Epoch 77/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.2846 - val_loss: 1.4602\n",
            "\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "Epoch 78/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 1.2743 - val_loss: 1.4077\n",
            "Epoch 79/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 1.2705 - val_loss: 1.4164\n",
            "Epoch 80/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.2563 - val_loss: 1.4635\n",
            "Epoch 81/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 1.2721 - val_loss: 1.3866\n",
            "Epoch 82/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.2598 - val_loss: 1.3867\n",
            "Epoch 83/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.2593 - val_loss: 1.4861\n",
            "Epoch 84/200\n",
            "3570/3570 [==============================] - 1s 214us/step - loss: 1.2665 - val_loss: 1.4041\n",
            "Epoch 85/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 1.2580 - val_loss: 1.4515\n",
            "Epoch 86/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.2745 - val_loss: 1.3835\n",
            "Epoch 87/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 1.2440 - val_loss: 1.5404\n",
            "Epoch 88/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.2656 - val_loss: 1.4354\n",
            "Epoch 89/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.2483 - val_loss: 1.4338\n",
            "Epoch 90/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 1.2695 - val_loss: 1.4215\n",
            "Epoch 91/200\n",
            "3570/3570 [==============================] - 1s 220us/step - loss: 1.2573 - val_loss: 1.4227\n",
            "Epoch 92/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 1.2599 - val_loss: 1.4702\n",
            "Epoch 93/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.2445 - val_loss: 1.4310\n",
            "Epoch 94/200\n",
            "3570/3570 [==============================] - 1s 209us/step - loss: 1.2425 - val_loss: 1.3963\n",
            "Epoch 95/200\n",
            "3570/3570 [==============================] - 1s 208us/step - loss: 1.2352 - val_loss: 1.3831\n",
            "Epoch 96/200\n",
            "3570/3570 [==============================] - 1s 204us/step - loss: 1.2644 - val_loss: 1.4993\n",
            "Epoch 97/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 1.2550 - val_loss: 1.3937\n",
            "Epoch 98/200\n",
            "3570/3570 [==============================] - 1s 214us/step - loss: 1.2349 - val_loss: 1.4462\n",
            "Epoch 99/200\n",
            "3570/3570 [==============================] - 1s 209us/step - loss: 1.2548 - val_loss: 1.4298\n",
            "Epoch 100/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.2619 - val_loss: 1.4081\n",
            "Epoch 101/200\n",
            "3570/3570 [==============================] - 1s 204us/step - loss: 1.2408 - val_loss: 1.4404\n",
            "Epoch 102/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.2502 - val_loss: 1.3911\n",
            "Epoch 103/200\n",
            "3570/3570 [==============================] - 1s 215us/step - loss: 1.2345 - val_loss: 1.4278\n",
            "Epoch 104/200\n",
            "3570/3570 [==============================] - 1s 208us/step - loss: 1.2445 - val_loss: 1.4269\n",
            "Epoch 105/200\n",
            "3570/3570 [==============================] - 1s 216us/step - loss: 1.2489 - val_loss: 1.4208\n",
            "\n",
            "Epoch 00105: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "Epoch 106/200\n",
            "3570/3570 [==============================] - 1s 214us/step - loss: 1.2294 - val_loss: 1.3989\n",
            "Epoch 107/200\n",
            "3570/3570 [==============================] - 1s 212us/step - loss: 1.2074 - val_loss: 1.3723\n",
            "Epoch 108/200\n",
            "3570/3570 [==============================] - 1s 213us/step - loss: 1.2302 - val_loss: 1.4644\n",
            "Epoch 109/200\n",
            "3570/3570 [==============================] - 1s 214us/step - loss: 1.2182 - val_loss: 1.4025\n",
            "Epoch 110/200\n",
            "3570/3570 [==============================] - 1s 219us/step - loss: 1.2115 - val_loss: 1.4539\n",
            "Epoch 111/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.2241 - val_loss: 1.3780\n",
            "Epoch 112/200\n",
            "3570/3570 [==============================] - 1s 208us/step - loss: 1.2145 - val_loss: 1.4435\n",
            "Epoch 113/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 1.2146 - val_loss: 1.4204\n",
            "Epoch 114/200\n",
            "3570/3570 [==============================] - 1s 209us/step - loss: 1.2152 - val_loss: 1.3701\n",
            "Epoch 115/200\n",
            "3570/3570 [==============================] - 1s 210us/step - loss: 1.2020 - val_loss: 1.3929\n",
            "Epoch 116/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.2085 - val_loss: 1.3966\n",
            "Epoch 117/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.2167 - val_loss: 1.4046\n",
            "Epoch 118/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 1.2082 - val_loss: 1.4041\n",
            "Epoch 119/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 1.2069 - val_loss: 1.4087\n",
            "Epoch 120/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 1.2215 - val_loss: 1.3909\n",
            "Epoch 121/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.2240 - val_loss: 1.4104\n",
            "Epoch 122/200\n",
            "3570/3570 [==============================] - 1s 208us/step - loss: 1.2168 - val_loss: 1.3966\n",
            "Epoch 123/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 1.2069 - val_loss: 1.3907\n",
            "Epoch 124/200\n",
            "3570/3570 [==============================] - 1s 221us/step - loss: 1.2044 - val_loss: 1.3744\n",
            "\n",
            "Epoch 00124: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "Epoch 125/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 1.1899 - val_loss: 1.3963\n",
            "Epoch 126/200\n",
            "3570/3570 [==============================] - 1s 212us/step - loss: 1.1929 - val_loss: 1.3856\n",
            "Epoch 127/200\n",
            "3570/3570 [==============================] - 1s 212us/step - loss: 1.1985 - val_loss: 1.3793\n",
            "Epoch 128/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 1.1902 - val_loss: 1.3800\n",
            "Epoch 129/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.1898 - val_loss: 1.4378\n",
            "Epoch 130/200\n",
            "3570/3570 [==============================] - 1s 209us/step - loss: 1.1872 - val_loss: 1.3687\n",
            "Epoch 131/200\n",
            "3570/3570 [==============================] - 1s 215us/step - loss: 1.1897 - val_loss: 1.4255\n",
            "Epoch 132/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 1.1880 - val_loss: 1.3880\n",
            "Epoch 133/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.1892 - val_loss: 1.3676\n",
            "Epoch 134/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 1.1930 - val_loss: 1.3913\n",
            "Epoch 135/200\n",
            "3570/3570 [==============================] - 1s 223us/step - loss: 1.1852 - val_loss: 1.4345\n",
            "Epoch 136/200\n",
            "3570/3570 [==============================] - 1s 216us/step - loss: 1.2037 - val_loss: 1.4007\n",
            "Epoch 137/200\n",
            "3570/3570 [==============================] - 1s 218us/step - loss: 1.1901 - val_loss: 1.3906\n",
            "Epoch 138/200\n",
            "3570/3570 [==============================] - 1s 222us/step - loss: 1.1827 - val_loss: 1.4327\n",
            "Epoch 139/200\n",
            "3570/3570 [==============================] - 1s 218us/step - loss: 1.1910 - val_loss: 1.3698\n",
            "Epoch 140/200\n",
            "3570/3570 [==============================] - 1s 214us/step - loss: 1.1904 - val_loss: 1.4093\n",
            "Epoch 141/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 1.1936 - val_loss: 1.4059\n",
            "Epoch 142/200\n",
            "3570/3570 [==============================] - 1s 215us/step - loss: 1.1773 - val_loss: 1.4541\n",
            "Epoch 143/200\n",
            "3570/3570 [==============================] - 1s 212us/step - loss: 1.1831 - val_loss: 1.3731\n",
            "\n",
            "Epoch 00143: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "Epoch 144/200\n",
            "3570/3570 [==============================] - 1s 216us/step - loss: 1.1754 - val_loss: 1.3714\n",
            "Epoch 145/200\n",
            "3570/3570 [==============================] - 1s 217us/step - loss: 1.1779 - val_loss: 1.3923\n",
            "Epoch 146/200\n",
            "3570/3570 [==============================] - 1s 219us/step - loss: 1.1751 - val_loss: 1.3736\n",
            "Epoch 147/200\n",
            "3570/3570 [==============================] - 1s 216us/step - loss: 1.1739 - val_loss: 1.3859\n",
            "Epoch 148/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 1.1695 - val_loss: 1.4060\n",
            "Epoch 149/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.1716 - val_loss: 1.3797\n",
            "Epoch 150/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.1721 - val_loss: 1.3905\n",
            "Epoch 151/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.1707 - val_loss: 1.3798\n",
            "Epoch 152/200\n",
            "3570/3570 [==============================] - 1s 209us/step - loss: 1.1662 - val_loss: 1.3951\n",
            "Epoch 153/200\n",
            "3570/3570 [==============================] - 1s 208us/step - loss: 1.1733 - val_loss: 1.4165\n",
            "\n",
            "Epoch 00153: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "Epoch 154/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.1686 - val_loss: 1.3836\n",
            "Epoch 155/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.1653 - val_loss: 1.3912\n",
            "Epoch 156/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 1.1594 - val_loss: 1.3867\n",
            "Epoch 157/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.1595 - val_loss: 1.3707\n",
            "Epoch 158/200\n",
            "3570/3570 [==============================] - 1s 201us/step - loss: 1.1595 - val_loss: 1.3852\n",
            "Epoch 159/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 1.1580 - val_loss: 1.3714\n",
            "Epoch 160/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.1592 - val_loss: 1.3723\n",
            "Epoch 161/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.1567 - val_loss: 1.3737\n",
            "Epoch 162/200\n",
            "3570/3570 [==============================] - 1s 208us/step - loss: 1.1559 - val_loss: 1.3813\n",
            "Epoch 163/200\n",
            "3570/3570 [==============================] - 1s 208us/step - loss: 1.1547 - val_loss: 1.3898\n",
            "\n",
            "Epoch 00163: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "Epoch 164/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 1.1531 - val_loss: 1.3867\n",
            "Epoch 165/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 1.1487 - val_loss: 1.3759\n",
            "Epoch 166/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.1522 - val_loss: 1.3641\n",
            "Epoch 167/200\n",
            "3570/3570 [==============================] - 1s 212us/step - loss: 1.1492 - val_loss: 1.4235\n",
            "Epoch 168/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.1544 - val_loss: 1.3799\n",
            "Epoch 169/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 1.1486 - val_loss: 1.3894\n",
            "Epoch 170/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.1485 - val_loss: 1.3758\n",
            "Epoch 171/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.1509 - val_loss: 1.3777\n",
            "Epoch 172/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.1510 - val_loss: 1.3795\n",
            "Epoch 173/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 1.1478 - val_loss: 1.3676\n",
            "Epoch 174/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.1501 - val_loss: 1.3716\n",
            "Epoch 175/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 1.1457 - val_loss: 1.3795\n",
            "Epoch 176/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.1507 - val_loss: 1.3985\n",
            "\n",
            "Epoch 00176: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "Epoch 177/200\n",
            "3570/3570 [==============================] - 1s 204us/step - loss: 1.1450 - val_loss: 1.3698\n",
            "Epoch 178/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 1.1431 - val_loss: 1.3815\n",
            "Epoch 179/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 1.1390 - val_loss: 1.3775\n",
            "Epoch 180/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.1429 - val_loss: 1.3727\n",
            "Epoch 181/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.1442 - val_loss: 1.3699\n",
            "Epoch 182/200\n",
            "3570/3570 [==============================] - 1s 201us/step - loss: 1.1438 - val_loss: 1.3659\n",
            "Epoch 183/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 1.1450 - val_loss: 1.3620\n",
            "Epoch 184/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 1.1410 - val_loss: 1.3828\n",
            "Epoch 185/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.1374 - val_loss: 1.4034\n",
            "Epoch 186/200\n",
            "3570/3570 [==============================] - 1s 210us/step - loss: 1.1448 - val_loss: 1.3674\n",
            "Epoch 187/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 1.1425 - val_loss: 1.3888\n",
            "Epoch 188/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 1.1437 - val_loss: 1.3652\n",
            "Epoch 189/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 1.1426 - val_loss: 1.3660\n",
            "Epoch 190/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.1432 - val_loss: 1.3839\n",
            "Epoch 191/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.1413 - val_loss: 1.3676\n",
            "Epoch 192/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 1.1367 - val_loss: 1.3718\n",
            "Epoch 193/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 1.1409 - val_loss: 1.3767\n",
            "\n",
            "Epoch 00193: ReduceLROnPlateau reducing learning rate to 7.508467933803331e-05.\n",
            "Epoch 194/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.1370 - val_loss: 1.3702\n",
            "Epoch 195/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 1.1369 - val_loss: 1.3719\n",
            "Epoch 196/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 1.1348 - val_loss: 1.3842\n",
            "Epoch 197/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 1.1359 - val_loss: 1.3658\n",
            "Epoch 198/200\n",
            "3570/3570 [==============================] - 1s 209us/step - loss: 1.1356 - val_loss: 1.3818\n",
            "Epoch 199/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 1.1351 - val_loss: 1.3791\n",
            "Epoch 200/200\n",
            "3570/3570 [==============================] - 1s 204us/step - loss: 1.1366 - val_loss: 1.3765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eDwj_BpJ15R2",
        "outputId": "6753379e-05c0-4464-bd74-73637d7e86d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(history.history['val_loss'][2:])\n",
        "plt.plot(history.history['loss'][2:])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6br-ZIRP15SC",
        "outputId": "a5da30b0-179e-4f47-85ca-ea6c5d41a654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 1.3038136416689714\n",
            "Std of error: 1.0262111180453686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fwxS7a2Q15SL"
      },
      "source": [
        "###KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LKatGDAy15SN",
        "outputId": "edcdaa61-8005-44d7-d482-e9cc2e745cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
        "\n",
        "knn = KNNR(n_neighbors = 2)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "error_knnr = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_knnr.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_knnr))\n",
        "print('Std of SE error:',np.std(error_knnr))\n",
        "\n",
        "plt.hist(np.asarray(error_knnr), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 1.5069478162480843\n",
            "Std of SE error: 1.3961397918263114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2PWb2JYSzik",
        "colab_type": "text"
      },
      "source": [
        "##Noise 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFHSdtqlSiio",
        "colab_type": "code",
        "outputId": "2ae531f9-dafb-4f6d-94f4-8afbc6c15391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = np.concatenate([n_data_dict[node] for node in nodes])\n",
        "y = [node_to_coordinates[node] for node in nodes]\n",
        "\n",
        "labels = []\n",
        "for label in y:\n",
        "    for i in range(100):\n",
        "        labels.append(label) \n",
        "labels = np.asarray(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "\n",
        "###DNN\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(4,))\n",
        "x = Dense(128, activation='relu')(inp)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs = 200, batch_size=16, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "plt.plot(history.history['val_loss'][2:])\n",
        "plt.plot(history.history['loss'][2:])\n",
        "plt.show()\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()\n",
        "\n",
        "###KNN\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
        "\n",
        "knn = KNNR(n_neighbors = 2)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "error_knnr = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_knnr.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_knnr))\n",
        "print('Std of SE error:',np.std(error_knnr))\n",
        "\n",
        "plt.hist(np.asarray(error_knnr), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3570, 4) (3570, 2)\n",
            "(1530, 4) (1530, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3570 samples, validate on 1530 samples\n",
            "Epoch 1/200\n",
            "3570/3570 [==============================] - 2s 662us/step - loss: 29.3724 - val_loss: 5.6664\n",
            "Epoch 2/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 4.0400 - val_loss: 3.1780\n",
            "Epoch 3/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 3.1261 - val_loss: 2.7186\n",
            "Epoch 4/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.8728 - val_loss: 2.6381\n",
            "Epoch 5/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.7493 - val_loss: 2.9387\n",
            "Epoch 6/200\n",
            "3570/3570 [==============================] - 1s 184us/step - loss: 2.7866 - val_loss: 2.5213\n",
            "Epoch 7/200\n",
            "3570/3570 [==============================] - 1s 186us/step - loss: 2.6919 - val_loss: 2.6814\n",
            "Epoch 8/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.6339 - val_loss: 2.4917\n",
            "Epoch 9/200\n",
            "3570/3570 [==============================] - 1s 187us/step - loss: 2.6633 - val_loss: 2.6367\n",
            "Epoch 10/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.6241 - val_loss: 2.4678\n",
            "Epoch 11/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.6230 - val_loss: 2.5103\n",
            "Epoch 12/200\n",
            "3570/3570 [==============================] - 1s 187us/step - loss: 2.5897 - val_loss: 2.4997\n",
            "Epoch 13/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.6073 - val_loss: 2.7175\n",
            "Epoch 14/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 2.6367 - val_loss: 2.8838\n",
            "Epoch 15/200\n",
            "3570/3570 [==============================] - 1s 187us/step - loss: 2.6208 - val_loss: 2.4422\n",
            "Epoch 16/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.6182 - val_loss: 2.4399\n",
            "Epoch 17/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.6116 - val_loss: 2.5722\n",
            "Epoch 18/200\n",
            "3570/3570 [==============================] - 1s 187us/step - loss: 2.5527 - val_loss: 2.4966\n",
            "Epoch 19/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 2.5796 - val_loss: 2.5065\n",
            "Epoch 20/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.5529 - val_loss: 2.6937\n",
            "Epoch 21/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 2.5673 - val_loss: 2.7014\n",
            "Epoch 22/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 2.5459 - val_loss: 2.5300\n",
            "Epoch 23/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 2.5354 - val_loss: 2.4890\n",
            "Epoch 24/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.5261 - val_loss: 2.4834\n",
            "Epoch 25/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.5262 - val_loss: 2.4505\n",
            "Epoch 26/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.5102 - val_loss: 2.5464\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "Epoch 27/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.4827 - val_loss: 2.4690\n",
            "Epoch 28/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.4463 - val_loss: 2.3570\n",
            "Epoch 29/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.4503 - val_loss: 2.3271\n",
            "Epoch 30/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.4510 - val_loss: 2.5817\n",
            "Epoch 31/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.4691 - val_loss: 2.3832\n",
            "Epoch 32/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.4399 - val_loss: 2.3553\n",
            "Epoch 33/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 2.4160 - val_loss: 2.4609\n",
            "Epoch 34/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.4378 - val_loss: 2.4012\n",
            "Epoch 35/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.4442 - val_loss: 2.3320\n",
            "Epoch 36/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.4391 - val_loss: 2.4152\n",
            "Epoch 37/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 2.4226 - val_loss: 2.3530\n",
            "Epoch 38/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.4306 - val_loss: 2.4761\n",
            "Epoch 39/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.4303 - val_loss: 2.3103\n",
            "Epoch 40/200\n",
            "3570/3570 [==============================] - 1s 201us/step - loss: 2.4049 - val_loss: 2.3502\n",
            "Epoch 41/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.4202 - val_loss: 2.3716\n",
            "Epoch 42/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.3839 - val_loss: 2.3051\n",
            "Epoch 43/200\n",
            "3570/3570 [==============================] - 1s 187us/step - loss: 2.3851 - val_loss: 2.4028\n",
            "Epoch 44/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.3718 - val_loss: 2.4558\n",
            "Epoch 45/200\n",
            "3570/3570 [==============================] - 1s 188us/step - loss: 2.4180 - val_loss: 2.3931\n",
            "Epoch 46/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.4077 - val_loss: 2.4126\n",
            "Epoch 47/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 2.3740 - val_loss: 2.3826\n",
            "Epoch 48/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.3675 - val_loss: 2.4401\n",
            "Epoch 49/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.3798 - val_loss: 2.3883\n",
            "Epoch 50/200\n",
            "3570/3570 [==============================] - 1s 199us/step - loss: 2.4021 - val_loss: 2.4483\n",
            "Epoch 51/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.3647 - val_loss: 2.3765\n",
            "Epoch 52/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.3670 - val_loss: 2.3589\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "Epoch 53/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 2.3374 - val_loss: 2.4194\n",
            "Epoch 54/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 2.3282 - val_loss: 2.3190\n",
            "Epoch 55/200\n",
            "3570/3570 [==============================] - 1s 201us/step - loss: 2.3355 - val_loss: 2.3670\n",
            "Epoch 56/200\n",
            "3570/3570 [==============================] - 1s 201us/step - loss: 2.3593 - val_loss: 2.2990\n",
            "Epoch 57/200\n",
            "3570/3570 [==============================] - 1s 203us/step - loss: 2.3019 - val_loss: 2.3889\n",
            "Epoch 58/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.3298 - val_loss: 2.3563\n",
            "Epoch 59/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 2.3172 - val_loss: 2.5033\n",
            "Epoch 60/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.3293 - val_loss: 2.3033\n",
            "Epoch 61/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.3267 - val_loss: 2.2847\n",
            "Epoch 62/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.3296 - val_loss: 2.3174\n",
            "Epoch 63/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.3058 - val_loss: 2.3198\n",
            "Epoch 64/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.3343 - val_loss: 2.4393\n",
            "Epoch 65/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.3704 - val_loss: 2.3729\n",
            "Epoch 66/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 2.3047 - val_loss: 2.3444\n",
            "Epoch 67/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.3067 - val_loss: 2.3185\n",
            "Epoch 68/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.3252 - val_loss: 2.3090\n",
            "Epoch 69/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.3089 - val_loss: 2.3762\n",
            "Epoch 70/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 2.3166 - val_loss: 2.4361\n",
            "Epoch 71/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.2949 - val_loss: 2.3158\n",
            "\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "Epoch 72/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.2897 - val_loss: 2.2995\n",
            "Epoch 73/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.2800 - val_loss: 2.3088\n",
            "Epoch 74/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.2745 - val_loss: 2.3911\n",
            "Epoch 75/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 2.2834 - val_loss: 2.3236\n",
            "Epoch 76/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.2653 - val_loss: 2.4145\n",
            "Epoch 77/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 2.2714 - val_loss: 2.3055\n",
            "Epoch 78/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.2722 - val_loss: 2.3064\n",
            "Epoch 79/200\n",
            "3570/3570 [==============================] - 1s 198us/step - loss: 2.2838 - val_loss: 2.3153\n",
            "Epoch 80/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 2.2939 - val_loss: 2.3044\n",
            "Epoch 81/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.2449 - val_loss: 2.3155\n",
            "\n",
            "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "Epoch 82/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.2576 - val_loss: 2.3099\n",
            "Epoch 83/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.2516 - val_loss: 2.3198\n",
            "Epoch 84/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.2439 - val_loss: 2.3058\n",
            "Epoch 85/200\n",
            "3570/3570 [==============================] - 1s 202us/step - loss: 2.2559 - val_loss: 2.3096\n",
            "Epoch 86/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.2586 - val_loss: 2.2894\n",
            "Epoch 87/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.2386 - val_loss: 2.2932\n",
            "Epoch 88/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.2377 - val_loss: 2.2883\n",
            "Epoch 89/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.2429 - val_loss: 2.3007\n",
            "Epoch 90/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.2351 - val_loss: 2.3010\n",
            "Epoch 91/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.2426 - val_loss: 2.3434\n",
            "\n",
            "Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "Epoch 92/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.2282 - val_loss: 2.2938\n",
            "Epoch 93/200\n",
            "3570/3570 [==============================] - 1s 185us/step - loss: 2.2294 - val_loss: 2.2875\n",
            "Epoch 94/200\n",
            "3570/3570 [==============================] - 1s 188us/step - loss: 2.2290 - val_loss: 2.2752\n",
            "Epoch 95/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.2298 - val_loss: 2.3077\n",
            "Epoch 96/200\n",
            "3570/3570 [==============================] - 1s 185us/step - loss: 2.2235 - val_loss: 2.3207\n",
            "Epoch 97/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.2294 - val_loss: 2.2774\n",
            "Epoch 98/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.2156 - val_loss: 2.3017\n",
            "Epoch 99/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.2092 - val_loss: 2.2986\n",
            "Epoch 100/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.2173 - val_loss: 2.2927\n",
            "Epoch 101/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.2086 - val_loss: 2.3031\n",
            "Epoch 102/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 2.2242 - val_loss: 2.3079\n",
            "Epoch 103/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.2249 - val_loss: 2.2985\n",
            "Epoch 104/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 2.2132 - val_loss: 2.3036\n",
            "\n",
            "Epoch 00104: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "Epoch 105/200\n",
            "3570/3570 [==============================] - 1s 185us/step - loss: 2.2047 - val_loss: 2.3105\n",
            "Epoch 106/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.2016 - val_loss: 2.2936\n",
            "Epoch 107/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1997 - val_loss: 2.2929\n",
            "Epoch 108/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1926 - val_loss: 2.3574\n",
            "Epoch 109/200\n",
            "3570/3570 [==============================] - 1s 197us/step - loss: 2.2031 - val_loss: 2.2909\n",
            "Epoch 110/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 2.1976 - val_loss: 2.3086\n",
            "Epoch 111/200\n",
            "3570/3570 [==============================] - 1s 187us/step - loss: 2.2049 - val_loss: 2.2761\n",
            "Epoch 112/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.2029 - val_loss: 2.3162\n",
            "Epoch 113/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1909 - val_loss: 2.2980\n",
            "Epoch 114/200\n",
            "3570/3570 [==============================] - 1s 188us/step - loss: 2.2006 - val_loss: 2.3203\n",
            "\n",
            "Epoch 00114: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "Epoch 115/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1948 - val_loss: 2.2956\n",
            "Epoch 116/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1926 - val_loss: 2.2963\n",
            "Epoch 117/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.1860 - val_loss: 2.3009\n",
            "Epoch 118/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 2.1867 - val_loss: 2.2936\n",
            "Epoch 119/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1849 - val_loss: 2.2840\n",
            "Epoch 120/200\n",
            "3570/3570 [==============================] - 1s 201us/step - loss: 2.1831 - val_loss: 2.2875\n",
            "Epoch 121/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1940 - val_loss: 2.2845\n",
            "Epoch 122/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.1837 - val_loss: 2.2912\n",
            "Epoch 123/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1824 - val_loss: 2.3146\n",
            "Epoch 124/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 2.1872 - val_loss: 2.2920\n",
            "\n",
            "Epoch 00124: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "Epoch 125/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.1829 - val_loss: 2.2897\n",
            "Epoch 126/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.1794 - val_loss: 2.2962\n",
            "Epoch 127/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 2.1731 - val_loss: 2.2850\n",
            "Epoch 128/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1757 - val_loss: 2.2875\n",
            "Epoch 129/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.1800 - val_loss: 2.2832\n",
            "Epoch 130/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1786 - val_loss: 2.3054\n",
            "Epoch 131/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 2.1764 - val_loss: 2.2951\n",
            "Epoch 132/200\n",
            "3570/3570 [==============================] - 1s 186us/step - loss: 2.1860 - val_loss: 2.2942\n",
            "Epoch 133/200\n",
            "3570/3570 [==============================] - 1s 186us/step - loss: 2.1723 - val_loss: 2.2815\n",
            "Epoch 134/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1754 - val_loss: 2.2772\n",
            "\n",
            "Epoch 00134: ReduceLROnPlateau reducing learning rate to 7.508467933803331e-05.\n",
            "Epoch 135/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.1694 - val_loss: 2.2770\n",
            "Epoch 136/200\n",
            "3570/3570 [==============================] - 1s 185us/step - loss: 2.1710 - val_loss: 2.2891\n",
            "Epoch 137/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1684 - val_loss: 2.2799\n",
            "Epoch 138/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.1703 - val_loss: 2.3015\n",
            "Epoch 139/200\n",
            "3570/3570 [==============================] - 1s 188us/step - loss: 2.1685 - val_loss: 2.2809\n",
            "Epoch 140/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1755 - val_loss: 2.2874\n",
            "Epoch 141/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1700 - val_loss: 2.2940\n",
            "Epoch 142/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 2.1696 - val_loss: 2.2801\n",
            "Epoch 143/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 2.1677 - val_loss: 2.2843\n",
            "Epoch 144/200\n",
            "3570/3570 [==============================] - 1s 213us/step - loss: 2.1686 - val_loss: 2.2876\n",
            "\n",
            "Epoch 00144: ReduceLROnPlateau reducing learning rate to 5.6313510867767036e-05.\n",
            "Epoch 145/200\n",
            "3570/3570 [==============================] - 1s 209us/step - loss: 2.1638 - val_loss: 2.2869\n",
            "Epoch 146/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 2.1654 - val_loss: 2.2800\n",
            "Epoch 147/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 2.1626 - val_loss: 2.2902\n",
            "Epoch 148/200\n",
            "3570/3570 [==============================] - 1s 212us/step - loss: 2.1675 - val_loss: 2.2803\n",
            "Epoch 149/200\n",
            "3570/3570 [==============================] - 1s 209us/step - loss: 2.1655 - val_loss: 2.3022\n",
            "Epoch 150/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 2.1663 - val_loss: 2.2862\n",
            "Epoch 151/200\n",
            "3570/3570 [==============================] - 1s 216us/step - loss: 2.1607 - val_loss: 2.2861\n",
            "Epoch 152/200\n",
            "3570/3570 [==============================] - 1s 205us/step - loss: 2.1625 - val_loss: 2.2979\n",
            "Epoch 153/200\n",
            "3570/3570 [==============================] - 1s 206us/step - loss: 2.1617 - val_loss: 2.2847\n",
            "Epoch 154/200\n",
            "3570/3570 [==============================] - 1s 214us/step - loss: 2.1608 - val_loss: 2.2822\n",
            "\n",
            "Epoch 00154: ReduceLROnPlateau reducing learning rate to 4.223513315082528e-05.\n",
            "Epoch 155/200\n",
            "3570/3570 [==============================] - 1s 211us/step - loss: 2.1606 - val_loss: 2.2871\n",
            "Epoch 156/200\n",
            "3570/3570 [==============================] - 1s 207us/step - loss: 2.1563 - val_loss: 2.2817\n",
            "Epoch 157/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.1593 - val_loss: 2.2813\n",
            "Epoch 158/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1603 - val_loss: 2.2836\n",
            "Epoch 159/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1622 - val_loss: 2.2825\n",
            "Epoch 160/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.1595 - val_loss: 2.2882\n",
            "Epoch 161/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1585 - val_loss: 2.2931\n",
            "Epoch 162/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1600 - val_loss: 2.2826\n",
            "Epoch 163/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1588 - val_loss: 2.2879\n",
            "Epoch 164/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.1577 - val_loss: 2.2802\n",
            "\n",
            "Epoch 00164: ReduceLROnPlateau reducing learning rate to 3.167634986311896e-05.\n",
            "Epoch 165/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.1544 - val_loss: 2.2861\n",
            "Epoch 166/200\n",
            "3570/3570 [==============================] - 1s 186us/step - loss: 2.1541 - val_loss: 2.2860\n",
            "Epoch 167/200\n",
            "3570/3570 [==============================] - 1s 189us/step - loss: 2.1533 - val_loss: 2.2839\n",
            "Epoch 168/200\n",
            "3570/3570 [==============================] - 1s 195us/step - loss: 2.1548 - val_loss: 2.2952\n",
            "Epoch 169/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.1558 - val_loss: 2.2843\n",
            "Epoch 170/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1540 - val_loss: 2.2898\n",
            "Epoch 171/200\n",
            "3570/3570 [==============================] - 1s 204us/step - loss: 2.1518 - val_loss: 2.2913\n",
            "Epoch 172/200\n",
            "3570/3570 [==============================] - 1s 188us/step - loss: 2.1544 - val_loss: 2.2830\n",
            "Epoch 173/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.1560 - val_loss: 2.2825\n",
            "Epoch 174/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1550 - val_loss: 2.2828\n",
            "\n",
            "Epoch 00174: ReduceLROnPlateau reducing learning rate to 2.3757263079460245e-05.\n",
            "Epoch 175/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1522 - val_loss: 2.2817\n",
            "Epoch 176/200\n",
            "3570/3570 [==============================] - 1s 188us/step - loss: 2.1540 - val_loss: 2.2847\n",
            "Epoch 177/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1531 - val_loss: 2.2817\n",
            "Epoch 178/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1529 - val_loss: 2.2816\n",
            "Epoch 179/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.1519 - val_loss: 2.2817\n",
            "Epoch 180/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1515 - val_loss: 2.2817\n",
            "Epoch 181/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1529 - val_loss: 2.2828\n",
            "Epoch 182/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1529 - val_loss: 2.2836\n",
            "Epoch 183/200\n",
            "3570/3570 [==============================] - 1s 194us/step - loss: 2.1539 - val_loss: 2.2839\n",
            "Epoch 184/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1530 - val_loss: 2.2853\n",
            "\n",
            "Epoch 00184: ReduceLROnPlateau reducing learning rate to 1.781794799171621e-05.\n",
            "Epoch 185/200\n",
            "3570/3570 [==============================] - 1s 190us/step - loss: 2.1513 - val_loss: 2.2829\n",
            "Epoch 186/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 2.1496 - val_loss: 2.2847\n",
            "Epoch 187/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1495 - val_loss: 2.2821\n",
            "Epoch 188/200\n",
            "3570/3570 [==============================] - 1s 200us/step - loss: 2.1510 - val_loss: 2.2845\n",
            "Epoch 189/200\n",
            "3570/3570 [==============================] - 1s 213us/step - loss: 2.1502 - val_loss: 2.2831\n",
            "Epoch 190/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.1512 - val_loss: 2.2817\n",
            "Epoch 191/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.1514 - val_loss: 2.2823\n",
            "Epoch 192/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1507 - val_loss: 2.2841\n",
            "Epoch 193/200\n",
            "3570/3570 [==============================] - 1s 193us/step - loss: 2.1496 - val_loss: 2.2821\n",
            "Epoch 194/200\n",
            "3570/3570 [==============================] - 1s 188us/step - loss: 2.1499 - val_loss: 2.2841\n",
            "\n",
            "Epoch 00194: ReduceLROnPlateau reducing learning rate to 1.3363460311666131e-05.\n",
            "Epoch 195/200\n",
            "3570/3570 [==============================] - 1s 196us/step - loss: 2.1491 - val_loss: 2.2813\n",
            "Epoch 196/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1486 - val_loss: 2.2835\n",
            "Epoch 197/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1482 - val_loss: 2.2828\n",
            "Epoch 198/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1486 - val_loss: 2.2845\n",
            "Epoch 199/200\n",
            "3570/3570 [==============================] - 1s 192us/step - loss: 2.1492 - val_loss: 2.2833\n",
            "Epoch 200/200\n",
            "3570/3570 [==============================] - 1s 191us/step - loss: 2.1477 - val_loss: 2.2831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean error: 1.6407133514762855\n",
            "Std of error: 1.3690036453377483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean error: 1.874149429584968\n",
            "Std of SE error: 1.6771674671814858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuN6BR-qQpuX",
        "colab_type": "text"
      },
      "source": [
        "##Noise 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUYbwgdEQgQ9",
        "colab_type": "code",
        "outputId": "5127f8b1-87db-43eb-dbf2-d62eefdfa208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = np.concatenate([n_data_dict[node] for node in nodes])\n",
        "y = [node_to_coordinates[node] for node in nodes]\n",
        "\n",
        "labels = []\n",
        "for label in y:\n",
        "    for i in range(100):\n",
        "        labels.append(label) \n",
        "labels = np.asarray(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "\n",
        "###DNN\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(4,))\n",
        "x = Dense(128, activation='relu')(inp)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs = 200, batch_size=16, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "plt.plot(history.history['val_loss'][2:])\n",
        "plt.plot(history.history['loss'][2:])\n",
        "plt.show()\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()\n",
        "\n",
        "###KNN\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
        "\n",
        "knn = KNNR(n_neighbors = 2)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "error_knnr = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_knnr.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_knnr))\n",
        "print('Std of SE error:',np.std(error_knnr))\n",
        "\n",
        "plt.hist(np.asarray(error_knnr), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3570, 4) (3570, 2)\n",
            "(1530, 4) (1530, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3570 samples, validate on 1530 samples\n",
            "Epoch 1/200\n",
            "3570/3570 [==============================] - 3s 742us/step - loss: 87.8104 - val_loss: 87.3876\n",
            "Epoch 2/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 86.8947 - val_loss: 87.3317\n",
            "Epoch 3/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 86.8775 - val_loss: 87.3542\n",
            "Epoch 4/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 86.8663 - val_loss: 87.3154\n",
            "Epoch 5/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 86.8776 - val_loss: 87.2978\n",
            "Epoch 6/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 86.8454 - val_loss: 87.3355\n",
            "Epoch 7/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 86.8542 - val_loss: 87.2950\n",
            "Epoch 8/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 86.8439 - val_loss: 87.2928\n",
            "Epoch 9/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 86.8530 - val_loss: 87.3174\n",
            "Epoch 10/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 86.8396 - val_loss: 87.2900\n",
            "Epoch 11/200\n",
            "3570/3570 [==============================] - 1s 284us/step - loss: 86.8382 - val_loss: 87.3279\n",
            "Epoch 12/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 86.8343 - val_loss: 87.2907\n",
            "Epoch 13/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 86.8287 - val_loss: 87.2946\n",
            "Epoch 14/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 86.8292 - val_loss: 87.3003\n",
            "Epoch 15/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 86.8276 - val_loss: 87.2903\n",
            "Epoch 16/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 86.8296 - val_loss: 87.3152\n",
            "Epoch 17/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 86.8289 - val_loss: 87.3849\n",
            "Epoch 18/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 86.8342 - val_loss: 87.3157\n",
            "Epoch 19/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 86.8257 - val_loss: 87.3003\n",
            "Epoch 20/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 86.8223 - val_loss: 87.2902\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "Epoch 21/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 86.8122 - val_loss: 87.3210\n",
            "Epoch 22/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 86.8103 - val_loss: 87.3444\n",
            "Epoch 23/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 86.8124 - val_loss: 87.2987\n",
            "Epoch 24/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 86.8080 - val_loss: 87.3119\n",
            "Epoch 25/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 86.8078 - val_loss: 87.3352\n",
            "Epoch 26/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 86.8147 - val_loss: 87.3219\n",
            "Epoch 27/200\n",
            "3570/3570 [==============================] - 1s 282us/step - loss: 86.8064 - val_loss: 87.3203\n",
            "Epoch 28/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 86.8021 - val_loss: 87.3369\n",
            "Epoch 29/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 86.8093 - val_loss: 87.3022\n",
            "Epoch 30/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 86.8083 - val_loss: 87.3085\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "Epoch 31/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 86.7911 - val_loss: 87.3045\n",
            "Epoch 32/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 86.7981 - val_loss: 87.3129\n",
            "Epoch 33/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 86.7928 - val_loss: 87.2961\n",
            "Epoch 34/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 86.7956 - val_loss: 87.3023\n",
            "Epoch 35/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 86.7945 - val_loss: 87.2978\n",
            "Epoch 36/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 86.7861 - val_loss: 87.3130\n",
            "Epoch 37/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 86.7927 - val_loss: 87.2993\n",
            "Epoch 38/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 86.7951 - val_loss: 87.3051\n",
            "Epoch 39/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 86.7888 - val_loss: 87.3106\n",
            "Epoch 40/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 86.7872 - val_loss: 87.2981\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "Epoch 41/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 86.7826 - val_loss: 87.3015\n",
            "Epoch 42/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 86.7852 - val_loss: 87.3015\n",
            "Epoch 43/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 57.6518 - val_loss: 5.3794\n",
            "Epoch 44/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 4.1073 - val_loss: 3.6033\n",
            "Epoch 45/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 3.4991 - val_loss: 3.5875\n",
            "Epoch 46/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.3466 - val_loss: 3.3186\n",
            "Epoch 47/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.2466 - val_loss: 3.3344\n",
            "Epoch 48/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 3.2022 - val_loss: 3.2591\n",
            "Epoch 49/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.1729 - val_loss: 3.2180\n",
            "Epoch 50/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.1166 - val_loss: 3.2278\n",
            "Epoch 51/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.0987 - val_loss: 3.1820\n",
            "Epoch 52/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 3.0713 - val_loss: 3.2188\n",
            "Epoch 53/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.0710 - val_loss: 3.1872\n",
            "Epoch 54/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 3.0457 - val_loss: 3.1718\n",
            "Epoch 55/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 3.0288 - val_loss: 3.2148\n",
            "Epoch 56/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.0158 - val_loss: 3.1359\n",
            "Epoch 57/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 3.0037 - val_loss: 3.1359\n",
            "Epoch 58/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 3.0085 - val_loss: 3.1959\n",
            "Epoch 59/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 2.9810 - val_loss: 3.2034\n",
            "Epoch 60/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 2.9728 - val_loss: 3.2247\n",
            "Epoch 61/200\n",
            "3570/3570 [==============================] - 1s 284us/step - loss: 2.9744 - val_loss: 3.1683\n",
            "Epoch 62/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 2.9601 - val_loss: 3.1811\n",
            "Epoch 63/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 2.9635 - val_loss: 3.1745\n",
            "Epoch 64/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.9361 - val_loss: 3.1321\n",
            "Epoch 65/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 2.9465 - val_loss: 3.1755\n",
            "Epoch 66/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.9536 - val_loss: 3.1725\n",
            "Epoch 67/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 2.9271 - val_loss: 3.1566\n",
            "Epoch 68/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.9360 - val_loss: 3.1397\n",
            "Epoch 69/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.9289 - val_loss: 3.1563\n",
            "Epoch 70/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.9260 - val_loss: 3.1296\n",
            "Epoch 71/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.8990 - val_loss: 3.1474\n",
            "Epoch 72/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.8963 - val_loss: 3.1280\n",
            "Epoch 73/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.8978 - val_loss: 3.1788\n",
            "Epoch 74/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 2.9064 - val_loss: 3.2368\n",
            "Epoch 75/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.8887 - val_loss: 3.2389\n",
            "Epoch 76/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.8935 - val_loss: 3.2141\n",
            "Epoch 77/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 2.8692 - val_loss: 3.1237\n",
            "Epoch 78/200\n",
            "3570/3570 [==============================] - 1s 302us/step - loss: 2.8852 - val_loss: 3.1361\n",
            "Epoch 79/200\n",
            "3570/3570 [==============================] - 1s 320us/step - loss: 2.8631 - val_loss: 3.1456\n",
            "Epoch 80/200\n",
            "3570/3570 [==============================] - 1s 307us/step - loss: 2.8539 - val_loss: 3.1175\n",
            "Epoch 81/200\n",
            "3570/3570 [==============================] - 1s 308us/step - loss: 2.8754 - val_loss: 3.1292\n",
            "Epoch 82/200\n",
            "3570/3570 [==============================] - 1s 307us/step - loss: 2.8617 - val_loss: 3.1293\n",
            "Epoch 83/200\n",
            "3570/3570 [==============================] - 1s 314us/step - loss: 2.8708 - val_loss: 3.1292\n",
            "Epoch 84/200\n",
            "3570/3570 [==============================] - 1s 307us/step - loss: 2.8521 - val_loss: 3.1513\n",
            "Epoch 85/200\n",
            "3570/3570 [==============================] - 1s 330us/step - loss: 2.8480 - val_loss: 3.1513\n",
            "Epoch 86/200\n",
            "3570/3570 [==============================] - 1s 309us/step - loss: 2.8496 - val_loss: 3.1809\n",
            "Epoch 87/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 2.8365 - val_loss: 3.1584\n",
            "Epoch 88/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.8281 - val_loss: 3.1482\n",
            "Epoch 89/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.8277 - val_loss: 3.3097\n",
            "Epoch 90/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 2.8371 - val_loss: 3.1932\n",
            "\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "Epoch 91/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.8189 - val_loss: 3.1431\n",
            "Epoch 92/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.8095 - val_loss: 3.1442\n",
            "Epoch 93/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 2.8214 - val_loss: 3.1593\n",
            "Epoch 94/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 2.7917 - val_loss: 3.2325\n",
            "Epoch 95/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 2.8020 - val_loss: 3.1478\n",
            "Epoch 96/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.8009 - val_loss: 3.1941\n",
            "Epoch 97/200\n",
            "3570/3570 [==============================] - 1s 284us/step - loss: 2.8019 - val_loss: 3.1368\n",
            "Epoch 98/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 2.7874 - val_loss: 3.1785\n",
            "Epoch 99/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 2.7854 - val_loss: 3.2011\n",
            "Epoch 100/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.7821 - val_loss: 3.1574\n",
            "\n",
            "Epoch 00100: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "Epoch 101/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.7780 - val_loss: 3.1628\n",
            "Epoch 102/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 2.7614 - val_loss: 3.1526\n",
            "Epoch 103/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 2.7616 - val_loss: 3.1763\n",
            "Epoch 104/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 2.7598 - val_loss: 3.2121\n",
            "Epoch 105/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 2.7710 - val_loss: 3.1532\n",
            "Epoch 106/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 2.7541 - val_loss: 3.1518\n",
            "Epoch 107/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 2.7637 - val_loss: 3.1415\n",
            "Epoch 108/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 2.7491 - val_loss: 3.1526\n",
            "Epoch 109/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 2.7506 - val_loss: 3.1520\n",
            "Epoch 110/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 2.7515 - val_loss: 3.1796\n",
            "\n",
            "Epoch 00110: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "Epoch 111/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 2.7433 - val_loss: 3.1617\n",
            "Epoch 112/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.7386 - val_loss: 3.1453\n",
            "Epoch 113/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 2.7392 - val_loss: 3.1484\n",
            "Epoch 114/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 2.7287 - val_loss: 3.1548\n",
            "Epoch 115/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 2.7351 - val_loss: 3.1456\n",
            "Epoch 116/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 2.7276 - val_loss: 3.1575\n",
            "Epoch 117/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.7279 - val_loss: 3.1703\n",
            "Epoch 118/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 2.7219 - val_loss: 3.1552\n",
            "Epoch 119/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.7212 - val_loss: 3.1420\n",
            "Epoch 120/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 2.7207 - val_loss: 3.1536\n",
            "\n",
            "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "Epoch 121/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 2.7071 - val_loss: 3.2000\n",
            "Epoch 122/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.7158 - val_loss: 3.1875\n",
            "Epoch 123/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.7054 - val_loss: 3.1479\n",
            "Epoch 124/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 2.7048 - val_loss: 3.1461\n",
            "Epoch 125/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.7090 - val_loss: 3.1777\n",
            "Epoch 126/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 2.7126 - val_loss: 3.1674\n",
            "Epoch 127/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.7052 - val_loss: 3.1618\n",
            "Epoch 128/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 2.7009 - val_loss: 3.1755\n",
            "Epoch 129/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.7045 - val_loss: 3.1579\n",
            "Epoch 130/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 2.7016 - val_loss: 3.1872\n",
            "\n",
            "Epoch 00130: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "Epoch 131/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.6976 - val_loss: 3.1531\n",
            "Epoch 132/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 2.6884 - val_loss: 3.1576\n",
            "Epoch 133/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 2.6906 - val_loss: 3.1805\n",
            "Epoch 134/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 2.6914 - val_loss: 3.1581\n",
            "Epoch 135/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.6898 - val_loss: 3.1704\n",
            "Epoch 136/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 2.6845 - val_loss: 3.1827\n",
            "Epoch 137/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 2.6859 - val_loss: 3.1668\n",
            "Epoch 138/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 2.6873 - val_loss: 3.1647\n",
            "Epoch 139/200\n",
            "3570/3570 [==============================] - 1s 302us/step - loss: 2.6863 - val_loss: 3.1619\n",
            "Epoch 140/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 2.6818 - val_loss: 3.1739\n",
            "\n",
            "Epoch 00140: ReduceLROnPlateau reducing learning rate to 7.508467933803331e-05.\n",
            "Epoch 141/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.6783 - val_loss: 3.1623\n",
            "Epoch 142/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.6809 - val_loss: 3.1634\n",
            "Epoch 143/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 2.6772 - val_loss: 3.1819\n",
            "Epoch 144/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.6760 - val_loss: 3.1591\n",
            "Epoch 145/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 2.6773 - val_loss: 3.1584\n",
            "Epoch 146/200\n",
            "3570/3570 [==============================] - 1s 282us/step - loss: 2.6744 - val_loss: 3.1596\n",
            "Epoch 147/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 2.6762 - val_loss: 3.1697\n",
            "Epoch 148/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 2.6712 - val_loss: 3.1603\n",
            "Epoch 149/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.6709 - val_loss: 3.1591\n",
            "Epoch 150/200\n",
            "3570/3570 [==============================] - 1s 304us/step - loss: 2.6686 - val_loss: 3.1670\n",
            "\n",
            "Epoch 00150: ReduceLROnPlateau reducing learning rate to 5.6313510867767036e-05.\n",
            "Epoch 151/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.6679 - val_loss: 3.1697\n",
            "Epoch 152/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 2.6679 - val_loss: 3.1621\n",
            "Epoch 153/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 2.6698 - val_loss: 3.1597\n",
            "Epoch 154/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 2.6637 - val_loss: 3.1668\n",
            "Epoch 155/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.6657 - val_loss: 3.1599\n",
            "Epoch 156/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.6646 - val_loss: 3.1643\n",
            "Epoch 157/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.6645 - val_loss: 3.1618\n",
            "Epoch 158/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 2.6627 - val_loss: 3.1601\n",
            "Epoch 159/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 2.6680 - val_loss: 3.1612\n",
            "Epoch 160/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 2.6605 - val_loss: 3.1656\n",
            "\n",
            "Epoch 00160: ReduceLROnPlateau reducing learning rate to 4.223513315082528e-05.\n",
            "Epoch 161/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 2.6593 - val_loss: 3.1592\n",
            "Epoch 162/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.6573 - val_loss: 3.1622\n",
            "Epoch 163/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 2.6580 - val_loss: 3.1631\n",
            "Epoch 164/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 2.6572 - val_loss: 3.1634\n",
            "Epoch 165/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.6574 - val_loss: 3.1592\n",
            "Epoch 166/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 2.6572 - val_loss: 3.1607\n",
            "Epoch 167/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 2.6591 - val_loss: 3.1603\n",
            "Epoch 168/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 2.6569 - val_loss: 3.1606\n",
            "Epoch 169/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.6578 - val_loss: 3.1618\n",
            "Epoch 170/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 2.6541 - val_loss: 3.1609\n",
            "\n",
            "Epoch 00170: ReduceLROnPlateau reducing learning rate to 3.167634986311896e-05.\n",
            "Epoch 171/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 2.6522 - val_loss: 3.1629\n",
            "Epoch 172/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 2.6531 - val_loss: 3.1618\n",
            "Epoch 173/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.6513 - val_loss: 3.1631\n",
            "Epoch 174/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.6506 - val_loss: 3.1715\n",
            "Epoch 175/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.6514 - val_loss: 3.1675\n",
            "Epoch 176/200\n",
            "3570/3570 [==============================] - 1s 284us/step - loss: 2.6543 - val_loss: 3.1688\n",
            "Epoch 177/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 2.6520 - val_loss: 3.1630\n",
            "Epoch 178/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 2.6516 - val_loss: 3.1653\n",
            "Epoch 179/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.6513 - val_loss: 3.1625\n",
            "Epoch 180/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.6507 - val_loss: 3.1694\n",
            "\n",
            "Epoch 00180: ReduceLROnPlateau reducing learning rate to 2.3757263079460245e-05.\n",
            "Epoch 181/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.6495 - val_loss: 3.1639\n",
            "Epoch 182/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.6478 - val_loss: 3.1636\n",
            "Epoch 183/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.6474 - val_loss: 3.1647\n",
            "Epoch 184/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.6479 - val_loss: 3.1666\n",
            "Epoch 185/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 2.6480 - val_loss: 3.1643\n",
            "Epoch 186/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.6476 - val_loss: 3.1635\n",
            "Epoch 187/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.6482 - val_loss: 3.1638\n",
            "Epoch 188/200\n",
            "3570/3570 [==============================] - 1s 284us/step - loss: 2.6467 - val_loss: 3.1640\n",
            "Epoch 189/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 2.6467 - val_loss: 3.1641\n",
            "Epoch 190/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.6472 - val_loss: 3.1642\n",
            "\n",
            "Epoch 00190: ReduceLROnPlateau reducing learning rate to 1.781794799171621e-05.\n",
            "Epoch 191/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 2.6447 - val_loss: 3.1647\n",
            "Epoch 192/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 2.6442 - val_loss: 3.1676\n",
            "Epoch 193/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 2.6435 - val_loss: 3.1657\n",
            "Epoch 194/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 2.6453 - val_loss: 3.1657\n",
            "Epoch 195/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 2.6458 - val_loss: 3.1652\n",
            "Epoch 196/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 2.6447 - val_loss: 3.1659\n",
            "Epoch 197/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 2.6431 - val_loss: 3.1706\n",
            "Epoch 198/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 2.6441 - val_loss: 3.1653\n",
            "Epoch 199/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 2.6442 - val_loss: 3.1662\n",
            "Epoch 200/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 2.6425 - val_loss: 3.1645\n",
            "\n",
            "Epoch 00200: ReduceLROnPlateau reducing learning rate to 1.3363460311666131e-05.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean error: 1.9403101800328475\n",
            "Std of error: 1.6012975515558763\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean error: 2.2343428633405082\n",
            "Std of SE error: 1.9246083232173525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5542pF0kHXnS"
      },
      "source": [
        "##Noise 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "966f7d41-0a35-47d8-e122-e127309cecc5",
        "id": "VtmQYmMBHXnY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = np.concatenate([n_data_dict[node] for node in nodes])\n",
        "y = [node_to_coordinates[node] for node in nodes]\n",
        "\n",
        "labels = []\n",
        "for label in y:\n",
        "    for i in range(100):\n",
        "        labels.append(label) \n",
        "labels = np.asarray(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "\n",
        "###DNN\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(4,))\n",
        "x = Dense(128, activation='relu')(inp)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(8, activation='relu')(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs = 200, batch_size=16, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "plt.plot(history.history['val_loss'][2:])\n",
        "plt.plot(history.history['loss'][2:])\n",
        "plt.show()\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()\n",
        "\n",
        "###KNN\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor as KNNR\n",
        "\n",
        "knn = KNNR(n_neighbors = 2)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "error_knnr = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_knnr.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_knnr))\n",
        "print('Std of SE error:',np.std(error_knnr))\n",
        "\n",
        "plt.hist(np.asarray(error_knnr), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3570, 4) (3570, 2)\n",
            "(1530, 4) (1530, 2)\n",
            "Train on 3570 samples, validate on 1530 samples\n",
            "Epoch 1/200\n",
            "3570/3570 [==============================] - 1s 325us/step - loss: 30.4179 - val_loss: 7.3086\n",
            "Epoch 2/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 5.6381 - val_loss: 5.4586\n",
            "Epoch 3/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 4.7799 - val_loss: 5.0525\n",
            "Epoch 4/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 4.5795 - val_loss: 5.1625\n",
            "Epoch 5/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 4.4553 - val_loss: 4.9608\n",
            "Epoch 6/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 4.3548 - val_loss: 5.0322\n",
            "Epoch 7/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 4.3583 - val_loss: 4.9601\n",
            "Epoch 8/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 4.3122 - val_loss: 4.8522\n",
            "Epoch 9/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 4.3280 - val_loss: 5.1404\n",
            "Epoch 10/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 4.3487 - val_loss: 4.9533\n",
            "Epoch 11/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 4.3197 - val_loss: 5.1096\n",
            "Epoch 12/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 4.2418 - val_loss: 4.8997\n",
            "Epoch 13/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 4.2686 - val_loss: 4.8969\n",
            "Epoch 14/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 4.2383 - val_loss: 5.1773\n",
            "Epoch 15/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 4.2435 - val_loss: 5.2023\n",
            "Epoch 16/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 4.2078 - val_loss: 4.9403\n",
            "Epoch 17/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 4.2137 - val_loss: 5.1221\n",
            "Epoch 18/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 4.1857 - val_loss: 5.0773\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "Epoch 19/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 4.1555 - val_loss: 4.9745\n",
            "Epoch 20/200\n",
            "3570/3570 [==============================] - 1s 306us/step - loss: 4.1326 - val_loss: 4.9658\n",
            "Epoch 21/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 4.1468 - val_loss: 5.0094\n",
            "Epoch 22/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 4.1130 - val_loss: 4.9043\n",
            "Epoch 23/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 4.1233 - val_loss: 4.9253\n",
            "Epoch 24/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 4.1257 - val_loss: 4.8387\n",
            "Epoch 25/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 4.1061 - val_loss: 4.8698\n",
            "Epoch 26/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 4.0644 - val_loss: 4.9301\n",
            "Epoch 27/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 4.0990 - val_loss: 4.8479\n",
            "Epoch 28/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 4.1009 - val_loss: 5.0188\n",
            "Epoch 29/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 4.0804 - val_loss: 4.8880\n",
            "Epoch 30/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 4.0964 - val_loss: 5.0487\n",
            "Epoch 31/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 4.0641 - val_loss: 4.9891\n",
            "Epoch 32/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 4.0693 - val_loss: 5.0878\n",
            "Epoch 33/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 4.1046 - val_loss: 4.9454\n",
            "Epoch 34/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 4.0852 - val_loss: 4.8747\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "Epoch 35/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 4.0578 - val_loss: 4.9307\n",
            "Epoch 36/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 4.0369 - val_loss: 4.8661\n",
            "Epoch 37/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 4.0478 - val_loss: 4.8902\n",
            "Epoch 38/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 4.0281 - val_loss: 4.9115\n",
            "Epoch 39/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 4.0189 - val_loss: 4.8433\n",
            "Epoch 40/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 4.0236 - val_loss: 4.8642\n",
            "Epoch 41/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.9991 - val_loss: 4.8820\n",
            "Epoch 42/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 3.9927 - val_loss: 4.8712\n",
            "Epoch 43/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.9987 - val_loss: 5.0381\n",
            "Epoch 44/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 4.0350 - val_loss: 4.9901\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "Epoch 45/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.9868 - val_loss: 4.8578\n",
            "Epoch 46/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 3.9832 - val_loss: 4.8823\n",
            "Epoch 47/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 3.9759 - val_loss: 5.0596\n",
            "Epoch 48/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 3.9752 - val_loss: 4.8629\n",
            "Epoch 49/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.9632 - val_loss: 4.9163\n",
            "Epoch 50/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.9851 - val_loss: 4.8586\n",
            "Epoch 51/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 3.9606 - val_loss: 4.8444\n",
            "Epoch 52/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.9676 - val_loss: 4.8879\n",
            "Epoch 53/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 3.9496 - val_loss: 4.8833\n",
            "Epoch 54/200\n",
            "3570/3570 [==============================] - 1s 310us/step - loss: 3.9583 - val_loss: 4.9553\n",
            "\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "Epoch 55/200\n",
            "3570/3570 [==============================] - 1s 318us/step - loss: 3.9382 - val_loss: 4.9283\n",
            "Epoch 56/200\n",
            "3570/3570 [==============================] - 1s 310us/step - loss: 3.9398 - val_loss: 4.9057\n",
            "Epoch 57/200\n",
            "3570/3570 [==============================] - 1s 305us/step - loss: 3.9229 - val_loss: 4.8663\n",
            "Epoch 58/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 3.9269 - val_loss: 4.8963\n",
            "Epoch 59/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.9239 - val_loss: 4.8871\n",
            "Epoch 60/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.9253 - val_loss: 4.8503\n",
            "Epoch 61/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 3.9203 - val_loss: 4.8684\n",
            "Epoch 62/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.9210 - val_loss: 4.8743\n",
            "Epoch 63/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.9266 - val_loss: 4.8750\n",
            "Epoch 64/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.9022 - val_loss: 4.8880\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "Epoch 65/200\n",
            "3570/3570 [==============================] - 1s 303us/step - loss: 3.8778 - val_loss: 4.8754\n",
            "Epoch 66/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.9052 - val_loss: 4.8780\n",
            "Epoch 67/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.8980 - val_loss: 4.8582\n",
            "Epoch 68/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.8865 - val_loss: 4.8744\n",
            "Epoch 69/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.9105 - val_loss: 4.8677\n",
            "Epoch 70/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 3.8795 - val_loss: 4.9409\n",
            "Epoch 71/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 3.8968 - val_loss: 4.8921\n",
            "Epoch 72/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 3.8867 - val_loss: 4.9150\n",
            "Epoch 73/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.8881 - val_loss: 4.9944\n",
            "Epoch 74/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.8810 - val_loss: 4.8713\n",
            "\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "Epoch 75/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 3.8775 - val_loss: 4.8608\n",
            "Epoch 76/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 3.8731 - val_loss: 4.8597\n",
            "Epoch 77/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.8650 - val_loss: 4.9007\n",
            "Epoch 78/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 3.8606 - val_loss: 4.8952\n",
            "Epoch 79/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.8573 - val_loss: 4.9080\n",
            "Epoch 80/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.8503 - val_loss: 4.8854\n",
            "Epoch 81/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 3.8489 - val_loss: 4.9172\n",
            "Epoch 82/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 3.8602 - val_loss: 4.8715\n",
            "Epoch 83/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.8528 - val_loss: 4.8716\n",
            "Epoch 84/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 3.8553 - val_loss: 4.8902\n",
            "\n",
            "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "Epoch 85/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 3.8397 - val_loss: 4.8791\n",
            "Epoch 86/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.8486 - val_loss: 4.8732\n",
            "Epoch 87/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.8331 - val_loss: 4.8864\n",
            "Epoch 88/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.8477 - val_loss: 4.9205\n",
            "Epoch 89/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 3.8343 - val_loss: 4.8650\n",
            "Epoch 90/200\n",
            "3570/3570 [==============================] - 1s 285us/step - loss: 3.8462 - val_loss: 4.8851\n",
            "Epoch 91/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.8358 - val_loss: 4.8728\n",
            "Epoch 92/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 3.8330 - val_loss: 4.8773\n",
            "Epoch 93/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 3.8405 - val_loss: 4.8807\n",
            "Epoch 94/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.8366 - val_loss: 4.9190\n",
            "\n",
            "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "Epoch 95/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.8222 - val_loss: 4.8763\n",
            "Epoch 96/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.8239 - val_loss: 4.8792\n",
            "Epoch 97/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.8237 - val_loss: 4.8827\n",
            "Epoch 98/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 3.8238 - val_loss: 4.8983\n",
            "Epoch 99/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 3.8220 - val_loss: 4.8820\n",
            "Epoch 100/200\n",
            "3570/3570 [==============================] - 1s 301us/step - loss: 3.8238 - val_loss: 4.9252\n",
            "Epoch 101/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.8219 - val_loss: 4.8861\n",
            "Epoch 102/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 3.8186 - val_loss: 4.8810\n",
            "Epoch 103/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.8218 - val_loss: 4.8811\n",
            "Epoch 104/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 3.8242 - val_loss: 4.8820\n",
            "\n",
            "Epoch 00104: ReduceLROnPlateau reducing learning rate to 7.508467933803331e-05.\n",
            "Epoch 105/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.8181 - val_loss: 4.8737\n",
            "Epoch 106/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 3.8105 - val_loss: 4.8853\n",
            "Epoch 107/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.8082 - val_loss: 4.8972\n",
            "Epoch 108/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.8133 - val_loss: 4.8885\n",
            "Epoch 109/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.8127 - val_loss: 4.8834\n",
            "Epoch 110/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 3.8098 - val_loss: 4.9048\n",
            "Epoch 111/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.8094 - val_loss: 4.8928\n",
            "Epoch 112/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 3.8083 - val_loss: 4.8983\n",
            "Epoch 113/200\n",
            "3570/3570 [==============================] - 1s 305us/step - loss: 3.8107 - val_loss: 4.8912\n",
            "Epoch 114/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.8043 - val_loss: 4.9020\n",
            "\n",
            "Epoch 00114: ReduceLROnPlateau reducing learning rate to 5.6313510867767036e-05.\n",
            "Epoch 115/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 3.8046 - val_loss: 4.8891\n",
            "Epoch 116/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.8043 - val_loss: 4.8863\n",
            "Epoch 117/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.8020 - val_loss: 4.8866\n",
            "Epoch 118/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7998 - val_loss: 4.8926\n",
            "Epoch 119/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.8000 - val_loss: 4.9114\n",
            "Epoch 120/200\n",
            "3570/3570 [==============================] - 1s 298us/step - loss: 3.8039 - val_loss: 4.8890\n",
            "Epoch 121/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.7999 - val_loss: 4.8866\n",
            "Epoch 122/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.7956 - val_loss: 4.9038\n",
            "Epoch 123/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 3.7973 - val_loss: 4.8872\n",
            "Epoch 124/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 3.7984 - val_loss: 4.8927\n",
            "\n",
            "Epoch 00124: ReduceLROnPlateau reducing learning rate to 4.223513315082528e-05.\n",
            "Epoch 125/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.7960 - val_loss: 4.8814\n",
            "Epoch 126/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.7941 - val_loss: 4.8884\n",
            "Epoch 127/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7935 - val_loss: 4.8893\n",
            "Epoch 128/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 3.7904 - val_loss: 4.8812\n",
            "Epoch 129/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.7920 - val_loss: 4.8901\n",
            "Epoch 130/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.7881 - val_loss: 4.8890\n",
            "Epoch 131/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.7915 - val_loss: 4.8845\n",
            "Epoch 132/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7916 - val_loss: 4.9068\n",
            "Epoch 133/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.7929 - val_loss: 4.8831\n",
            "Epoch 134/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.7899 - val_loss: 4.8880\n",
            "\n",
            "Epoch 00134: ReduceLROnPlateau reducing learning rate to 3.167634986311896e-05.\n",
            "Epoch 135/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 3.7892 - val_loss: 4.8849\n",
            "Epoch 136/200\n",
            "3570/3570 [==============================] - 1s 284us/step - loss: 3.7894 - val_loss: 4.8869\n",
            "Epoch 137/200\n",
            "3570/3570 [==============================] - 1s 299us/step - loss: 3.7870 - val_loss: 4.8866\n",
            "Epoch 138/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 3.7866 - val_loss: 4.8993\n",
            "Epoch 139/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 3.7899 - val_loss: 4.8884\n",
            "Epoch 140/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.7878 - val_loss: 4.8943\n",
            "Epoch 141/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 3.7840 - val_loss: 4.8867\n",
            "Epoch 142/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.7849 - val_loss: 4.8918\n",
            "Epoch 143/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7863 - val_loss: 4.8856\n",
            "Epoch 144/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 3.7849 - val_loss: 4.8902\n",
            "\n",
            "Epoch 00144: ReduceLROnPlateau reducing learning rate to 2.3757263079460245e-05.\n",
            "Epoch 145/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.7830 - val_loss: 4.8861\n",
            "Epoch 146/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 3.7812 - val_loss: 4.8992\n",
            "Epoch 147/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.7839 - val_loss: 4.8952\n",
            "Epoch 148/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.7819 - val_loss: 4.8941\n",
            "Epoch 149/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 3.7819 - val_loss: 4.8923\n",
            "Epoch 150/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.7819 - val_loss: 4.8892\n",
            "Epoch 151/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.7826 - val_loss: 4.8880\n",
            "Epoch 152/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.7820 - val_loss: 4.8960\n",
            "Epoch 153/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 3.7817 - val_loss: 4.8887\n",
            "Epoch 154/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.7824 - val_loss: 4.8891\n",
            "\n",
            "Epoch 00154: ReduceLROnPlateau reducing learning rate to 1.781794799171621e-05.\n",
            "Epoch 155/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 3.7789 - val_loss: 4.8928\n",
            "Epoch 156/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 3.7794 - val_loss: 4.8899\n",
            "Epoch 157/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 3.7794 - val_loss: 4.8892\n",
            "Epoch 158/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 3.7796 - val_loss: 4.8910\n",
            "Epoch 159/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 3.7785 - val_loss: 4.8905\n",
            "Epoch 160/200\n",
            "3570/3570 [==============================] - 1s 297us/step - loss: 3.7778 - val_loss: 4.8892\n",
            "Epoch 161/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 3.7781 - val_loss: 4.8895\n",
            "Epoch 162/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.7787 - val_loss: 4.8885\n",
            "Epoch 163/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.7785 - val_loss: 4.8901\n",
            "Epoch 164/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.7784 - val_loss: 4.8913\n",
            "\n",
            "Epoch 00164: ReduceLROnPlateau reducing learning rate to 1.3363460311666131e-05.\n",
            "Epoch 165/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 3.7770 - val_loss: 4.8901\n",
            "Epoch 166/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.7780 - val_loss: 4.8888\n",
            "Epoch 167/200\n",
            "3570/3570 [==============================] - 1s 288us/step - loss: 3.7774 - val_loss: 4.8923\n",
            "Epoch 168/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.7760 - val_loss: 4.8911\n",
            "Epoch 169/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7775 - val_loss: 4.8888\n",
            "Epoch 170/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.7765 - val_loss: 4.8925\n",
            "Epoch 171/200\n",
            "3570/3570 [==============================] - 1s 300us/step - loss: 3.7759 - val_loss: 4.8944\n",
            "Epoch 172/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.7763 - val_loss: 4.8926\n",
            "Epoch 173/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7760 - val_loss: 4.8917\n",
            "Epoch 174/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 3.7758 - val_loss: 4.8910\n",
            "\n",
            "Epoch 00174: ReduceLROnPlateau reducing learning rate to 1.0022595233749598e-05.\n",
            "Epoch 175/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7743 - val_loss: 4.8913\n",
            "Epoch 176/200\n",
            "3570/3570 [==============================] - 1s 286us/step - loss: 3.7746 - val_loss: 4.8927\n",
            "Epoch 177/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.7757 - val_loss: 4.8911\n",
            "Epoch 178/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7741 - val_loss: 4.8943\n",
            "Epoch 179/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7752 - val_loss: 4.8907\n",
            "Epoch 180/200\n",
            "3570/3570 [==============================] - 1s 295us/step - loss: 3.7738 - val_loss: 4.8914\n",
            "Epoch 181/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 3.7746 - val_loss: 4.8914\n",
            "Epoch 182/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.7748 - val_loss: 4.8935\n",
            "Epoch 183/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.7739 - val_loss: 4.8897\n",
            "Epoch 184/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.7744 - val_loss: 4.8929\n",
            "\n",
            "Epoch 00184: ReduceLROnPlateau reducing learning rate to 7.516946425312199e-06.\n",
            "Epoch 185/200\n",
            "3570/3570 [==============================] - 1s 283us/step - loss: 3.7727 - val_loss: 4.8932\n",
            "Epoch 186/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.7732 - val_loss: 4.8926\n",
            "Epoch 187/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.7727 - val_loss: 4.8905\n",
            "Epoch 188/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7731 - val_loss: 4.8924\n",
            "Epoch 189/200\n",
            "3570/3570 [==============================] - 1s 293us/step - loss: 3.7725 - val_loss: 4.8919\n",
            "Epoch 190/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.7728 - val_loss: 4.8917\n",
            "Epoch 191/200\n",
            "3570/3570 [==============================] - 1s 296us/step - loss: 3.7729 - val_loss: 4.8921\n",
            "Epoch 192/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7728 - val_loss: 4.8919\n",
            "Epoch 193/200\n",
            "3570/3570 [==============================] - 1s 292us/step - loss: 3.7723 - val_loss: 4.8946\n",
            "Epoch 194/200\n",
            "3570/3570 [==============================] - 1s 287us/step - loss: 3.7726 - val_loss: 4.8911\n",
            "\n",
            "Epoch 00194: ReduceLROnPlateau reducing learning rate to 5.637709818984149e-06.\n",
            "Epoch 195/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.7715 - val_loss: 4.8931\n",
            "Epoch 196/200\n",
            "3570/3570 [==============================] - 1s 291us/step - loss: 3.7717 - val_loss: 4.8920\n",
            "Epoch 197/200\n",
            "3570/3570 [==============================] - 1s 282us/step - loss: 3.7720 - val_loss: 4.8924\n",
            "Epoch 198/200\n",
            "3570/3570 [==============================] - 1s 290us/step - loss: 3.7715 - val_loss: 4.8921\n",
            "Epoch 199/200\n",
            "3570/3570 [==============================] - 1s 294us/step - loss: 3.7716 - val_loss: 4.8928\n",
            "Epoch 200/200\n",
            "3570/3570 [==============================] - 1s 289us/step - loss: 3.7716 - val_loss: 4.8922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean error: 2.2793951158478585\n",
            "Std of error: 2.142129896951026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAARAElEQVR4nO3de6xlZX3G8e9TxktFK7cTilw6WIkNmlrJCWK1hohVBAK0IQRj7Kg0E1OwWtvoWBMxTUygtlpseslUqGNDuBS1kKpVihjTPxgdKHJHBgSZycCMRcHGpDr66x97DW4P58wc9tqXc+b9fpKTvS7v2uuXddZ59nvetffaqSokSfu/X5p1AZKk6TDwJakRBr4kNcLAl6RGGPiS1Ig1sy4A4LDDDqu1a9fOugxJWlVuueWW71XV3HLbr4jAX7t2LVu2bJl1GZK0qiR5+Jm0d0hHkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IasSI+aTsNazd84anphy4+fYaVSNJs2MOXpEYY+JLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNWKfgZ/k8iQ7k9w5tOxjSe5NcnuSzyc5aGjdB5NsTXJfkjdNqnBJ0jOznB7+p4FTFyy7AXh5Vf0m8G3ggwBJjgfOA17WbfP3SQ4YW7WSpJHtM/Cr6uvA4wuWfaWqdnezNwNHddNnAVdV1f9V1XeArcCJY6xXkjSicYzhvxP4Ujd9JPDI0Lpt3TJJ0oz1CvwkHwJ2A1eMsO36JFuSbNm1a1efMiRJyzBy4Cd5O3AG8Naqqm7xduDooWZHdcuepqo2VtV8Vc3Pzc2NWoYkaZlGCvwkpwLvB86sqh8NrboeOC/Jc5IcCxwHfKN/mZKkvvZ5P/wkVwInA4cl2QZcxOBdOc8BbkgCcHNVvauq7kpyDXA3g6GeC6rqp5MqXpK0fPsM/Kp6yyKLL9tL+48CH+1TlCRp/PykrSQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakR+/zg1f5o7YYvPDX90MWnz7ASSZoee/iS1AgDX5Ia0eSQznI47CNpf2MPX5IaYeBLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mN2GfgJ7k8yc4kdw4tOyTJDUnu7x4P7pYnySeTbE1ye5ITJlm8JGn5ltPD/zRw6oJlG4Abq+o44MZuHuDNwHHdz3rgH8ZTpiSpr30GflV9HXh8weKzgE3d9Cbg7KHln6mBm4GDkhwxrmIlSaMb9X74h1fVjm76UeDwbvpI4JGhdtu6ZTtYIMl6Bv8FcMwxx4xYRn/e915SK3pftK2qAmqE7TZW1XxVzc/NzfUtQ5K0D6MG/mN7hmq6x53d8u3A0UPtjuqWSZJmbNQhneuBdcDF3eN1Q8svTHIV8CrgiaGhn1XLYR9J+4N9Bn6SK4GTgcOSbAMuYhD01yQ5H3gYOLdr/kXgNGAr8CPgHROoWZI0gn0GflW9ZYlVpyzStoAL+hYlSRo/P2krSY0w8CWpEaNetG2WF3AlrVb28CWpEQa+JDXCIZ0hw8M1krS/sYcvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWrEfv0FKH6hiST9XK8efpI/SXJXkjuTXJnkuUmOTbI5ydYkVyd59riKlSSNbuTAT3Ik8MfAfFW9HDgAOA+4BPhEVb0E+D5w/jgKlST103cMfw3wy0nWAM8DdgCvB67t1m8Czu65D0nSGIwc+FW1Hfgr4LsMgv4J4BbgB1W1u2u2DThyse2TrE+yJcmWXbt2jVqGJGmZ+gzpHAycBRwLvAg4EDh1udtX1caqmq+q+bm5uVHLkCQtU58hnTcA36mqXVX1E+BzwGuAg7ohHoCjgO09a5QkjUGfwP8ucFKS5yUJcApwN3ATcE7XZh1wXb8SJUnj0GcMfzODi7O3And0z7UR+ADwviRbgUOBy8ZQpySpp14fvKqqi4CLFix+EDixz/NKksbPWytIUiP261srTNrwrRseuvj0GVYiSftmD1+SGmHgS1IjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IaYeBLUiMMfElqhIEvSY0w8CWpEWtmXcD+aO2GLzw1/dDFp8+wEkn6uV49/CQHJbk2yb1J7kny6iSHJLkhyf3d48HjKlaSNLq+QzqXAv9RVb8BvAK4B9gA3FhVxwE3dvOSpBkbOfCTvBB4HXAZQFX9uKp+AJwFbOqabQLO7lukJKm/Pj38Y4FdwD8n+e8kn0pyIHB4Ve3o2jwKHL7YxknWJ9mSZMuuXbt6lCFJWo4+F23XACcA766qzUkuZcHwTVVVklps46raCGwEmJ+fX7TNMzV8sVSS9Iv69PC3AduqanM3fy2DF4DHkhwB0D3u7FeiJGkcRu7hV9WjSR5J8tKqug84Bbi7+1kHXNw9XjeWSlc4/7uQtNL1fR/+u4ErkjwbeBB4B4P/Gq5Jcj7wMHBuz31IksagV+BX1W3A/CKrTunzvJKk8fPWCpLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RG+I1XU+Q3YUmaJXv4ktQIA1+SGmHgS1IjHMOfMG+bLGmlsIcvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqRG9b62Q5ABgC7C9qs5IcixwFXAocAvwtqr6cd/97G+8VbKkaRtHD/89wD1D85cAn6iqlwDfB84fwz4kST31CvwkRwGnA5/q5gO8Hri2a7IJOLvPPiRJ49G3h/83wPuBn3XzhwI/qKrd3fw24MjFNkyyPsmWJFt27drVswxJ0r6MHPhJzgB2VtUto2xfVRurar6q5ufm5kYtQ5K0TH0u2r4GODPJacBzgV8BLgUOSrKm6+UfBWzvX6Ykqa+Re/hV9cGqOqqq1gLnAV+tqrcCNwHndM3WAdf1rlKS1Nsk3of/AeB9SbYyGNO/bAL7kCQ9Q2P5isOq+hrwtW76QeDEcTyvJGl8/KStJDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNGMv78DU+3idf0qTYw5ekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqRG+D38FGH7vvSRNij18SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IaMXLgJzk6yU1J7k5yV5L3dMsPSXJDkvu7x4PHV64kaVR9evi7gT+tquOBk4ALkhwPbABurKrjgBu7eUnSjI38Sduq2gHs6KZ/mOQe4EjgLODkrtkm4GvAB3pVuRf786dU/fYrSeM0llsrJFkLvBLYDBzevRgAPAocvsQ264H1AMccc8w4ytivGf6S+up90TbJ84HPAu+tqieH11VVAbXYdlW1sarmq2p+bm6ubxmSpH3oFfhJnsUg7K+oqs91ix9LckS3/ghgZ78SJUnj0OddOgEuA+6pqo8PrboeWNdNrwOuG708SdK49BnDfw3wNuCOJLd1y/4cuBi4Jsn5wMPAuf1KlCSNQ5936fwXkCVWnzLq82o8vMgraSE/aStJjfAbr/Yj+/NnEiT1Zw9fkhphD3+Vs1cvabns4UtSIwx8SWqEQzqrkMM4kkZhD1+SGmHgS1IjDHxJaoSBL0mN8KJt47znjtQOA78BC9/VM+lg90VEWpkc0pGkRtjDb9BS7+O3Zy7t3wx8jazPC4QvLtL0OaQjSY2wh69F2QOX9j/28CWpEfbw9Yx44zZp9TLwtU/LCflxtZE0OQ7pSFIj7OFrovr06pfadvgi8jQuLi+1j9V+YXu1169nzh6+JDViYj38JKcClwIHAJ+qqosntS+tbtMc218pvdpJ1zGp518px0+jmUjgJzkA+Dvgd4FtwDeTXF9Vd09if2rLSr+IvJyhqJX2/HsL8nENy42rvqVM86aAffc3qxfOSQ3pnAhsraoHq+rHwFXAWRPalyRpGVJV43/S5Bzg1Kr6w27+bcCrqurCoTbrgfXd7EuB+0bc3WHA93qUO0nWNpqVXBus7PqsbTQruTZYur5fq6q55T7JzN6lU1UbgY19nyfJlqqaH0NJY2dto1nJtcHKrs/aRrOSa4Px1TepIZ3twNFD80d1yyRJMzKpwP8mcFySY5M8GzgPuH5C+5IkLcNEhnSqaneSC4EvM3hb5uVVddck9sUYhoUmyNpGs5Jrg5Vdn7WNZiXXBmOqbyIXbSVJK4+ftJWkRhj4ktSIVRH4SU5Ncl+SrUk2LLL+OUmu7tZvTrJ2irUdneSmJHcnuSvJexZpc3KSJ5Lc1v18eIr1PZTkjm6/WxZZnySf7I7d7UlOmFJdLx06HrcleTLJexe0mepxS3J5kp1J7hxadkiSG5Lc3z0evMS267o29ydZN6XaPpbk3u739vkkBy2x7V7PgQnV9pEk24d+d6ctse1e/7YnVNvVQ3U9lOS2Jbad9HFbNDsmes5V1Yr+YXDR9wHgxcCzgW8Bxy9o80fAP3bT5wFXT7G+I4ATuukXAN9epL6TgX+f0fF7CDhsL+tPA74EBDgJ2Dyj3/GjDD5EMrPjBrwOOAG4c2jZXwIbuukNwCWLbHcI8GD3eHA3ffAUansjsKabvmSx2pZzDkyoto8Af7aM3/te/7YnUduC9X8NfHhGx23R7JjkObcaevjLuU3DWcCmbvpa4JQkmUZxVbWjqm7tpn8I3AMcOY19j8lZwGdq4GbgoCRHTLmGU4AHqurhKe/3F1TV14HHFywePrc2AWcvsumbgBuq6vGq+j5wA3DqpGurqq9U1e5u9mYGn3eZuiWO23JM/BYse6uty4hzgSvHuc/l2kt2TOycWw2BfyTwyND8Np4eqE+16f4AngAOnUp1Q7qhpFcCmxdZ/eok30rypSQvm2JZBXwlyS3d7SwWWs7xnbTzWPqPblbHbY/Dq2pHN/0ocPgibVbCMXwng//UFrOvc2BSLuyGmy5fYlhi1sftd4DHqur+JdZP7bgtyI6JnXOrIfBXhSTPBz4LvLeqnlyw+lYGwxWvAP4W+LcplvbaqjoBeDNwQZLXTXHf+5TBB/POBP51kdWzPG5PU4P/pVfc+5iTfAjYDVyxRJNZnAP/APw68FvADgZDJyvNW9h7734qx21v2THuc241BP5ybtPwVJska4AXAv8zleoG+3wWg1/YFVX1uYXrq+rJqvrfbvqLwLOSHDaN2qpqe/e4E/g8g3+jh836NhhvBm6tqscWrpjlcRvy2J4hru5x5yJtZnYMk7wdOAN4axcOT7OMc2DsquqxqvppVf0M+Kcl9jnL47YG+H3g6qXaTOO4LZEdEzvnVkPgL+c2DdcDe65SnwN8damTf9y6ccDLgHuq6uNLtPnVPdcUkpzI4LhP/AUpyYFJXrBnmsFFvjsXNLse+IMMnAQ8MfTv5DQs2cua1XFbYPjcWgdct0ibLwNvTHJwN3Txxm7ZRGXwJUPvB86sqh8t0WY558Akahu+DvR7S+xzlrdgeQNwb1VtW2zlNI7bXrJjcufcpK5Aj/lq9mkMrmA/AHyoW/YXDE50gOcyGBLYCnwDePEUa3stg3+5bgdu635OA94FvKtrcyFwF4N3IdwM/PaUantxt89vdfvfc+yGawuDL6t5ALgDmJ/isTuQQYC/cGjZzI4bgxeeHcBPGIyJns/gWtCNwP3AfwKHdG3nGXyT255t39mdf1uBd0yptq0MxnH3nHd73qn2IuCLezsHplDbv3Tn0+0MAuyIhbV180/72550bd3yT+85z4baTvu4LZUdEzvnvLWCJDViNQzpSJLGwMCXpEYY+JLUCANfkhph4EtSIwx8SWqEgS9Jjfh/F6CTKfmfV4IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean error: 2.625616216305287\n",
            "Std of SE error: 2.591576779420003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB3x1O6akdim",
        "colab_type": "text"
      },
      "source": [
        "#Trajectory Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUqaIqQrkcv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_trajectory(n_data_dict, data_dict, timesteps, num_of_samples=50000, show_images=False):\n",
        "\n",
        "    rssi_seq = []\n",
        "    coordinate_seq = []\n",
        "\n",
        "    n_rssi_seq = []\n",
        "    n_coordinate_seq = []\n",
        "\n",
        "    for e in range(num_of_samples):                 \n",
        "        traj_seq = []\n",
        "        n_traj_seq = []\n",
        "        label_seq = []\n",
        "\n",
        "        seed_node_id = np.random.randint(0,len(nodes),1)[0] #Generate a random integer from [0,51] as starting node_id\n",
        "        seed_node = nodes[seed_node_id]\n",
        "\n",
        "        seed_rssi_vector, n_seed_rssi_vector, seed_label = get_node_attributes(seed_node, n_data_dict, data_dict)\n",
        "        traj_seq.append(seed_rssi_vector)\n",
        "        n_traj_seq.append(n_seed_rssi_vector)\n",
        "        label_seq.append(seed_label)\n",
        "\n",
        "        for t in range(timesteps-1):                #Since one node has already been appended \n",
        "            neighbours = nodes_connected_to[seed_node]\n",
        "            len_of_neighbours = len(neighbours)\n",
        "            next_seed_id = np.random.randint(0,len_of_neighbours,1)[0]\n",
        "\n",
        "            seed_node = neighbours[next_seed_id]\n",
        "            seed_rssi_vector, n_seed_rssi_vector, seed_label = get_node_attributes(seed_node, n_data_dict, data_dict)\n",
        "            traj_seq.append(seed_rssi_vector)\n",
        "            n_traj_seq.append(n_seed_rssi_vector)\n",
        "            label_seq.append(seed_label)\n",
        "\n",
        "        traj_seq = np.asarray(traj_seq)\n",
        "        n_traj_seq = np.asarray(n_traj_seq)\n",
        "        label_seq = np.asarray(label_seq)\n",
        "        \n",
        "        rssi_seq.append(traj_seq)\n",
        "        coordinate_seq.append(label_seq)\n",
        "\n",
        "        n_rssi_seq.append(n_traj_seq)    \n",
        "        n_coordinate_seq.append(label_seq)\n",
        "\n",
        "        if e%(num_of_samples/5)==0 and show_images==True:\n",
        "            print('Sample:',e)\n",
        "            draw_trajectory(label_seq) \n",
        "\n",
        "    rssi_seq = np.asarray(rssi_seq)\n",
        "    coordinate_seq = np.asarray(coordinate_seq)\n",
        "\n",
        "    n_rssi_seq = np.asarray(n_rssi_seq)\n",
        "    n_coordinate_seq = np.asarray(n_coordinate_seq)\n",
        "    \n",
        "    return n_rssi_seq, n_coordinate_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB_hgmjQnryw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_node_attributes(node, n_data_dict, data_dict):\n",
        "    RANDOM_ID = np.random.randint(0,100,1)[0]\n",
        "    \n",
        "    rssi_vector = data_dict[node][RANDOM_ID,:]\n",
        "    n_rssi_vector = n_data_dict[node][RANDOM_ID,:]\n",
        "    label = np.asarray(node_to_coordinates[node])\n",
        "    \n",
        "    return rssi_vector, n_rssi_vector, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxygl8WPmn5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_trajectory(label_map, scale=2):\n",
        "    traj_map = np.zeros((7*scale,25*scale))\n",
        "\n",
        "    for t,n in enumerate(label_map):\n",
        "        traj_map[n[0]*scale,(n[1]+1)*scale] = t+1\n",
        "\n",
        "    plt.figure(figsize=(7*scale*0.65,3*scale*0.65))\n",
        "    plt.imshow(traj_map,cmap='gray')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWin3YbZ1Hry",
        "colab_type": "text"
      },
      "source": [
        "#Objective 1: MISO : RSSI seq to final position prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQORIjJcwT7w",
        "colab_type": "text"
      },
      "source": [
        "##Generalized function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUeGnsewWKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "def MISO(n_rssi_seq, n_coordinate_seq):\n",
        "    X = n_rssi_seq\n",
        "    Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "    #Standarize\n",
        "    mean = np.mean(X_train)\n",
        "    std = np.mean(X_test)\n",
        "    X_train = (X_train - mean) / std\n",
        "    X_test = (X_test - mean) / std\n",
        "\n",
        "\n",
        "    inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "    x = GRU(128, return_sequences=True)(inp)\n",
        "    x = Dropout(.25)(x)\n",
        "    x = GRU(64, return_sequences=True)(x)\n",
        "    x = Dropout(.25)(x)\n",
        "    x = GRU(32, return_sequences=True)(x)\n",
        "    x = Dropout(.25)(x)\n",
        "    x = GRU(8, return_sequences = False)(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "    model = Model(inp, x)\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=0)\n",
        "\n",
        "    history = model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=0, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    error_nn = []\n",
        "    for e in range(y_pred.shape[0]):\n",
        "        error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "    return np.mean(error_nn), np.std(error_nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HZE7ulAJkDCz"
      },
      "source": [
        "##Noise : 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mpYwC61LkDDF"
      },
      "source": [
        "- Noise var: 0 \n",
        "- Buffer size: 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqMkENkqGFzd",
        "colab_type": "code",
        "outputId": "e23d57e9-8fba-4eda-b23b-2ecb68face26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(128, return_sequences=True)(inp)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(64, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(32, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(8, return_sequences = False)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of SE error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35000, 3, 4) (35000, 2)\n",
            "(15000, 3, 4) (15000, 2)\n",
            "Epoch 1/100\n",
            "1094/1094 [==============================] - 8s 8ms/step - loss: 30.5352 - val_loss: 12.1965 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 9.2888 - val_loss: 3.3963 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 5.3717 - val_loss: 1.4404 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 4.1497 - val_loss: 1.0429 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 3.8962 - val_loss: 0.8521 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.5004 - val_loss: 0.8057 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 3.2925 - val_loss: 0.7429 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.9980 - val_loss: 0.6010 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.7826 - val_loss: 0.5598 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.5787 - val_loss: 0.5145 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.4320 - val_loss: 0.4513 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 2.2399 - val_loss: 0.3928 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 2.1008 - val_loss: 0.4096 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 2.0296 - val_loss: 0.3482 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8809 - val_loss: 0.3308 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8086 - val_loss: 0.3563 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6999 - val_loss: 0.2707 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6416 - val_loss: 0.2722 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5883 - val_loss: 0.3883 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5319 - val_loss: 0.3621 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5120 - val_loss: 0.3549 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4519 - val_loss: 0.3516 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4375 - val_loss: 0.3259 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4160 - val_loss: 0.3738 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3987 - val_loss: 0.3636 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3892 - val_loss: 0.3338 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "1094/1094 [==============================] - ETA: 0s - loss: 1.3810\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3810 - val_loss: 0.3569 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3599 - val_loss: 0.3570 - lr: 7.5000e-04\n",
            "Epoch 29/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3410 - val_loss: 0.4028 - lr: 7.5000e-04\n",
            "Epoch 30/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3295 - val_loss: 0.3657 - lr: 7.5000e-04\n",
            "Epoch 31/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 1.3275 - val_loss: 0.3807 - lr: 7.5000e-04\n",
            "Epoch 32/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3263 - val_loss: 0.3513 - lr: 7.5000e-04\n",
            "Epoch 33/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3162 - val_loss: 0.3490 - lr: 7.5000e-04\n",
            "Epoch 34/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3075 - val_loss: 0.3858 - lr: 7.5000e-04\n",
            "Epoch 35/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3148 - val_loss: 0.3285 - lr: 7.5000e-04\n",
            "Epoch 36/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3168 - val_loss: 0.3544 - lr: 7.5000e-04\n",
            "Epoch 37/100\n",
            "1092/1094 [============================>.] - ETA: 0s - loss: 1.2904\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2907 - val_loss: 0.3749 - lr: 7.5000e-04\n",
            "Epoch 38/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2821 - val_loss: 0.3740 - lr: 5.6250e-04\n",
            "Epoch 39/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2717 - val_loss: 0.3965 - lr: 5.6250e-04\n",
            "Epoch 40/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2716 - val_loss: 0.3595 - lr: 5.6250e-04\n",
            "Epoch 41/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2825 - val_loss: 0.3549 - lr: 5.6250e-04\n",
            "Epoch 42/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2617 - val_loss: 0.3510 - lr: 5.6250e-04\n",
            "Epoch 43/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2766 - val_loss: 0.3555 - lr: 5.6250e-04\n",
            "Epoch 44/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2534 - val_loss: 0.3836 - lr: 5.6250e-04\n",
            "Epoch 45/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2648 - val_loss: 0.3535 - lr: 5.6250e-04\n",
            "Epoch 46/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2455 - val_loss: 0.3600 - lr: 5.6250e-04\n",
            "Epoch 47/100\n",
            "1086/1094 [============================>.] - ETA: 0s - loss: 1.2354\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2345 - val_loss: 0.3674 - lr: 5.6250e-04\n",
            "Epoch 48/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2398 - val_loss: 0.3583 - lr: 4.2187e-04\n",
            "Epoch 49/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2452 - val_loss: 0.3425 - lr: 4.2187e-04\n",
            "Epoch 50/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2433 - val_loss: 0.3490 - lr: 4.2187e-04\n",
            "Epoch 51/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2515 - val_loss: 0.3541 - lr: 4.2187e-04\n",
            "Epoch 52/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2541 - val_loss: 0.3585 - lr: 4.2187e-04\n",
            "Epoch 53/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2500 - val_loss: 0.3834 - lr: 4.2187e-04\n",
            "Epoch 54/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2323 - val_loss: 0.3415 - lr: 4.2187e-04\n",
            "Epoch 55/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2436 - val_loss: 0.3409 - lr: 4.2187e-04\n",
            "Epoch 56/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2240 - val_loss: 0.3283 - lr: 4.2187e-04\n",
            "Epoch 57/100\n",
            "1087/1094 [============================>.] - ETA: 0s - loss: 1.2361\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2358 - val_loss: 0.3580 - lr: 4.2187e-04\n",
            "Epoch 58/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2382 - val_loss: 0.3637 - lr: 3.1641e-04\n",
            "Epoch 59/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2136 - val_loss: 0.3467 - lr: 3.1641e-04\n",
            "Epoch 60/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2085 - val_loss: 0.3359 - lr: 3.1641e-04\n",
            "Epoch 61/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2321 - val_loss: 0.3449 - lr: 3.1641e-04\n",
            "Epoch 62/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2209 - val_loss: 0.3608 - lr: 3.1641e-04\n",
            "Epoch 63/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2278 - val_loss: 0.3357 - lr: 3.1641e-04\n",
            "Epoch 64/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2235 - val_loss: 0.3397 - lr: 3.1641e-04\n",
            "Epoch 65/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2049 - val_loss: 0.3448 - lr: 3.1641e-04\n",
            "Epoch 66/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2210 - val_loss: 0.3582 - lr: 3.1641e-04\n",
            "Epoch 67/100\n",
            "1087/1094 [============================>.] - ETA: 0s - loss: 1.2014\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2009 - val_loss: 0.3469 - lr: 3.1641e-04\n",
            "Epoch 68/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2257 - val_loss: 0.3435 - lr: 2.3730e-04\n",
            "Epoch 69/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2283 - val_loss: 0.3424 - lr: 2.3730e-04\n",
            "Epoch 70/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2285 - val_loss: 0.3423 - lr: 2.3730e-04\n",
            "Epoch 71/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2230 - val_loss: 0.3694 - lr: 2.3730e-04\n",
            "Epoch 72/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2107 - val_loss: 0.3437 - lr: 2.3730e-04\n",
            "Epoch 73/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2123 - val_loss: 0.3403 - lr: 2.3730e-04\n",
            "Epoch 74/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2112 - val_loss: 0.3340 - lr: 2.3730e-04\n",
            "Epoch 75/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2032 - val_loss: 0.3486 - lr: 2.3730e-04\n",
            "Epoch 76/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2172 - val_loss: 0.3514 - lr: 2.3730e-04\n",
            "Epoch 77/100\n",
            "1091/1094 [============================>.] - ETA: 0s - loss: 1.1965\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1963 - val_loss: 0.3381 - lr: 2.3730e-04\n",
            "Epoch 78/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2179 - val_loss: 0.3485 - lr: 1.7798e-04\n",
            "Epoch 79/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2355 - val_loss: 0.3295 - lr: 1.7798e-04\n",
            "Epoch 80/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2040 - val_loss: 0.3556 - lr: 1.7798e-04\n",
            "Epoch 81/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2088 - val_loss: 0.3285 - lr: 1.7798e-04\n",
            "Epoch 82/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2060 - val_loss: 0.3388 - lr: 1.7798e-04\n",
            "Epoch 83/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2147 - val_loss: 0.3379 - lr: 1.7798e-04\n",
            "Epoch 84/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 1.2090 - val_loss: 0.3322 - lr: 1.7798e-04\n",
            "Epoch 85/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2027 - val_loss: 0.3304 - lr: 1.7798e-04\n",
            "Epoch 86/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1877 - val_loss: 0.3427 - lr: 1.7798e-04\n",
            "Epoch 87/100\n",
            "1089/1094 [============================>.] - ETA: 0s - loss: 1.2052\n",
            "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2042 - val_loss: 0.3228 - lr: 1.7798e-04\n",
            "Epoch 88/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2011 - val_loss: 0.3270 - lr: 1.3348e-04\n",
            "Epoch 89/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1924 - val_loss: 0.3367 - lr: 1.3348e-04\n",
            "Epoch 90/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1970 - val_loss: 0.3318 - lr: 1.3348e-04\n",
            "Epoch 91/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2058 - val_loss: 0.3456 - lr: 1.3348e-04\n",
            "Epoch 92/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1893 - val_loss: 0.3326 - lr: 1.3348e-04\n",
            "Epoch 93/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1937 - val_loss: 0.3360 - lr: 1.3348e-04\n",
            "Epoch 94/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1985 - val_loss: 0.3306 - lr: 1.3348e-04\n",
            "Epoch 95/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1938 - val_loss: 0.3401 - lr: 1.3348e-04\n",
            "Epoch 96/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1824 - val_loss: 0.3443 - lr: 1.3348e-04\n",
            "Epoch 97/100\n",
            "1087/1094 [============================>.] - ETA: 0s - loss: 1.2130\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2123 - val_loss: 0.3400 - lr: 1.3348e-04\n",
            "Epoch 98/100\n",
            "1094/1094 [==============================] - 8s 8ms/step - loss: 1.1932 - val_loss: 0.3348 - lr: 1.0011e-04\n",
            "Epoch 99/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1867 - val_loss: 0.3321 - lr: 1.0011e-04\n",
            "Epoch 100/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.1922 - val_loss: 0.3371 - lr: 1.0011e-04\n",
            "Mean error: 0.635236282754876\n",
            "Std of SE error: 0.5203569525743207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px-rKjhPeMdE",
        "colab_type": "text"
      },
      "source": [
        "##Noise : 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF4H_qVMedYN",
        "colab_type": "text"
      },
      "source": [
        "- Noise var: 2 \n",
        "- Buffer size: 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elHpDTHsCdPU",
        "colab_type": "code",
        "outputId": "a677ce82-3b22-400a-bcd4-77c028f1b882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "- Noise var: 4 \n",
        "- Buffer size: 3\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(4, return_sequences=True)(inp)\n",
        "x = Dropout(0.5)(x)\n",
        "x = GRU(3, return_sequences=True)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = GRU(2, return_sequences=False)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('\n",
        "'''\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(128, return_sequences=True)(inp)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(64, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(32, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(8, return_sequences = False)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of SE error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35000, 3, 4) (35000, 2)\n",
            "(15000, 3, 4) (15000, 2)\n",
            "Epoch 1/100\n",
            "1094/1094 [==============================] - 9s 8ms/step - loss: 31.3860 - val_loss: 12.8647 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 9.6316 - val_loss: 3.6921 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 5.3917 - val_loss: 1.5434 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 4.2506 - val_loss: 1.2309 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.9294 - val_loss: 0.8688 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.6077 - val_loss: 0.8125 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.3448 - val_loss: 0.7612 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.0880 - val_loss: 0.7628 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.8731 - val_loss: 0.6694 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.7013 - val_loss: 0.6008 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.4954 - val_loss: 0.5549 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.3287 - val_loss: 0.4789 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.1728 - val_loss: 0.5260 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0713 - val_loss: 0.4745 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9539 - val_loss: 0.4221 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8746 - val_loss: 0.4100 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7775 - val_loss: 0.4207 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7332 - val_loss: 0.4037 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6880 - val_loss: 0.4128 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6243 - val_loss: 0.4777 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6030 - val_loss: 0.4972 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5437 - val_loss: 0.4782 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5589 - val_loss: 0.4676 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5267 - val_loss: 0.4001 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5256 - val_loss: 0.4203 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4798 - val_loss: 0.4798 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4887 - val_loss: 0.4491 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4813 - val_loss: 0.4161 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4519 - val_loss: 0.4473 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4512 - val_loss: 0.4379 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4051 - val_loss: 0.4616 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4166 - val_loss: 0.4863 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4095 - val_loss: 0.4499 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "1086/1094 [============================>.] - ETA: 0s - loss: 1.4219\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4216 - val_loss: 0.4579 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3911 - val_loss: 0.4533 - lr: 7.5000e-04\n",
            "Epoch 36/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3896 - val_loss: 0.5032 - lr: 7.5000e-04\n",
            "Epoch 37/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3956 - val_loss: 0.4533 - lr: 7.5000e-04\n",
            "Epoch 38/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3736 - val_loss: 0.4327 - lr: 7.5000e-04\n",
            "Epoch 39/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3688 - val_loss: 0.4678 - lr: 7.5000e-04\n",
            "Epoch 40/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3696 - val_loss: 0.4572 - lr: 7.5000e-04\n",
            "Epoch 41/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3770 - val_loss: 0.4602 - lr: 7.5000e-04\n",
            "Epoch 42/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3558 - val_loss: 0.4318 - lr: 7.5000e-04\n",
            "Epoch 43/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3492 - val_loss: 0.4666 - lr: 7.5000e-04\n",
            "Epoch 44/100\n",
            "1093/1094 [============================>.] - ETA: 0s - loss: 1.3562\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3562 - val_loss: 0.4431 - lr: 7.5000e-04\n",
            "Epoch 45/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3334 - val_loss: 0.4608 - lr: 5.6250e-04\n",
            "Epoch 46/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3164 - val_loss: 0.4514 - lr: 5.6250e-04\n",
            "Epoch 47/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3329 - val_loss: 0.4244 - lr: 5.6250e-04\n",
            "Epoch 48/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3254 - val_loss: 0.4298 - lr: 5.6250e-04\n",
            "Epoch 49/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3065 - val_loss: 0.4225 - lr: 5.6250e-04\n",
            "Epoch 50/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3191 - val_loss: 0.4246 - lr: 5.6250e-04\n",
            "Epoch 51/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3245 - val_loss: 0.4169 - lr: 5.6250e-04\n",
            "Epoch 52/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3113 - val_loss: 0.4460 - lr: 5.6250e-04\n",
            "Epoch 53/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3193 - val_loss: 0.4332 - lr: 5.6250e-04\n",
            "Epoch 54/100\n",
            "1091/1094 [============================>.] - ETA: 0s - loss: 1.3104\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3110 - val_loss: 0.4386 - lr: 5.6250e-04\n",
            "Epoch 55/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3001 - val_loss: 0.4496 - lr: 4.2187e-04\n",
            "Epoch 56/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2982 - val_loss: 0.4052 - lr: 4.2187e-04\n",
            "Epoch 57/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2944 - val_loss: 0.4211 - lr: 4.2187e-04\n",
            "Epoch 58/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3093 - val_loss: 0.4211 - lr: 4.2187e-04\n",
            "Epoch 59/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3155 - val_loss: 0.4310 - lr: 4.2187e-04\n",
            "Epoch 60/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2846 - val_loss: 0.4256 - lr: 4.2187e-04\n",
            "Epoch 61/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2859 - val_loss: 0.4117 - lr: 4.2187e-04\n",
            "Epoch 62/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2868 - val_loss: 0.4272 - lr: 4.2187e-04\n",
            "Epoch 63/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2794 - val_loss: 0.4193 - lr: 4.2187e-04\n",
            "Epoch 64/100\n",
            "1093/1094 [============================>.] - ETA: 0s - loss: 1.2858\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2857 - val_loss: 0.4043 - lr: 4.2187e-04\n",
            "Epoch 65/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2872 - val_loss: 0.4259 - lr: 3.1641e-04\n",
            "Epoch 66/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2921 - val_loss: 0.4049 - lr: 3.1641e-04\n",
            "Epoch 67/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2976 - val_loss: 0.4083 - lr: 3.1641e-04\n",
            "Epoch 68/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2793 - val_loss: 0.4217 - lr: 3.1641e-04\n",
            "Epoch 69/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2918 - val_loss: 0.4179 - lr: 3.1641e-04\n",
            "Epoch 70/100\n",
            "1094/1094 [==============================] - 8s 8ms/step - loss: 1.2870 - val_loss: 0.4171 - lr: 3.1641e-04\n",
            "Epoch 71/100\n",
            "1094/1094 [==============================] - 8s 8ms/step - loss: 1.2906 - val_loss: 0.4270 - lr: 3.1641e-04\n",
            "Epoch 72/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2777 - val_loss: 0.4186 - lr: 3.1641e-04\n",
            "Epoch 73/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2677 - val_loss: 0.4088 - lr: 3.1641e-04\n",
            "Epoch 74/100\n",
            "1091/1094 [============================>.] - ETA: 0s - loss: 1.2916\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2911 - val_loss: 0.4319 - lr: 3.1641e-04\n",
            "Epoch 75/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2826 - val_loss: 0.4352 - lr: 2.3730e-04\n",
            "Epoch 76/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2946 - val_loss: 0.4108 - lr: 2.3730e-04\n",
            "Epoch 77/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2665 - val_loss: 0.4087 - lr: 2.3730e-04\n",
            "Epoch 78/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2858 - val_loss: 0.4122 - lr: 2.3730e-04\n",
            "Epoch 79/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2889 - val_loss: 0.3992 - lr: 2.3730e-04\n",
            "Epoch 80/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2675 - val_loss: 0.4097 - lr: 2.3730e-04\n",
            "Epoch 81/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2832 - val_loss: 0.4085 - lr: 2.3730e-04\n",
            "Epoch 82/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2862 - val_loss: 0.4128 - lr: 2.3730e-04\n",
            "Epoch 83/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2658 - val_loss: 0.4080 - lr: 2.3730e-04\n",
            "Epoch 84/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2715 - val_loss: 0.4103 - lr: 2.3730e-04\n",
            "Epoch 85/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2647 - val_loss: 0.4126 - lr: 2.3730e-04\n",
            "Epoch 86/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2776 - val_loss: 0.4069 - lr: 2.3730e-04\n",
            "Epoch 87/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2576 - val_loss: 0.4039 - lr: 2.3730e-04\n",
            "Epoch 88/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2633 - val_loss: 0.4253 - lr: 2.3730e-04\n",
            "Epoch 89/100\n",
            "1088/1094 [============================>.] - ETA: 0s - loss: 1.2724\n",
            "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2725 - val_loss: 0.4121 - lr: 2.3730e-04\n",
            "Epoch 90/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2561 - val_loss: 0.4081 - lr: 1.7798e-04\n",
            "Epoch 91/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2626 - val_loss: 0.3986 - lr: 1.7798e-04\n",
            "Epoch 92/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2744 - val_loss: 0.4015 - lr: 1.7798e-04\n",
            "Epoch 93/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2673 - val_loss: 0.4035 - lr: 1.7798e-04\n",
            "Epoch 94/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2799 - val_loss: 0.4092 - lr: 1.7798e-04\n",
            "Epoch 95/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2735 - val_loss: 0.3971 - lr: 1.7798e-04\n",
            "Epoch 96/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2544 - val_loss: 0.4010 - lr: 1.7798e-04\n",
            "Epoch 97/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2534 - val_loss: 0.4117 - lr: 1.7798e-04\n",
            "Epoch 98/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2639 - val_loss: 0.3940 - lr: 1.7798e-04\n",
            "Epoch 99/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2544 - val_loss: 0.4004 - lr: 1.7798e-04\n",
            "Epoch 100/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.2498 - val_loss: 0.4076 - lr: 1.7798e-04\n",
            "Mean error: 0.7198572314936809\n",
            "Std of SE error: 0.5449794046984565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAO50lEQVR4nO3de4xc5XnH8e8TzCUJKW6xRV1fulRBqShqCl1RV0gRiluJQISRSpClCgx1ZKklTdJUKm7+KGrVP4hUhdKLiCycykQoMSKouEBaRVxU9Q/cmEvKxaXaUohtmWCIMWlpmrp6+se8wGB2vWe9szszz34/0kjn8s7MM0e7v3nnPWfeicxEklTL+4ZdgCRp8Ax3SSrIcJekggx3SSrIcJekgpYNuwCAFStW5MTExLDLkKSx8vjjj7+amSun2zcS4T4xMcHevXuHXYYkjZWIeGmmfQ7LSFJBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBI/EN1VE3se2Bt5dfvOWKIVYiSd3Yc5ekggx3SSrIcJekggx3SSrIcJekggx3SSrIcJekggx3SSrIcJekggx3SSrIcJekggx3SSrIcJekggx3SSrIKX/nyOl/JY0De+6SVJDhLkkFGe6SVJDhLkkFGe6SVJDhLkkFGe6SVJDhLkkFGe6SVJDhLkkFdQr3iPi9iHg2Ip6JiK9HxBkRcW5E7ImIqYjYFRGntbant/Wptn9iIV+AJOm9Zg33iFgNfBaYzMwLgFOATcCXgFsz88PAEWBLu8sW4EjbfmtrJ0laRF2HZZYB74+IZcAHgEPAx4F72v6dwFVteWNbp+3fEBExmHIlSV3MGu6ZeRD4M+B79EL9KPA48HpmHmvNDgCr2/JqYH+777HW/uzjHzcitkbE3ojYe/jw4fm+DklSny7DMj9Jrzd+LvAzwAeBy+b7xJm5PTMnM3Ny5cqV8304SVKfLsMyvwb8R2Yezsz/Be4FLgGWt2EagDXAwbZ8EFgL0PafBbw20KolSSfUJdy/B6yPiA+0sfMNwHPAI8DVrc1m4L62vLut0/Y/nJk5uJIlSbPpMua+h96J0SeAp9t9tgM3AV+IiCl6Y+o72l12AGe37V8Ati1A3ZKkE+j0M3uZeTNw83GbXwAunqbtj4BPzb80SdLJ8huqklSQ4S5JBRnuklSQ4S5JBRnuklSQ4S5JBRnuklSQ4S5JBRnuklSQ4S5JBRnuklSQ4S5JBRnuklSQ4S5JBXWa8ncpmtj2wLBLkKSTZs9dkgoy3CWpIMNdkgoy3CWpIMNdkgoy3CWpIMNdkgoy3CWpIMNdkgoy3CWpIKcfmIf+KQpevOWKIVYiSe9mz12SCjLcJakgw12SCjLcJakgw12SCjLcJakgw12SCjLcJakgw12SCjLcJamgTtMPRMRy4A7gAiCB3wKeB3YBE8CLwDWZeSQiArgNuBx4E7g+M58YeOUjxqkIJI2Srj3324C/z8yfBz4K7AO2AQ9l5nnAQ20d4BPAee22Fbh9oBVLkmY1a7hHxFnAx4AdAJn548x8HdgI7GzNdgJXteWNwJ3Z8xiwPCJWDbxySdKMuvTczwUOA38TEU9GxB0R8UHgnMw81Nq8DJzTllcD+/vuf6Bte5eI2BoReyNi7+HDh0/+FUiS3qNLuC8DLgJuz8wLgf/inSEYADIz6Y3Fd5aZ2zNzMjMnV65cOZe7SpJm0eWE6gHgQGbuaev30Av370fEqsw81IZdXmn7DwJr++6/pm1bMjy5KmnYZu25Z+bLwP6I+EjbtAF4DtgNbG7bNgP3teXdwHXRsx442jd8I0laBF1/iel3gbsi4jTgBeAGem8Md0fEFuAl4JrW9kF6l0FO0bsU8oaBVjxm7MVLGoZO4Z6ZTwGT0+zaME3bBG6cZ10lGfSSFovfUJWkggx3SSrIcJekggx3SSrIcJekggx3SSrIcJekggx3SSrIcJekgrpOPyBpiPx2s+bKcJdGVH+gS3PlsIwkFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBTj8wJM4VImkh2XOXpIIMd0kqyHCXpIIMd0kqyHCXpIIMd0kqyHCXpIIMd0kqyHCXpIL8huqY85uukqZjuPfx1+YlVWG4jwB735IGzTF3SSqoc7hHxCkR8WRE3N/Wz42IPRExFRG7IuK0tv30tj7V9k8sTOmSpJnMpef+OWBf3/qXgFsz88PAEWBL274FONK239raSZIWUadwj4g1wBXAHW09gI8D97QmO4Gr2vLGtk7bv6G1lyQtkq4nVP8c+APgQ239bOD1zDzW1g8Aq9vyamA/QGYei4ijrf2r/Q8YEVuBrQDr1q072frL8eSqpEGYNdwj4pPAK5n5eERcOqgnzsztwHaAycnJHNTjavH4RiSNri4990uAKyPicuAM4CeA24DlEbGs9d7XAAdb+4PAWuBARCwDzgJeG3jlS5jX40uazaxj7pn5h5m5JjMngE3Aw5n5m8AjwNWt2Wbgvra8u63T9j+cmfbMT8LEtgfevknSXMznS0w3Ad+IiD8FngR2tO07gK9FxBTwA3pvCJonA17SXMwp3DPzUeDRtvwCcPE0bX4EfGoAtUmSTpLfUJWkggx3SSrIcJekggx3SSrIcJekggx3SSrIcJekggx3SSrIcJekgvwNVc2J0yBI48GeuyQVZLhLUkGGuyQVZLhLUkGGuyQVZLhLUkGGuyQVZLhLUkGGuyQVZLhLUkFOP1BI/9QAL95yxRArkTRs9twlqSDDXZIKMtwlqSDDXZIKMtwlqaAlf7WMPz4hqSJ77pJUkOEuSQUZ7pJUkOEuSQUt+ROqVTkVgbS0Ge4aON9YpOFzWEaSCjLcJamgWYdlImItcCdwDpDA9sy8LSJ+CtgFTAAvAtdk5pGICOA24HLgTeD6zHxiYcpXFw6TSEtPl577MeD3M/N8YD1wY0ScD2wDHsrM84CH2jrAJ4Dz2m0rcPvAq5YkndCsPffMPAQcass/jIh9wGpgI3Bpa7YTeBS4qW2/MzMTeCwilkfEqvY4Q+d0A5KWgjldLRMRE8CFwB7gnL7AfpnesA30gn9/390OtG3vCveI2EqvZ8+6devmWLZGjW+a0mjpHO4RcSbwTeDzmflGb2i9JzMzInIuT5yZ24HtAJOTk3O6rwZjMcbiHe+XhqPT1TIRcSq9YL8rM+9tm78fEava/lXAK237QWBt393XtG2SpEUya7i3q192APsy88t9u3YDm9vyZuC+vu3XRc964OiojLdL0lLRZVjmEuBa4OmIeKpt+yJwC3B3RGwBXgKuafsepHcZ5BS9SyFvGGjFkqRZdbla5p+AmGH3hmnaJ3DjPOuSJM3Dkphbxis5ZneiY+SJUGn8LIlw1zt8o5OWBueWkaSCDHdJKshwl6SCHHPXrBynl8aPPXdJKsieuxaN88xIi8eeuyQVVLbn7jixpKXMnrskFWS4S1JBhrskFVR2zF2jzStnpIVlz12SCjLcJamgUsMyXv4oST323CWpIMNdkgoy3CWpIMNdkgoy3CWpoFJXy2g8+YUmafDsuUtSQfbcpTE203c7/AQkw10jxSEaaTAclpGkguy5a2Q55HDy/AQkw10aM86hpC7GPtz9Q5ek9xr7cJd0Yg7RLE2Gu8aOY/Enz6BfOrxaRpIKsueuMrqcf7G3qqXCcNeSMtOwhEM9qsZw15K11K+0cvy9tgUZc4+IyyLi+YiYiohtC/EckqSZDbznHhGnAH8N/DpwAPhOROzOzOcG/VzSQluo3v2o9ZQdrqpnIYZlLgamMvMFgIj4BrARMNylZtyHhE6m/rm+acz1DWemx1mqIjMH+4ARVwOXZean2/q1wK9k5meOa7cV2NpWPwI8f5JPuQJ49STvO2zjWrt1L65xrRvGt/ZxqftnM3PldDuGdkI1M7cD2+f7OBGxNzMnB1DSohvX2q17cY1r3TC+tY9r3f0W4oTqQWBt3/qatk2StEgWIty/A5wXEedGxGnAJmD3AjyPJGkGAx+WycxjEfEZ4B+AU4CvZuazg36ePvMe2hmica3duhfXuNYN41v7uNb9toGfUJUkDZ8Th0lSQYa7JBU0NuE+25QGEXF6ROxq+/dExMTiV/leHeq+PiIOR8RT7fbpYdR5vIj4akS8EhHPzLA/IuIv2uv6l4i4aLFrnE6Hui+NiKN9x/uPFrvG6UTE2oh4JCKei4hnI+Jz07QZ1WPepfaRO+4RcUZE/HNEfLfV/cfTtBnJXOkkM0f+Ru/E7L8DPwecBnwXOP+4Nr8DfKUtbwJ2jUnd1wN/Nexap6n9Y8BFwDMz7L8c+BYQwHpgz7Br7lj3pcD9w65zmrpWARe15Q8B/zbN38qoHvMutY/ccW/H8cy2fCqwB1h/XJuRy5Wut3Hpub89pUFm/hh4a0qDfhuBnW35HmBDRMQi1jidLnWPpMz8R+AHJ2iyEbgzex4DlkfEqsWpbmYd6h5JmXkoM59oyz8E9gGrj2s2qse8S+0jpx3H/2yrp7bb8VeYjGKudDIu4b4a2N+3foD3/vG83SYzjwFHgbMXpbqZdakb4Dfax+x7ImLtNPtHUdfXNop+tX0U/1ZE/MKwizle++h/Ib2eZL+RP+YnqB1G8LhHxCkR8RTwCvDtzJzxmI9QrnQyLuFe2d8BE5n5i8C3eaeXoIXxBL35OD4K/CXwt0Ou510i4kzgm8DnM/ONYdczF7PUPpLHPTP/LzN/id436S+OiAuGXdOgjEu4d5nS4O02EbEMOAt4bVGqm9msdWfma5n5P231DuCXF6m2+RrLaSYy8423Popn5oPAqRGxYshlARARp9ILx7sy895pmozsMZ+t9lE+7gCZ+TrwCHDZcbtGMVc6GZdw7zKlwW5gc1u+Gng421mQIZq17uPGTK+kN145DnYD17UrONYDRzPz0LCLmk1E/PRbY6YRcTG9/4Gh/7O2mnYA+zLzyzM0G8lj3qX2UTzuEbEyIpa35ffT+w2Kfz2u2SjmSidj8TN7OcOUBhHxJ8DezNxN74/raxExRe+E2qbhVdzTse7PRsSVwDF6dV8/tIL7RMTX6V3hsCIiDgA30zvhRGZ+BXiQ3tUbU8CbwA3DqfTdOtR9NfDbEXEM+G9g04j8s14CXAs83caAAb4IrIPRPuZ0q30Uj/sqYGf0fmDofcDdmXn/qOdKV04/IEkFjcuwjCRpDgx3SSrIcJekggx3SSrIcJekggx3SSrIcJekgv4fZisfkZTVux4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lI44WUElxy4M"
      },
      "source": [
        "##Noise : 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aUvfeQVWxy4Y"
      },
      "source": [
        "- Noise var: 4\n",
        "- Buffer size: 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSJ9sjps_IbG",
        "colab_type": "code",
        "outputId": "78f330db-bdf0-43ae-aa1f-078d6b0f4ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "- Noise var: 4 \n",
        "- Buffer size: 3\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(4, return_sequences=True)(inp)\n",
        "x = Dropout(0.5)(x)\n",
        "x = GRU(3, return_sequences=True)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = GRU(2, return_sequences=False)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('\n",
        "'''\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(128, return_sequences=True)(inp)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(64, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(32, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(8, return_sequences = False)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of SE error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35000, 3, 4) (35000, 2)\n",
            "(15000, 3, 4) (15000, 2)\n",
            "Epoch 1/100\n",
            "1094/1094 [==============================] - 8s 8ms/step - loss: 32.7380 - val_loss: 13.4046 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 9.9510 - val_loss: 3.9277 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 5.6222 - val_loss: 1.7485 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 4.3862 - val_loss: 1.1785 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1094/1094 [==============================] - 7s 7ms/step - loss: 3.9939 - val_loss: 1.0263 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.7274 - val_loss: 0.9295 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.4633 - val_loss: 0.9970 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.2259 - val_loss: 0.9195 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.0042 - val_loss: 0.8742 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.8018 - val_loss: 0.7432 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.6658 - val_loss: 0.6970 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.4969 - val_loss: 0.7136 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.3603 - val_loss: 0.5926 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.2607 - val_loss: 0.6239 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.1734 - val_loss: 0.5936 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0697 - val_loss: 0.6144 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9696 - val_loss: 0.5643 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9124 - val_loss: 0.6159 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8646 - val_loss: 0.5806 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8108 - val_loss: 0.5685 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7770 - val_loss: 0.6217 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7240 - val_loss: 0.7148 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6656 - val_loss: 0.6621 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6541 - val_loss: 0.6365 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6404 - val_loss: 0.6467 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6354 - val_loss: 0.6622 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "1090/1094 [============================>.] - ETA: 0s - loss: 1.6043\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6036 - val_loss: 0.6073 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5929 - val_loss: 0.6241 - lr: 7.5000e-04\n",
            "Epoch 29/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5690 - val_loss: 0.5995 - lr: 7.5000e-04\n",
            "Epoch 30/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5666 - val_loss: 0.6072 - lr: 7.5000e-04\n",
            "Epoch 31/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5497 - val_loss: 0.6026 - lr: 7.5000e-04\n",
            "Epoch 32/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5608 - val_loss: 0.5932 - lr: 7.5000e-04\n",
            "Epoch 33/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5418 - val_loss: 0.6034 - lr: 7.5000e-04\n",
            "Epoch 34/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5233 - val_loss: 0.6038 - lr: 7.5000e-04\n",
            "Epoch 35/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5250 - val_loss: 0.5951 - lr: 7.5000e-04\n",
            "Epoch 36/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5140 - val_loss: 0.5973 - lr: 7.5000e-04\n",
            "Epoch 37/100\n",
            "1089/1094 [============================>.] - ETA: 0s - loss: 1.5255\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5245 - val_loss: 0.5806 - lr: 7.5000e-04\n",
            "Epoch 38/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4821 - val_loss: 0.5937 - lr: 5.6250e-04\n",
            "Epoch 39/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4950 - val_loss: 0.5846 - lr: 5.6250e-04\n",
            "Epoch 40/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4893 - val_loss: 0.5659 - lr: 5.6250e-04\n",
            "Epoch 41/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4869 - val_loss: 0.5850 - lr: 5.6250e-04\n",
            "Epoch 42/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4906 - val_loss: 0.5794 - lr: 5.6250e-04\n",
            "Epoch 43/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5036 - val_loss: 0.5848 - lr: 5.6250e-04\n",
            "Epoch 44/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4876 - val_loss: 0.5653 - lr: 5.6250e-04\n",
            "Epoch 45/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4808 - val_loss: 0.5787 - lr: 5.6250e-04\n",
            "Epoch 46/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4752 - val_loss: 0.5926 - lr: 5.6250e-04\n",
            "Epoch 47/100\n",
            "1090/1094 [============================>.] - ETA: 0s - loss: 1.4615\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4613 - val_loss: 0.5867 - lr: 5.6250e-04\n",
            "Epoch 48/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4453 - val_loss: 0.5748 - lr: 4.2187e-04\n",
            "Epoch 49/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4524 - val_loss: 0.5775 - lr: 4.2187e-04\n",
            "Epoch 50/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4487 - val_loss: 0.5609 - lr: 4.2187e-04\n",
            "Epoch 51/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4419 - val_loss: 0.5685 - lr: 4.2187e-04\n",
            "Epoch 52/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4357 - val_loss: 0.5576 - lr: 4.2187e-04\n",
            "Epoch 53/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4366 - val_loss: 0.5772 - lr: 4.2187e-04\n",
            "Epoch 54/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4464 - val_loss: 0.5591 - lr: 4.2187e-04\n",
            "Epoch 55/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4284 - val_loss: 0.5823 - lr: 4.2187e-04\n",
            "Epoch 56/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4171 - val_loss: 0.5644 - lr: 4.2187e-04\n",
            "Epoch 57/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4050 - val_loss: 0.5741 - lr: 4.2187e-04\n",
            "Epoch 58/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4074 - val_loss: 0.5753 - lr: 4.2187e-04\n",
            "Epoch 59/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4170 - val_loss: 0.5569 - lr: 4.2187e-04\n",
            "Epoch 60/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4131 - val_loss: 0.5762 - lr: 4.2187e-04\n",
            "Epoch 61/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4095 - val_loss: 0.5630 - lr: 4.2187e-04\n",
            "Epoch 62/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4001 - val_loss: 0.6003 - lr: 4.2187e-04\n",
            "Epoch 63/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3982 - val_loss: 0.5630 - lr: 4.2187e-04\n",
            "Epoch 64/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4087 - val_loss: 0.5653 - lr: 4.2187e-04\n",
            "Epoch 65/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4131 - val_loss: 0.5337 - lr: 4.2187e-04\n",
            "Epoch 66/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3870 - val_loss: 0.5757 - lr: 4.2187e-04\n",
            "Epoch 67/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4015 - val_loss: 0.5622 - lr: 4.2187e-04\n",
            "Epoch 68/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4012 - val_loss: 0.5380 - lr: 4.2187e-04\n",
            "Epoch 69/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3941 - val_loss: 0.5596 - lr: 4.2187e-04\n",
            "Epoch 70/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4039 - val_loss: 0.5684 - lr: 4.2187e-04\n",
            "Epoch 71/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4127 - val_loss: 0.5580 - lr: 4.2187e-04\n",
            "Epoch 72/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3841 - val_loss: 0.5621 - lr: 4.2187e-04\n",
            "Epoch 73/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4135 - val_loss: 0.5571 - lr: 4.2187e-04\n",
            "Epoch 74/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.4101 - val_loss: 0.5668 - lr: 4.2187e-04\n",
            "Epoch 75/100\n",
            "1086/1094 [============================>.] - ETA: 0s - loss: 1.3937\n",
            "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3942 - val_loss: 0.5516 - lr: 4.2187e-04\n",
            "Epoch 76/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3849 - val_loss: 0.5415 - lr: 3.1641e-04\n",
            "Epoch 77/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3757 - val_loss: 0.5611 - lr: 3.1641e-04\n",
            "Epoch 78/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3893 - val_loss: 0.5574 - lr: 3.1641e-04\n",
            "Epoch 79/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3734 - val_loss: 0.5724 - lr: 3.1641e-04\n",
            "Epoch 80/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3887 - val_loss: 0.5453 - lr: 3.1641e-04\n",
            "Epoch 81/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3835 - val_loss: 0.5362 - lr: 3.1641e-04\n",
            "Epoch 82/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3781 - val_loss: 0.5478 - lr: 3.1641e-04\n",
            "Epoch 83/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3602 - val_loss: 0.5533 - lr: 3.1641e-04\n",
            "Epoch 84/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3895 - val_loss: 0.5521 - lr: 3.1641e-04\n",
            "Epoch 85/100\n",
            "1089/1094 [============================>.] - ETA: 0s - loss: 1.3985\n",
            "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3999 - val_loss: 0.5672 - lr: 3.1641e-04\n",
            "Epoch 86/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3821 - val_loss: 0.5456 - lr: 2.3730e-04\n",
            "Epoch 87/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3747 - val_loss: 0.5401 - lr: 2.3730e-04\n",
            "Epoch 88/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3822 - val_loss: 0.5591 - lr: 2.3730e-04\n",
            "Epoch 89/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3792 - val_loss: 0.5393 - lr: 2.3730e-04\n",
            "Epoch 90/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3764 - val_loss: 0.5501 - lr: 2.3730e-04\n",
            "Epoch 91/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3608 - val_loss: 0.5519 - lr: 2.3730e-04\n",
            "Epoch 92/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3908 - val_loss: 0.5302 - lr: 2.3730e-04\n",
            "Epoch 93/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3850 - val_loss: 0.5623 - lr: 2.3730e-04\n",
            "Epoch 94/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3749 - val_loss: 0.5640 - lr: 2.3730e-04\n",
            "Epoch 95/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3873 - val_loss: 0.5498 - lr: 2.3730e-04\n",
            "Epoch 96/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3724 - val_loss: 0.5388 - lr: 2.3730e-04\n",
            "Epoch 97/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3625 - val_loss: 0.5487 - lr: 2.3730e-04\n",
            "Epoch 98/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3692 - val_loss: 0.5387 - lr: 2.3730e-04\n",
            "Epoch 99/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3782 - val_loss: 0.5483 - lr: 2.3730e-04\n",
            "Epoch 100/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.3744 - val_loss: 0.5432 - lr: 2.3730e-04\n",
            "Mean error: 0.848402533085361\n",
            "Std of SE error: 0.6054491568544028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu625Fc7RRuY",
        "colab_type": "text"
      },
      "source": [
        "##Noise 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pR-NzR86_WT",
        "colab_type": "code",
        "outputId": "38831949-0b50-40fa-b320-49621b480fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "- Noise var: 4 \n",
        "- Buffer size: 3\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(4, return_sequences=True)(inp)\n",
        "x = Dropout(0.5)(x)\n",
        "x = GRU(3, return_sequences=True)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = GRU(2, return_sequences=False)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('\n",
        "'''\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(128, return_sequences=True)(inp)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(64, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(32, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(8, return_sequences = False)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of SE error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35000, 3, 4) (35000, 2)\n",
            "(15000, 3, 4) (15000, 2)\n",
            "Epoch 1/100\n",
            "1094/1094 [==============================] - 8s 8ms/step - loss: 31.4651 - val_loss: 13.3363 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 9.9076 - val_loss: 4.0919 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 5.8146 - val_loss: 2.0160 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 4.6384 - val_loss: 1.3839 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 4.0325 - val_loss: 1.2078 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.7681 - val_loss: 1.1246 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.6003 - val_loss: 1.0350 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.3293 - val_loss: 1.0464 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.1647 - val_loss: 0.8964 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.9430 - val_loss: 0.8557 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.7704 - val_loss: 0.8179 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.6283 - val_loss: 0.7950 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.4819 - val_loss: 0.7545 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.3853 - val_loss: 0.7505 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.2684 - val_loss: 0.6947 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.1599 - val_loss: 0.7307 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.1139 - val_loss: 0.7374 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0679 - val_loss: 0.7092 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0077 - val_loss: 0.7071 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9718 - val_loss: 0.8347 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9385 - val_loss: 0.7994 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8833 - val_loss: 0.7557 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8909 - val_loss: 0.8053 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8309 - val_loss: 0.8143 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "1091/1094 [============================>.] - ETA: 0s - loss: 1.8317\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8312 - val_loss: 0.7808 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7988 - val_loss: 0.7624 - lr: 7.5000e-04\n",
            "Epoch 27/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7946 - val_loss: 0.7921 - lr: 7.5000e-04\n",
            "Epoch 28/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8037 - val_loss: 0.7679 - lr: 7.5000e-04\n",
            "Epoch 29/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7890 - val_loss: 0.7922 - lr: 7.5000e-04\n",
            "Epoch 30/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7656 - val_loss: 0.8115 - lr: 7.5000e-04\n",
            "Epoch 31/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7482 - val_loss: 0.7526 - lr: 7.5000e-04\n",
            "Epoch 32/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7260 - val_loss: 0.7598 - lr: 7.5000e-04\n",
            "Epoch 33/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7634 - val_loss: 0.7650 - lr: 7.5000e-04\n",
            "Epoch 34/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7156 - val_loss: 0.7394 - lr: 7.5000e-04\n",
            "Epoch 35/100\n",
            "1094/1094 [==============================] - ETA: 0s - loss: 1.7392\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7392 - val_loss: 0.7471 - lr: 7.5000e-04\n",
            "Epoch 36/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7168 - val_loss: 0.7639 - lr: 5.6250e-04\n",
            "Epoch 37/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7185 - val_loss: 0.7494 - lr: 5.6250e-04\n",
            "Epoch 38/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7055 - val_loss: 0.7973 - lr: 5.6250e-04\n",
            "Epoch 39/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6992 - val_loss: 0.7463 - lr: 5.6250e-04\n",
            "Epoch 40/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6903 - val_loss: 0.7667 - lr: 5.6250e-04\n",
            "Epoch 41/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6846 - val_loss: 0.7598 - lr: 5.6250e-04\n",
            "Epoch 42/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6893 - val_loss: 0.7680 - lr: 5.6250e-04\n",
            "Epoch 43/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6912 - val_loss: 0.7650 - lr: 5.6250e-04\n",
            "Epoch 44/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6683 - val_loss: 0.7949 - lr: 5.6250e-04\n",
            "Epoch 45/100\n",
            "1086/1094 [============================>.] - ETA: 0s - loss: 1.6709\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6706 - val_loss: 0.7513 - lr: 5.6250e-04\n",
            "Epoch 46/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6599 - val_loss: 0.7625 - lr: 4.2187e-04\n",
            "Epoch 47/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6575 - val_loss: 0.7655 - lr: 4.2187e-04\n",
            "Epoch 48/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6719 - val_loss: 0.7545 - lr: 4.2187e-04\n",
            "Epoch 49/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6333 - val_loss: 0.7435 - lr: 4.2187e-04\n",
            "Epoch 50/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6774 - val_loss: 0.7619 - lr: 4.2187e-04\n",
            "Epoch 51/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6398 - val_loss: 0.7358 - lr: 4.2187e-04\n",
            "Epoch 52/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6495 - val_loss: 0.7556 - lr: 4.2187e-04\n",
            "Epoch 53/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6208 - val_loss: 0.7326 - lr: 4.2187e-04\n",
            "Epoch 54/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6389 - val_loss: 0.7555 - lr: 4.2187e-04\n",
            "Epoch 55/100\n",
            "1093/1094 [============================>.] - ETA: 0s - loss: 1.6370\n",
            "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6370 - val_loss: 0.7702 - lr: 4.2187e-04\n",
            "Epoch 56/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6304 - val_loss: 0.7558 - lr: 3.1641e-04\n",
            "Epoch 57/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6260 - val_loss: 0.7633 - lr: 3.1641e-04\n",
            "Epoch 58/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6358 - val_loss: 0.7487 - lr: 3.1641e-04\n",
            "Epoch 59/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6258 - val_loss: 0.7470 - lr: 3.1641e-04\n",
            "Epoch 60/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6291 - val_loss: 0.7551 - lr: 3.1641e-04\n",
            "Epoch 61/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6006 - val_loss: 0.7497 - lr: 3.1641e-04\n",
            "Epoch 62/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6134 - val_loss: 0.7493 - lr: 3.1641e-04\n",
            "Epoch 63/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6038 - val_loss: 0.7565 - lr: 3.1641e-04\n",
            "Epoch 64/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6019 - val_loss: 0.7408 - lr: 3.1641e-04\n",
            "Epoch 65/100\n",
            "1094/1094 [==============================] - ETA: 0s - loss: 1.5948\n",
            "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5948 - val_loss: 0.7700 - lr: 3.1641e-04\n",
            "Epoch 66/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6088 - val_loss: 0.7301 - lr: 2.3730e-04\n",
            "Epoch 67/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6034 - val_loss: 0.7566 - lr: 2.3730e-04\n",
            "Epoch 68/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6090 - val_loss: 0.7325 - lr: 2.3730e-04\n",
            "Epoch 69/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6097 - val_loss: 0.7514 - lr: 2.3730e-04\n",
            "Epoch 70/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5936 - val_loss: 0.7560 - lr: 2.3730e-04\n",
            "Epoch 71/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6017 - val_loss: 0.7694 - lr: 2.3730e-04\n",
            "Epoch 72/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5873 - val_loss: 0.7368 - lr: 2.3730e-04\n",
            "Epoch 73/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5874 - val_loss: 0.7648 - lr: 2.3730e-04\n",
            "Epoch 74/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6104 - val_loss: 0.7367 - lr: 2.3730e-04\n",
            "Epoch 75/100\n",
            "1087/1094 [============================>.] - ETA: 0s - loss: 1.6002\n",
            "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5996 - val_loss: 0.7541 - lr: 2.3730e-04\n",
            "Epoch 76/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6054 - val_loss: 0.7519 - lr: 1.7798e-04\n",
            "Epoch 77/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5913 - val_loss: 0.7378 - lr: 1.7798e-04\n",
            "Epoch 78/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5957 - val_loss: 0.7581 - lr: 1.7798e-04\n",
            "Epoch 79/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5775 - val_loss: 0.7597 - lr: 1.7798e-04\n",
            "Epoch 80/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.6043 - val_loss: 0.7437 - lr: 1.7798e-04\n",
            "Epoch 81/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5926 - val_loss: 0.7458 - lr: 1.7798e-04\n",
            "Epoch 82/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5844 - val_loss: 0.7573 - lr: 1.7798e-04\n",
            "Epoch 83/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5933 - val_loss: 0.7623 - lr: 1.7798e-04\n",
            "Epoch 84/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5857 - val_loss: 0.7527 - lr: 1.7798e-04\n",
            "Epoch 85/100\n",
            "1086/1094 [============================>.] - ETA: 0s - loss: 1.5794\n",
            "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5792 - val_loss: 0.7572 - lr: 1.7798e-04\n",
            "Epoch 86/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5809 - val_loss: 0.7554 - lr: 1.3348e-04\n",
            "Epoch 87/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5823 - val_loss: 0.7520 - lr: 1.3348e-04\n",
            "Epoch 88/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5921 - val_loss: 0.7518 - lr: 1.3348e-04\n",
            "Epoch 89/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5900 - val_loss: 0.7344 - lr: 1.3348e-04\n",
            "Epoch 90/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5813 - val_loss: 0.7424 - lr: 1.3348e-04\n",
            "Epoch 91/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5916 - val_loss: 0.7482 - lr: 1.3348e-04\n",
            "Epoch 92/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5911 - val_loss: 0.7649 - lr: 1.3348e-04\n",
            "Epoch 93/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5692 - val_loss: 0.7415 - lr: 1.3348e-04\n",
            "Epoch 94/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5954 - val_loss: 0.7473 - lr: 1.3348e-04\n",
            "Epoch 95/100\n",
            "1090/1094 [============================>.] - ETA: 0s - loss: 1.5782\n",
            "Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5782 - val_loss: 0.7611 - lr: 1.3348e-04\n",
            "Epoch 96/100\n",
            "1094/1094 [==============================] - 8s 8ms/step - loss: 1.5863 - val_loss: 0.7620 - lr: 1.0011e-04\n",
            "Epoch 97/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5898 - val_loss: 0.7536 - lr: 1.0011e-04\n",
            "Epoch 98/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5665 - val_loss: 0.7533 - lr: 1.0011e-04\n",
            "Epoch 99/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5906 - val_loss: 0.7573 - lr: 1.0011e-04\n",
            "Epoch 100/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.5746 - val_loss: 0.7514 - lr: 1.0011e-04\n",
            "Mean error: 1.0331840867581512\n",
            "Std of SE error: 0.6598535907731375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4UDcfBvzQW1",
        "colab_type": "text"
      },
      "source": [
        "##Noise 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIP0P3o4zF_S",
        "colab_type": "code",
        "outputId": "58401a50-d02a-48ed-a91d-663c07d5a9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "##Noise 6\n",
        "\n",
        "##Noise : 4\n",
        "'''\n",
        "- Noise var: 4\n",
        "- Buffer size: 3\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(4, return_sequences=True)(inp)\n",
        "x = Dropout(0.5)(x)\n",
        "x = GRU(3, return_sequences=True)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = GRU(2, return_sequences=False)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('\n",
        "'''\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(128, return_sequences=True)(inp)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(64, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(32, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(8, return_sequences = False)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of SE error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35000, 3, 4) (35000, 2)\n",
            "(15000, 3, 4) (15000, 2)\n",
            "Epoch 1/100\n",
            "1094/1094 [==============================] - 9s 8ms/step - loss: 34.2163 - val_loss: 14.5840 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 10.5947 - val_loss: 4.4286 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 6.0054 - val_loss: 2.2371 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 4.8109 - val_loss: 1.5338 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 4.3652 - val_loss: 1.3688 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 4.0863 - val_loss: 1.3390 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.8619 - val_loss: 1.3179 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.6445 - val_loss: 1.2869 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.4175 - val_loss: 1.2483 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.2056 - val_loss: 1.1795 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 3.0887 - val_loss: 1.1677 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.9044 - val_loss: 1.0790 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.7581 - val_loss: 1.0508 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.6584 - val_loss: 1.0218 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.5808 - val_loss: 1.0843 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.4597 - val_loss: 0.9874 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.3864 - val_loss: 1.0154 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.3203 - val_loss: 1.0755 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.2946 - val_loss: 1.0308 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.2106 - val_loss: 1.1306 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.1717 - val_loss: 1.0673 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.1475 - val_loss: 1.0884 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.1305 - val_loss: 1.1007 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0804 - val_loss: 1.0245 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0982 - val_loss: 1.0686 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "1087/1094 [============================>.] - ETA: 0s - loss: 2.0752\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0735 - val_loss: 1.0580 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0317 - val_loss: 1.0760 - lr: 7.5000e-04\n",
            "Epoch 28/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0475 - val_loss: 1.0662 - lr: 7.5000e-04\n",
            "Epoch 29/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0276 - val_loss: 1.0568 - lr: 7.5000e-04\n",
            "Epoch 30/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0111 - val_loss: 1.0635 - lr: 7.5000e-04\n",
            "Epoch 31/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0075 - val_loss: 1.0265 - lr: 7.5000e-04\n",
            "Epoch 32/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0015 - val_loss: 1.0649 - lr: 7.5000e-04\n",
            "Epoch 33/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9826 - val_loss: 1.0833 - lr: 7.5000e-04\n",
            "Epoch 34/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9824 - val_loss: 1.0276 - lr: 7.5000e-04\n",
            "Epoch 35/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 2.0017 - val_loss: 1.0583 - lr: 7.5000e-04\n",
            "Epoch 36/100\n",
            "1091/1094 [============================>.] - ETA: 0s - loss: 1.9666\n",
            "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9677 - val_loss: 1.0790 - lr: 7.5000e-04\n",
            "Epoch 37/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9715 - val_loss: 1.0705 - lr: 5.6250e-04\n",
            "Epoch 38/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9552 - val_loss: 1.0389 - lr: 5.6250e-04\n",
            "Epoch 39/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9379 - val_loss: 1.0275 - lr: 5.6250e-04\n",
            "Epoch 40/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9064 - val_loss: 1.0395 - lr: 5.6250e-04\n",
            "Epoch 41/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9507 - val_loss: 1.1047 - lr: 5.6250e-04\n",
            "Epoch 42/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9291 - val_loss: 1.0524 - lr: 5.6250e-04\n",
            "Epoch 43/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9241 - val_loss: 1.0592 - lr: 5.6250e-04\n",
            "Epoch 44/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9094 - val_loss: 1.0807 - lr: 5.6250e-04\n",
            "Epoch 45/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9146 - val_loss: 1.0533 - lr: 5.6250e-04\n",
            "Epoch 46/100\n",
            "1094/1094 [==============================] - ETA: 0s - loss: 1.8886\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8886 - val_loss: 1.0761 - lr: 5.6250e-04\n",
            "Epoch 47/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.9030 - val_loss: 1.1017 - lr: 4.2187e-04\n",
            "Epoch 48/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8898 - val_loss: 1.0286 - lr: 4.2187e-04\n",
            "Epoch 49/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8993 - val_loss: 1.0757 - lr: 4.2187e-04\n",
            "Epoch 50/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8818 - val_loss: 1.0567 - lr: 4.2187e-04\n",
            "Epoch 51/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8627 - val_loss: 1.1621 - lr: 4.2187e-04\n",
            "Epoch 52/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8600 - val_loss: 1.0709 - lr: 4.2187e-04\n",
            "Epoch 53/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8616 - val_loss: 1.0601 - lr: 4.2187e-04\n",
            "Epoch 54/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8581 - val_loss: 1.0377 - lr: 4.2187e-04\n",
            "Epoch 55/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8502 - val_loss: 1.0544 - lr: 4.2187e-04\n",
            "Epoch 56/100\n",
            "1087/1094 [============================>.] - ETA: 0s - loss: 1.8569\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8575 - val_loss: 1.0509 - lr: 4.2187e-04\n",
            "Epoch 57/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8482 - val_loss: 1.0618 - lr: 3.1641e-04\n",
            "Epoch 58/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8559 - val_loss: 1.0339 - lr: 3.1641e-04\n",
            "Epoch 59/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8383 - val_loss: 1.0346 - lr: 3.1641e-04\n",
            "Epoch 60/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8044 - val_loss: 1.0500 - lr: 3.1641e-04\n",
            "Epoch 61/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8191 - val_loss: 1.0416 - lr: 3.1641e-04\n",
            "Epoch 62/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8342 - val_loss: 1.0683 - lr: 3.1641e-04\n",
            "Epoch 63/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8316 - val_loss: 1.0315 - lr: 3.1641e-04\n",
            "Epoch 64/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8202 - val_loss: 1.0277 - lr: 3.1641e-04\n",
            "Epoch 65/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8308 - val_loss: 1.0480 - lr: 3.1641e-04\n",
            "Epoch 66/100\n",
            "1091/1094 [============================>.] - ETA: 0s - loss: 1.8227\n",
            "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8221 - val_loss: 1.0416 - lr: 3.1641e-04\n",
            "Epoch 67/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8212 - val_loss: 1.0402 - lr: 2.3730e-04\n",
            "Epoch 68/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7980 - val_loss: 1.0769 - lr: 2.3730e-04\n",
            "Epoch 69/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8302 - val_loss: 1.0325 - lr: 2.3730e-04\n",
            "Epoch 70/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8025 - val_loss: 1.0336 - lr: 2.3730e-04\n",
            "Epoch 71/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7976 - val_loss: 1.0362 - lr: 2.3730e-04\n",
            "Epoch 72/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8484 - val_loss: 1.0354 - lr: 2.3730e-04\n",
            "Epoch 73/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8143 - val_loss: 1.0418 - lr: 2.3730e-04\n",
            "Epoch 74/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7912 - val_loss: 1.0559 - lr: 2.3730e-04\n",
            "Epoch 75/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7921 - val_loss: 1.0359 - lr: 2.3730e-04\n",
            "Epoch 76/100\n",
            "1091/1094 [============================>.] - ETA: 0s - loss: 1.7982\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7982 - val_loss: 1.0846 - lr: 2.3730e-04\n",
            "Epoch 77/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7946 - val_loss: 1.0449 - lr: 1.7798e-04\n",
            "Epoch 78/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.8066 - val_loss: 1.0565 - lr: 1.7798e-04\n",
            "Epoch 79/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7905 - val_loss: 1.0472 - lr: 1.7798e-04\n",
            "Epoch 80/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7853 - val_loss: 1.0537 - lr: 1.7798e-04\n",
            "Epoch 81/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7950 - val_loss: 1.0210 - lr: 1.7798e-04\n",
            "Epoch 82/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7950 - val_loss: 1.0273 - lr: 1.7798e-04\n",
            "Epoch 83/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7847 - val_loss: 1.0335 - lr: 1.7798e-04\n",
            "Epoch 84/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7911 - val_loss: 1.0388 - lr: 1.7798e-04\n",
            "Epoch 85/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7779 - val_loss: 1.0705 - lr: 1.7798e-04\n",
            "Epoch 86/100\n",
            "1093/1094 [============================>.] - ETA: 0s - loss: 1.7868\n",
            "Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7864 - val_loss: 1.0538 - lr: 1.7798e-04\n",
            "Epoch 87/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7846 - val_loss: 1.0395 - lr: 1.3348e-04\n",
            "Epoch 88/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7871 - val_loss: 1.0575 - lr: 1.3348e-04\n",
            "Epoch 89/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7757 - val_loss: 1.0495 - lr: 1.3348e-04\n",
            "Epoch 90/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7892 - val_loss: 1.0382 - lr: 1.3348e-04\n",
            "Epoch 91/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7794 - val_loss: 1.0548 - lr: 1.3348e-04\n",
            "Epoch 92/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7755 - val_loss: 1.0396 - lr: 1.3348e-04\n",
            "Epoch 93/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7779 - val_loss: 1.0353 - lr: 1.3348e-04\n",
            "Epoch 94/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7692 - val_loss: 1.0327 - lr: 1.3348e-04\n",
            "Epoch 95/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7872 - val_loss: 1.0357 - lr: 1.3348e-04\n",
            "Epoch 96/100\n",
            "1091/1094 [============================>.] - ETA: 0s - loss: 1.7776\n",
            "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7774 - val_loss: 1.0510 - lr: 1.3348e-04\n",
            "Epoch 97/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7860 - val_loss: 1.0426 - lr: 1.0011e-04\n",
            "Epoch 98/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7907 - val_loss: 1.0499 - lr: 1.0011e-04\n",
            "Epoch 99/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7700 - val_loss: 1.0543 - lr: 1.0011e-04\n",
            "Epoch 100/100\n",
            "1094/1094 [==============================] - 8s 7ms/step - loss: 1.7863 - val_loss: 1.0382 - lr: 1.0011e-04\n",
            "Mean error: 1.2227232449124497\n",
            "Std of SE error: 0.7624868009808289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQS9frtzQ26K",
        "colab_type": "text"
      },
      "source": [
        "##Noise 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62k4F-zXQzwd",
        "colab_type": "code",
        "outputId": "be8e5eb9-1df8-4955-b1e5-5bfc256c1e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(128, return_sequences=True)(inp)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(64, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(32, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(8, return_sequences = False)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of SE error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35000, 3, 4) (35000, 2)\n",
            "(15000, 3, 4) (15000, 2)\n",
            "Epoch 1/100\n",
            "1094/1094 [==============================] - 12s 11ms/step - loss: 31.5577 - val_loss: 13.0252 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 9.9580 - val_loss: 4.1886 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 6.0626 - val_loss: 2.5368 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 4.9377 - val_loss: 1.7333 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 4.5365 - val_loss: 1.5927 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 4.2261 - val_loss: 1.6007 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.9397 - val_loss: 1.5193 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.7293 - val_loss: 1.4759 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.5772 - val_loss: 1.4208 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.4167 - val_loss: 1.4389 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.2630 - val_loss: 1.4259 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.1702 - val_loss: 1.3578 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.9943 - val_loss: 1.3051 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.9241 - val_loss: 1.3241 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.7904 - val_loss: 1.2692 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.6900 - val_loss: 1.2751 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.6459 - val_loss: 1.2470 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.6138 - val_loss: 1.3216 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.5500 - val_loss: 1.2942 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.4964 - val_loss: 1.3130 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.4321 - val_loss: 1.2873 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4062 - val_loss: 1.2682 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.4004 - val_loss: 1.2838 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.3769 - val_loss: 1.2676 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.3601 - val_loss: 1.2811 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.3361 - val_loss: 1.2543 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "1089/1094 [============================>.] - ETA: 0s - loss: 2.3222\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.3222 - val_loss: 1.2682 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2999 - val_loss: 1.2621 - lr: 7.5000e-04\n",
            "Epoch 29/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2981 - val_loss: 1.2717 - lr: 7.5000e-04\n",
            "Epoch 30/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2983 - val_loss: 1.2553 - lr: 7.5000e-04\n",
            "Epoch 31/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2773 - val_loss: 1.2756 - lr: 7.5000e-04\n",
            "Epoch 32/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2662 - val_loss: 1.3022 - lr: 7.5000e-04\n",
            "Epoch 33/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2698 - val_loss: 1.2784 - lr: 7.5000e-04\n",
            "Epoch 34/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2553 - val_loss: 1.2581 - lr: 7.5000e-04\n",
            "Epoch 35/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2511 - val_loss: 1.3071 - lr: 7.5000e-04\n",
            "Epoch 36/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2570 - val_loss: 1.2593 - lr: 7.5000e-04\n",
            "Epoch 37/100\n",
            "1089/1094 [============================>.] - ETA: 0s - loss: 2.2251\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2247 - val_loss: 1.2893 - lr: 7.5000e-04\n",
            "Epoch 38/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.2203 - val_loss: 1.3420 - lr: 5.6250e-04\n",
            "Epoch 39/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.2116 - val_loss: 1.3187 - lr: 5.6250e-04\n",
            "Epoch 40/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1846 - val_loss: 1.2930 - lr: 5.6250e-04\n",
            "Epoch 41/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.2216 - val_loss: 1.2615 - lr: 5.6250e-04\n",
            "Epoch 42/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.2087 - val_loss: 1.3164 - lr: 5.6250e-04\n",
            "Epoch 43/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1821 - val_loss: 1.3220 - lr: 5.6250e-04\n",
            "Epoch 44/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.1723 - val_loss: 1.3046 - lr: 5.6250e-04\n",
            "Epoch 45/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1933 - val_loss: 1.3326 - lr: 5.6250e-04\n",
            "Epoch 46/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1752 - val_loss: 1.2996 - lr: 5.6250e-04\n",
            "Epoch 47/100\n",
            "1092/1094 [============================>.] - ETA: 0s - loss: 2.1809\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1802 - val_loss: 1.3096 - lr: 5.6250e-04\n",
            "Epoch 48/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1615 - val_loss: 1.3158 - lr: 4.2187e-04\n",
            "Epoch 49/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1725 - val_loss: 1.3276 - lr: 4.2187e-04\n",
            "Epoch 50/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1688 - val_loss: 1.3280 - lr: 4.2187e-04\n",
            "Epoch 51/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1668 - val_loss: 1.3603 - lr: 4.2187e-04\n",
            "Epoch 52/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1719 - val_loss: 1.3234 - lr: 4.2187e-04\n",
            "Epoch 53/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1415 - val_loss: 1.3155 - lr: 4.2187e-04\n",
            "Epoch 54/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1807 - val_loss: 1.3425 - lr: 4.2187e-04\n",
            "Epoch 55/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1552 - val_loss: 1.3190 - lr: 4.2187e-04\n",
            "Epoch 56/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1482 - val_loss: 1.2966 - lr: 4.2187e-04\n",
            "Epoch 57/100\n",
            "1092/1094 [============================>.] - ETA: 0s - loss: 2.1362\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1361 - val_loss: 1.3273 - lr: 4.2187e-04\n",
            "Epoch 58/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1186 - val_loss: 1.2958 - lr: 3.1641e-04\n",
            "Epoch 59/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1501 - val_loss: 1.2949 - lr: 3.1641e-04\n",
            "Epoch 60/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1303 - val_loss: 1.3335 - lr: 3.1641e-04\n",
            "Epoch 61/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1369 - val_loss: 1.3125 - lr: 3.1641e-04\n",
            "Epoch 62/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1472 - val_loss: 1.3288 - lr: 3.1641e-04\n",
            "Epoch 63/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1228 - val_loss: 1.3415 - lr: 3.1641e-04\n",
            "Epoch 64/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1184 - val_loss: 1.3190 - lr: 3.1641e-04\n",
            "Epoch 65/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1136 - val_loss: 1.3481 - lr: 3.1641e-04\n",
            "Epoch 66/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1129 - val_loss: 1.3493 - lr: 3.1641e-04\n",
            "Epoch 67/100\n",
            "1090/1094 [============================>.] - ETA: 0s - loss: 2.1028\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1027 - val_loss: 1.3132 - lr: 3.1641e-04\n",
            "Epoch 68/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0881 - val_loss: 1.3147 - lr: 2.3730e-04\n",
            "Epoch 69/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1194 - val_loss: 1.3333 - lr: 2.3730e-04\n",
            "Epoch 70/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.0847 - val_loss: 1.3253 - lr: 2.3730e-04\n",
            "Epoch 71/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1141 - val_loss: 1.3074 - lr: 2.3730e-04\n",
            "Epoch 72/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1039 - val_loss: 1.3271 - lr: 2.3730e-04\n",
            "Epoch 73/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0715 - val_loss: 1.3164 - lr: 2.3730e-04\n",
            "Epoch 74/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.1033 - val_loss: 1.3105 - lr: 2.3730e-04\n",
            "Epoch 75/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0786 - val_loss: 1.3087 - lr: 2.3730e-04\n",
            "Epoch 76/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0682 - val_loss: 1.3149 - lr: 2.3730e-04\n",
            "Epoch 77/100\n",
            "1093/1094 [============================>.] - ETA: 0s - loss: 2.0943\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0942 - val_loss: 1.3070 - lr: 2.3730e-04\n",
            "Epoch 78/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0686 - val_loss: 1.3235 - lr: 1.7798e-04\n",
            "Epoch 79/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0866 - val_loss: 1.3001 - lr: 1.7798e-04\n",
            "Epoch 80/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0673 - val_loss: 1.3105 - lr: 1.7798e-04\n",
            "Epoch 81/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.0749 - val_loss: 1.3122 - lr: 1.7798e-04\n",
            "Epoch 82/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0896 - val_loss: 1.3022 - lr: 1.7798e-04\n",
            "Epoch 83/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0860 - val_loss: 1.3176 - lr: 1.7798e-04\n",
            "Epoch 84/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0739 - val_loss: 1.3155 - lr: 1.7798e-04\n",
            "Epoch 85/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0717 - val_loss: 1.3209 - lr: 1.7798e-04\n",
            "Epoch 86/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0631 - val_loss: 1.3117 - lr: 1.7798e-04\n",
            "Epoch 87/100\n",
            "1089/1094 [============================>.] - ETA: 0s - loss: 2.0653\n",
            "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0643 - val_loss: 1.3333 - lr: 1.7798e-04\n",
            "Epoch 88/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0554 - val_loss: 1.3254 - lr: 1.3348e-04\n",
            "Epoch 89/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0759 - val_loss: 1.3219 - lr: 1.3348e-04\n",
            "Epoch 90/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0503 - val_loss: 1.3543 - lr: 1.3348e-04\n",
            "Epoch 91/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.0721 - val_loss: 1.3207 - lr: 1.3348e-04\n",
            "Epoch 92/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.0679 - val_loss: 1.3271 - lr: 1.3348e-04\n",
            "Epoch 93/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.0422 - val_loss: 1.3238 - lr: 1.3348e-04\n",
            "Epoch 94/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0588 - val_loss: 1.3254 - lr: 1.3348e-04\n",
            "Epoch 95/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0461 - val_loss: 1.3173 - lr: 1.3348e-04\n",
            "Epoch 96/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.0479 - val_loss: 1.3189 - lr: 1.3348e-04\n",
            "Epoch 97/100\n",
            "1091/1094 [============================>.] - ETA: 0s - loss: 2.0395\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0391 - val_loss: 1.3212 - lr: 1.3348e-04\n",
            "Epoch 98/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.0634 - val_loss: 1.3361 - lr: 1.0011e-04\n",
            "Epoch 99/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0604 - val_loss: 1.3339 - lr: 1.0011e-04\n",
            "Epoch 100/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.0237 - val_loss: 1.3148 - lr: 1.0011e-04\n",
            "Mean error: 1.365275412516544\n",
            "Std of SE error: 0.8749616189872315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UcTyjjSVHfzU"
      },
      "source": [
        "##Noise 12\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ccb2ab22-8cf2-47f1-dc5d-788bf3268b16",
        "id": "1h5foyzXHfzW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = n_rssi_seq\n",
        "Y = n_coordinate_seq[:,-1,:]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = GRU(128, return_sequences=True)(inp)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(64, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(32, return_sequences=True)(x)\n",
        "x = Dropout(.25)(x)\n",
        "x = GRU(8, return_sequences = False)(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Dense(2, activation = 'relu')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[lr])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of SE error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35000, 3, 4) (35000, 2)\n",
            "(15000, 3, 4) (15000, 2)\n",
            "Epoch 1/100\n",
            "1094/1094 [==============================] - 12s 11ms/step - loss: 30.8973 - val_loss: 12.8916 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 10.0019 - val_loss: 4.4718 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 6.5187 - val_loss: 2.7473 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 5.2736 - val_loss: 2.0471 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 4.7834 - val_loss: 1.8838 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 4.5212 - val_loss: 1.8840 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 4.3158 - val_loss: 1.8435 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 4.0962 - val_loss: 1.7478 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.8827 - val_loss: 1.7697 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.7235 - val_loss: 1.7242 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.5359 - val_loss: 1.6932 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.4708 - val_loss: 1.7143 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.3154 - val_loss: 1.6239 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.2251 - val_loss: 1.5930 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 3.1196 - val_loss: 1.6348 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 3.0479 - val_loss: 1.6261 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 3.0104 - val_loss: 1.6397 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.9012 - val_loss: 1.6481 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.8719 - val_loss: 1.6226 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.8245 - val_loss: 1.6294 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.7681 - val_loss: 1.6722 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.7410 - val_loss: 1.6890 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.7141 - val_loss: 1.6783 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "1093/1094 [============================>.] - ETA: 0s - loss: 2.7056\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.7051 - val_loss: 1.6556 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.6755 - val_loss: 1.6169 - lr: 7.5000e-04\n",
            "Epoch 26/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.6504 - val_loss: 1.6105 - lr: 7.5000e-04\n",
            "Epoch 27/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.6399 - val_loss: 1.5806 - lr: 7.5000e-04\n",
            "Epoch 28/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5987 - val_loss: 1.6081 - lr: 7.5000e-04\n",
            "Epoch 29/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.6260 - val_loss: 1.6251 - lr: 7.5000e-04\n",
            "Epoch 30/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5983 - val_loss: 1.6129 - lr: 7.5000e-04\n",
            "Epoch 31/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.6093 - val_loss: 1.6083 - lr: 7.5000e-04\n",
            "Epoch 32/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5811 - val_loss: 1.6221 - lr: 7.5000e-04\n",
            "Epoch 33/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5659 - val_loss: 1.6075 - lr: 7.5000e-04\n",
            "Epoch 34/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5542 - val_loss: 1.6080 - lr: 7.5000e-04\n",
            "Epoch 35/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5590 - val_loss: 1.5978 - lr: 7.5000e-04\n",
            "Epoch 36/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5509 - val_loss: 1.6434 - lr: 7.5000e-04\n",
            "Epoch 37/100\n",
            "1091/1094 [============================>.] - ETA: 0s - loss: 2.5635\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5624 - val_loss: 1.6317 - lr: 7.5000e-04\n",
            "Epoch 38/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5566 - val_loss: 1.6207 - lr: 5.6250e-04\n",
            "Epoch 39/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5199 - val_loss: 1.6626 - lr: 5.6250e-04\n",
            "Epoch 40/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5161 - val_loss: 1.6597 - lr: 5.6250e-04\n",
            "Epoch 41/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5082 - val_loss: 1.6876 - lr: 5.6250e-04\n",
            "Epoch 42/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.5109 - val_loss: 1.6476 - lr: 5.6250e-04\n",
            "Epoch 43/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.4945 - val_loss: 1.6322 - lr: 5.6250e-04\n",
            "Epoch 44/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4878 - val_loss: 1.6703 - lr: 5.6250e-04\n",
            "Epoch 45/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4901 - val_loss: 1.6700 - lr: 5.6250e-04\n",
            "Epoch 46/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4813 - val_loss: 1.6581 - lr: 5.6250e-04\n",
            "Epoch 47/100\n",
            "1093/1094 [============================>.] - ETA: 0s - loss: 2.4727\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4730 - val_loss: 1.7018 - lr: 5.6250e-04\n",
            "Epoch 48/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4664 - val_loss: 1.7068 - lr: 4.2187e-04\n",
            "Epoch 49/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.4607 - val_loss: 1.6794 - lr: 4.2187e-04\n",
            "Epoch 50/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4917 - val_loss: 1.6948 - lr: 4.2187e-04\n",
            "Epoch 51/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4395 - val_loss: 1.6702 - lr: 4.2187e-04\n",
            "Epoch 52/100\n",
            "1094/1094 [==============================] - 12s 11ms/step - loss: 2.4317 - val_loss: 1.6883 - lr: 4.2187e-04\n",
            "Epoch 53/100\n",
            "1094/1094 [==============================] - 12s 11ms/step - loss: 2.4425 - val_loss: 1.6733 - lr: 4.2187e-04\n",
            "Epoch 54/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4519 - val_loss: 1.7034 - lr: 4.2187e-04\n",
            "Epoch 55/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4223 - val_loss: 1.6930 - lr: 4.2187e-04\n",
            "Epoch 56/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4281 - val_loss: 1.6957 - lr: 4.2187e-04\n",
            "Epoch 57/100\n",
            "1092/1094 [============================>.] - ETA: 0s - loss: 2.4289\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4286 - val_loss: 1.6839 - lr: 4.2187e-04\n",
            "Epoch 58/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4342 - val_loss: 1.7065 - lr: 3.1641e-04\n",
            "Epoch 59/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4049 - val_loss: 1.7129 - lr: 3.1641e-04\n",
            "Epoch 60/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4187 - val_loss: 1.6795 - lr: 3.1641e-04\n",
            "Epoch 61/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4021 - val_loss: 1.6940 - lr: 3.1641e-04\n",
            "Epoch 62/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4059 - val_loss: 1.7028 - lr: 3.1641e-04\n",
            "Epoch 63/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3980 - val_loss: 1.7135 - lr: 3.1641e-04\n",
            "Epoch 64/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4055 - val_loss: 1.7098 - lr: 3.1641e-04\n",
            "Epoch 65/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.4289 - val_loss: 1.7364 - lr: 3.1641e-04\n",
            "Epoch 66/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3947 - val_loss: 1.7272 - lr: 3.1641e-04\n",
            "Epoch 67/100\n",
            "1094/1094 [==============================] - ETA: 0s - loss: 2.3925\n",
            "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3925 - val_loss: 1.7263 - lr: 3.1641e-04\n",
            "Epoch 68/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3812 - val_loss: 1.7221 - lr: 2.3730e-04\n",
            "Epoch 69/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3753 - val_loss: 1.7001 - lr: 2.3730e-04\n",
            "Epoch 70/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3853 - val_loss: 1.6999 - lr: 2.3730e-04\n",
            "Epoch 71/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3728 - val_loss: 1.7208 - lr: 2.3730e-04\n",
            "Epoch 72/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3606 - val_loss: 1.7178 - lr: 2.3730e-04\n",
            "Epoch 73/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3637 - val_loss: 1.6994 - lr: 2.3730e-04\n",
            "Epoch 74/100\n",
            "1094/1094 [==============================] - 10s 10ms/step - loss: 2.3432 - val_loss: 1.7361 - lr: 2.3730e-04\n",
            "Epoch 75/100\n",
            "1094/1094 [==============================] - 12s 11ms/step - loss: 2.3696 - val_loss: 1.7084 - lr: 2.3730e-04\n",
            "Epoch 76/100\n",
            "1094/1094 [==============================] - 12s 11ms/step - loss: 2.3506 - val_loss: 1.7128 - lr: 2.3730e-04\n",
            "Epoch 77/100\n",
            "1090/1094 [============================>.] - ETA: 0s - loss: 2.3571\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3571 - val_loss: 1.7156 - lr: 2.3730e-04\n",
            "Epoch 78/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3704 - val_loss: 1.7264 - lr: 1.7798e-04\n",
            "Epoch 79/100\n",
            "1094/1094 [==============================] - 14s 13ms/step - loss: 2.3466 - val_loss: 1.7242 - lr: 1.7798e-04\n",
            "Epoch 80/100\n",
            "1094/1094 [==============================] - 12s 11ms/step - loss: 2.3401 - val_loss: 1.7139 - lr: 1.7798e-04\n",
            "Epoch 81/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3450 - val_loss: 1.7242 - lr: 1.7798e-04\n",
            "Epoch 82/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3367 - val_loss: 1.7122 - lr: 1.7798e-04\n",
            "Epoch 83/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3450 - val_loss: 1.7112 - lr: 1.7798e-04\n",
            "Epoch 84/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3385 - val_loss: 1.7174 - lr: 1.7798e-04\n",
            "Epoch 85/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3329 - val_loss: 1.7137 - lr: 1.7798e-04\n",
            "Epoch 86/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3489 - val_loss: 1.7155 - lr: 1.7798e-04\n",
            "Epoch 87/100\n",
            "1093/1094 [============================>.] - ETA: 0s - loss: 2.3401\n",
            "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0001334838816546835.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3402 - val_loss: 1.7190 - lr: 1.7798e-04\n",
            "Epoch 88/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3301 - val_loss: 1.7315 - lr: 1.3348e-04\n",
            "Epoch 89/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3267 - val_loss: 1.7159 - lr: 1.3348e-04\n",
            "Epoch 90/100\n",
            "1094/1094 [==============================] - 13s 12ms/step - loss: 2.3312 - val_loss: 1.7382 - lr: 1.3348e-04\n",
            "Epoch 91/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3178 - val_loss: 1.7126 - lr: 1.3348e-04\n",
            "Epoch 92/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3120 - val_loss: 1.7136 - lr: 1.3348e-04\n",
            "Epoch 93/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3246 - val_loss: 1.7219 - lr: 1.3348e-04\n",
            "Epoch 94/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3281 - val_loss: 1.7392 - lr: 1.3348e-04\n",
            "Epoch 95/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3120 - val_loss: 1.7252 - lr: 1.3348e-04\n",
            "Epoch 96/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3147 - val_loss: 1.7402 - lr: 1.3348e-04\n",
            "Epoch 97/100\n",
            "1089/1094 [============================>.] - ETA: 0s - loss: 2.3223\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.00010011290578404441.\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3230 - val_loss: 1.7476 - lr: 1.3348e-04\n",
            "Epoch 98/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3015 - val_loss: 1.7270 - lr: 1.0011e-04\n",
            "Epoch 99/100\n",
            "1094/1094 [==============================] - 11s 10ms/step - loss: 2.3057 - val_loss: 1.7350 - lr: 1.0011e-04\n",
            "Epoch 100/100\n",
            "1094/1094 [==============================] - 14s 12ms/step - loss: 2.2925 - val_loss: 1.7254 - lr: 1.0011e-04\n",
            "Mean error: 1.5507567868944354\n",
            "Std of SE error: 1.0227030722198451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFi9M3vUw-xk",
        "colab_type": "text"
      },
      "source": [
        "#Objective: Noise Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4VSPJmpxBeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8149399f-7a7c-4b69-c48f-a01d2206f2ef"
      },
      "source": [
        "for ts in range(6,15,3):\n",
        "    for noise_std in range(0,14,2):\n",
        "        print('')\n",
        "        print('--------------------------------------------------------------------------------------------------')\n",
        "        print('Noise Std: ', noise_std)\n",
        "        print('Timesteps: ', ts)\n",
        "        \n",
        "        data_dict, n_data_dict = create_data(noise_std)\n",
        "        n_rssi_data, n_coordinate_data = generate_trajectory(data_dict=data_dict, n_data_dict=n_data_dict, timesteps=ts, show_images=True)\n",
        "\n",
        "        #NN_mean_error, NN_std_error, KNN_mean_error, KNN_std_error = SISO(data_dict)\n",
        "        LSTM_mean_error, LSTM_std_error = MISO(n_rssi_seq=n_rssi_data, n_coordinate_seq=n_coordinate_data)\n",
        "\n",
        "        #print('SISO :', 'NN mean error: ', NN_mean_error, 'NN_std_error: ', NN_std_error ) #, 'KNN mean error:',KNN_mean_error,'KNN std error:',KNN_std_error)\n",
        "        print('MISO :', 'LSTM mean error:', LSTM_mean_error, 'LSTM std error:', LSTM_std_error)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Noise Std:  0\n",
            "Timesteps:  6\n",
            "Sample: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sample: 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sample: 20000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAACtCAYAAAAUNc27AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAALX0lEQVR4nO3df4xm1V3H8ffHXQilrQLaENxFwUgwS1OLmTRUjDbQJtuWdPtHaWjEUCWZf/yBpoYs+ofRpIlGU2lio5lQhEQCFkotaaJ2Q0nRpK7dAbTsbiuItiwubBusVk2Ka7/+MTd2Ou6PyXPvnPvMfd6vZDP3njn7nG9ysrOfOffce1NVSJIkbbXvGrsASZK0GAwdkiSpCUOHJElqwtAhSZKaMHRIkqQmDB2SJKmJnS0HS+L9uZIkTd/Xqup1Gxt7rXQk2ZvkS0meTbK/z2dJkqTJ+PKpGmcOHUl2AB8B3g7sAd6XZM+snydJkqatz0rHm4Bnq+q5qnoFeADYN0xZkiRpavqEjl3A8+vOj3VtkiRJ/8+WbyRNsgwsb/U4kiRpvvUJHS8Al6473921fYeqWgFWwLtXJElaZH0ur3weuCLJ5UnOBW4CHhmmLEmSNDUzr3RU1ckkvwD8JbADuLuqDg9WmSRJmpRUtbvi4eUVSZIWwmpVLW1s9DHokiSpCUOHJElqwtAhSZKaMHRIkqQmDB2SJKkJQ4ckSWrC0CFJkpowdEiSpCYMHZIkqQlDhyRJasLQIUmSmjB0SJKkJmYOHUkuTfJYkiNJDie5bcjCJEnStMz8anvgJPCBqnoiyWuB1SQHqurIQLVJkqQJmXmlo6qOV9UT3fE3gKPArqEKkyRJ09JnpeP/JLkMuBo4eIrvLQPLQ4wjSZK2r1RVvw9IXgN8FvhgVT18lr79BpMkSdvBalUtbWzsdfdKknOAjwP3nS1wSJKkxdbn7pUAHwWOVtWHhitJkiRNUZ+VjmuBnwGuS/JU9+cdA9UlSZImZuaNpFX110AGrEWSJE2YTySVJElNGDokSVIThg5JktSEoUOSJDVh6JAkSU0YOiRJUhOGDkmS1IShQ5IkNWHokCRJTQzyavsp2bNnz6b6HTlyZIsrkSRpWlzpkCRJTfQOHUl2JHkyyaeGKEiSJE3TECsdtwFHB/gcSZI0Yb1CR5LdwDuBu4YpR5IkTVXflY47gduBb52uQ5LlJIeSHOo5liRJ2sZmDh1JbgBOVNXqmfpV1UpVLVXV0qxjSZKk7a/PSse1wLuS/DPwAHBdkj8ZpCpJkjQ5M4eOqrqjqnZX1WXATcBnqurmwSqTJEmT4nM6JElSE6mqdoMl7QaTJEljWT3VXk5XOiRJUhOGDkmS1IShQ5IkNWHokCRJTRg6JElSE4YOSZLUhKFDkiQ1YeiQJElNGDokSVITO8cuYN7ceOONm+r34IMPbnElkiRNiysdkiSpiV6hI8kFSR5K8sUkR5O8eajCJEnStPS9vPJh4C+q6j1JzgXOH6AmSZI0QTOHjiTfA/wk8H6AqnoFeGWYsiRJ0tT0ubxyOfBV4I+TPJnkriSvHqguSZI0MX1Cx07gx4A/rKqrgf8E9m/slGQ5yaEkh3qMJUmStrk+oeMYcKyqDnbnD7EWQr5DVa1U1VJVLfUYS5IkbXMzh46qehF4PsmVXdP1wJFBqpIkSZPT9+6VXwTu6+5ceQ742f4lSZKkKUpVtRssaTeYJEkay+qptlX4RFJJktSEoUOSJDVh6JAkSU0YOiRJUhOGDkmS1IShQ5IkNWHokCRJTRg6JElSE4YOSZLUhKFDkiQ10ffdKzqLzT5mPskWVyJJ0rhc6ZAkSU30Ch1JfiXJ4SRPJ7k/yXlDFSZJkqZl5tCRZBfwS8BSVb0e2AHcNFRhkiRpWvpeXtkJvCrJTuB84F/6lyRJkqZo5tBRVS8Avwd8BTgO/FtVfXpjvyTLSQ4lOTR7mZIkabvrc3nlQmAfcDnw/cCrk9y8sV9VrVTVUlUtzV6mJEna7vpcXnkr8E9V9dWq+m/gYeDHhylLkiRNTZ/Q8RXgmiTnZ+0hE9cDR4cpS5IkTU2fPR0HgYeAJ4AvdJ+1MlBdkiRpYrLZJ2YOMljSbjBJkjSW1VPt5fSJpJIkqQlDhyRJasLQIUmSmjB0SJKkJgwdkiSpCUOHJElqwtAhSZKaMHRIkqQmDB2SJKmJnWMXMHWHDx/eVL+rrrpqiyuRJGlcrnRIkqQmzho6ktyd5ESSp9e1XZTkQJJnuq8Xbm2ZkiRpu9vMSsc9wN4NbfuBR6vqCuDR7lySJOm0zho6qupx4OUNzfuAe7vje4F3D1yXJEmamFk3kl5cVce74xeBi0/XMckysDzjOJIkaSJ6371SVZWkzvD9FWAF4Ez9JEnStM1698pLSS4B6L6eGK4kSZI0RbOGjkeAW7rjW4BPDlOOJEmaqs3cMns/8DngyiTHktwK/DbwtiTPAG/tziVJkk4rVe22WbinQ5KkhbBaVUsbG30iqSRJasLQIUmSmjB0SJKkJgwdkiSpCUOHJElqwtAhSZKaMHRIkqQmDB2SJKkJQ4ckSWrC0CFJkpowdEiSpCY288K3u5OcSPL0urbfTfLFJH+f5BNJLtjaMiVJ0na3mZWOe4C9G9oOAK+vqjcA/wDcMXBdkiRpYs4aOqrqceDlDW2frqqT3enfALu3oDZJkjQhQ+zp+Dngzwf4HEmSNGE7+/zlJL8OnATuO0OfZWC5zziSJGn7mzl0JHk/cANwfVXV6fpV1Qqw0v2d0/aTJEnTNlPoSLIXuB34qar6r2FLkiRJU7SZW2bvBz4HXJnkWJJbgT8AXgscSPJUkj/a4jolSdI2lzNcGRl+MC+vSJK0CFaramljo08klSRJTRg6JElSE4YOSZLUhKFDkiQ1YeiQJElNGDokSVIThg5JktSEoUOSJDVh6JAkSU0YOiRJUhO9Xm0/g68BX97Q9n1du+aL8zJ/nJP55LzMH+dkfD94qsam7145ZQHJoVM9n13jcl7mj3Myn5yX+eOczC8vr0iSpCYMHZIkqYl5CB0rYxegU3Je5o9zMp+cl/njnMyp0fd0SJKkxTAPKx2SJGkBjBo6kuxN8qUkzybZP2YtiyrJ3UlOJHl6XdtFSQ4keab7euGYNS6iJJcmeSzJkSSHk9zWtTs3I0lyXpK/TfJ33Zz8Ztd+eZKD3c+xP01y7ti1LpokO5I8meRT3blzMqdGCx1JdgAfAd4O7AHel2TPWPUssHuAvRva9gOPVtUVwKPdudo6CXygqvYA1wA/3/37cG7G803guqr6UeCNwN4k1wC/A/x+Vf0w8K/ArSPWuKhuA46uO3dO5tSYKx1vAp6tqueq6hXgAWDfiPUspKp6HHh5Q/M+4N7u+F7g3U2LElV1vKqe6I6/wdoP1F04N6OpNf/RnZ7T/SngOuChrt05aSzJbuCdwF3deXBO5taYoWMX8Py682Ndm8Z3cVUd745fBC4es5hFl+Qy4GrgIM7NqLpl/KeAE8AB4B+Br1fVya6LP8fauxO4HfhWd/69OCdzy42kOqNau73JW5xGkuQ1wMeBX66qf1//Peemvar6n6p6I7CbtdXaHxm5pIWW5AbgRFWtjl2LNqf1u1fWewG4dN357q5N43spySVVdTzJJaz9VqfGkpzDWuC4r6oe7pqdmzlQVV9P8hjwZuCCJDu736z9OdbWtcC7krwDOA/4buDDOCdza8yVjs8DV3S7jM8FbgIeGbEefdsjwC3d8S3AJ0esZSF116U/Chytqg+t+5ZzM5Ikr0tyQXf8KuBtrO21eQx4T9fNOWmoqu6oqt1VdRlr/4d8pqp+Gudkbo36cLAund4J7ADurqoPjlbMgkpyP/AW1t7K+BLwG8CfAR8DfoC1twK/t6o2bjbVFkryE8BfAV/g29eqf421fR3OzQiSvIG1TYk7WPuF7WNV9VtJfoi1jfAXAU8CN1fVN8erdDEleQvwq1V1g3Myv3wiqSRJasKNpJIkqQlDhyRJasLQIUmSmjB0SJKkJgwdkiSpCUOHJElqwtAhSZKaMHRIkqQm/hfPDcdKMdsvSgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sample: 30000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sample: 40000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "MISO : LSTM mean error: 0.6343135642366965 LSTM std error: 0.5351784095680698\n",
            "\n",
            "--------------------------------------------------------------------------------------------------\n",
            "Noise Std:  2\n",
            "Timesteps:  6\n",
            "Sample: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sample: 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAACtCAYAAAAUNc27AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAALQ0lEQVR4nO3df4hm113H8ffH3YQ0bTWJlhB3o4kYIptSGxlKakRD0sK2Xbr9o4QUK6kG5h9/RGkJW/1DFAqKUlOwKEO6JmBIXNJoQ0HtEoNRqGt3kmiz2dbEaJuNm2xLrFaFxrVf/5iLnY77Y3junXOfuc/7BcPce+bMc75w2Gc/c+6590lVIUmStNW+Y+wCJEnSYjB0SJKkJgwdkiSpCUOHJElqwtAhSZKaMHRIkqQmdrYcLIn350qSNH1frao3bGzstdKRZG+SLyZ5LsmBPq8lSZIm40tnapw5dCTZAXwceAewB3hfkj2zvp4kSZq2PisdbwGeq6rnq+pV4EFg/zBlSZKkqekTOnYBL6w7P9G1SZIk/T9bvpE0yTKwvNXjSJKk+dYndLwIXLnufHfX9m2qagVYAe9ekSRpkfW5vPI54JokVye5ELgNeGSYsiRJ0tTMvNJRVaeT/Bzw58AO4GBVHRusMkmSNCmpanfFw8srkiQthNWqWtrY6GPQJUlSE4YOSZLUhKFDkiQ1YeiQJElNGDokSVIThg5JktSEoUOSJDVh6JAkSU0YOiRJUhOGDkmS1IShQ5IkNWHokCRJTcwcOpJcmeSxJM8kOZbkziELkyRJ0zLzR9sDp4EPVtUTSV4PrCY5XFXPDFSbJEmakJlXOqrqZFU90R1/HTgO7BqqMEmSNC19Vjr+T5KrgOuBI2f42TKwPMQ4kiRp+0pV9XuB5HXAXwIfqaqHz9O332CSJGk7WK2qpY2Nve5eSXIB8Eng/vMFDkmStNj63L0S4BPA8ar66HAlSZKkKeqz0nEj8FPAzUme6r7eOVBdkiRpYmbeSFpVfw1kwFokSdKE+URSSZLUhKFDkiQ1YeiQJElNGDokSVIThg5JktSEoUOSJDVh6JAkSU0YOiRJUhOGDkmS1IShQ5IkNWHokCRJTfQOHUl2JHkyyaeHKEiSJE3TECsddwLHB3gdSZI0Yb1CR5LdwLuAe4YpR5IkTVXflY67gbuAb56tQ5LlJEeTHO05liRJ2sZmDh1J9gGnqmr1XP2qaqWqlqpqadaxJEnS9tdnpeNG4N1J/hl4ELg5yR8OUpUkSZqcVFX/F0luAj5UVfvO06//YJIkad6tnukKh8/pkCRJTQyy0rHpwVzpkCRpEbjSIUmSxmPokCRJTRg6JElSE4YOSZLUhKFDkiQ1YeiQJElNGDokSVIThg5JktSEoUOSJDVh6JAkSU0YOiRJUhO9QkeSS5I8lOQLSY4neetQhUmSpGnZ2fP3Pwb8WVW9N8mFwMUD1CRJkiZo5tCR5LuAHwc+AFBVrwKvDlOWJEmamj6XV64GvgL8QZInk9yT5LUD1SVJkiamT+jYCfwI8HtVdT3wn8CBjZ2SLCc5muRoj7EkSdI21yd0nABOVNWR7vwh1kLIt6mqlapaqqqlHmNJkqRtbubQUVUvAS8kubZrugV4ZpCqJEnS5PS9e+Xngfu7O1eeB366f0mSJGmKeoWOqnoK8LKJJEk6L59IKkmSmjB0SJKkJgwdkiSpCUOHJElqwtAhSZKaMHRIkqQmDB2SJKkJQ4ckSWrC0CFJkpowdEiSpCYMHZIkqQlDhyRJaqJX6EjyS0mOJXk6yQNJLhqqMEmSNC0zh44ku4BfAJaq6o3ADuC2oQqTJEnT0vfyyk7gNUl2AhcD/9K/JEmSNEUzh46qehH4beDLwEng36rqMxv7JVlOcjTJ0dnLlCRJ212fyyuXAvuBq4HvBV6b5P0b+1XVSlUtVdXS7GVKkqTtrs/llbcB/1RVX6mq/wYeBn50mLIkSdLU9AkdXwZuSHJxkgC3AMeHKUuSJE1Nnz0dR4CHgCeAz3evtTJQXZIkaWJSVe0GS9oNJkmSxrJ6pr2cPpFUkiQ1YeiQJElNGDokSVIThg5JktSEoUOSJDVh6JAkSU0YOiRJUhOGDkmS1IShQ5IkNbFz7AIkSfNrs0+tXvsIrvM7duzYpvpdd911m+p36NChTfUDuPXWWzfdV1vDlQ5JktTEeUNHkoNJTiV5el3bZUkOJ3m2+37p1pYpSZK2u82sdNwL7N3QdgB4tKquAR7tziVJks7qvKGjqh4HXtnQvB+4rzu+D3jPwHVJkqSJmXUj6eVVdbI7fgm4/GwdkywDyzOOI0mSJqL33StVVUnOur25qlaAFYBz9ZMkSdM2690rLye5AqD7fmq4kiRJ0hTNGjoeAW7vjm8HPjVMOZIkaao2c8vsA8BngWuTnEhyB/AbwNuTPAu8rTuXJEk6q2z2aXODDOaeDkmSFsFqVS1tbPSJpJIkqQlDhyRJasLQIUmSmjB0SJKkJgwdkiSpCUOHJElqwtAhSZKaMHRIkqQmDB2SJKkJQ4ckSWrC0CFJkprYzAe+HUxyKsnT69p+K8kXkvx9kj9OcsnWlilJkra7zax03Avs3dB2GHhjVb0J+AfgwwPXJUmSJua8oaOqHgde2dD2mao63Z3+DbB7C2qTJEkTMsSejp8B/nSA15EkSRO2s88vJ/kV4DRw/zn6LAPLfcaRJEnb38yhI8kHgH3ALVVVZ+tXVSvASvc7Z+0nSZKmbabQkWQvcBfwE1X1X8OWJEmSpmgzt8w+AHwWuDbJiSR3AL8LvB44nOSpJL+/xXVKkqRtLue4MjL8YF5ekSRpEaxW1dLGRp9IKkmSmjB0SJKkJgwdkiSpCUOHJElqwtAhSZKaMHRIkqQmDB2SJKkJQ4ckSWrC0CFJkpowdEiSpCZ6fbT9DL4KfGlD2/d07Zovzsv8cU7mk/Myf5yT8X3/mRqbfvbKGQtIjp7p+ewal/Myf5yT+eS8zB/nZH55eUWSJDVh6JAkSU3MQ+hYGbsAnZHzMn+ck/nkvMwf52ROjb6nQ5IkLYZ5WOmQJEkLYNTQkWRvki8meS7JgTFrWVRJDiY5leTpdW2XJTmc5Nnu+6Vj1riIklyZ5LEkzyQ5luTOrt25GUmSi5L8bZK/6+bk17r2q5Mc6d7H/ijJhWPXumiS7EjyZJJPd+fOyZwaLXQk2QF8HHgHsAd4X5I9Y9WzwO4F9m5oOwA8WlXXAI9252rrNPDBqtoD3AD8bPfvw7kZzzeAm6vqh4E3A3uT3AD8JvA7VfWDwL8Cd4xY46K6Ezi+7tw5mVNjrnS8BXiuqp6vqleBB4H9I9azkKrqceCVDc37gfu64/uA9zQtSlTVyap6ojv+OmtvqLtwbkZTa/6jO72g+yrgZuChrt05aSzJbuBdwD3deXBO5taYoWMX8MK68xNdm8Z3eVWd7I5fAi4fs5hFl+Qq4HrgCM7NqLpl/KeAU8Bh4B+Br1XV6a6L72Pt3Q3cBXyzO/9unJO55UZSnVOt3d7kLU4jSfI64JPAL1bVv6//mXPTXlX9T1W9GdjN2mrtD41c0kJLsg84VVWrY9eizWn92SvrvQhcue58d9em8b2c5IqqOpnkCtb+qlNjSS5gLXDcX1UPd83OzRyoqq8leQx4K3BJkp3dX9a+j7V1I/DuJO8ELgK+E/gYzsncGnOl43PANd0u4wuB24BHRqxH3/IIcHt3fDvwqRFrWUjddelPAMer6qPrfuTcjCTJG5Jc0h2/Bng7a3ttHgPe23VzThqqqg9X1e6quoq1/0P+oqp+Eudkbo36cLAund4N7AAOVtVHRitmQSV5ALiJtU9lfBn4VeBPgEPA97H2qcC3VtXGzabaQkl+DPgr4PN861r1L7O2r8O5GUGSN7G2KXEHa3+wHaqqX0/yA6xthL8MeBJ4f1V9Y7xKF1OSm4APVdU+52R++URSSZLUhBtJJUlSE4YOSZLUhKFDkiQ1YeiQJElNGDokSVIThg5JktSEoUOSJDVh6JAkSU38L7xuvixFzLOjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sample: 20000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sample: 30000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Sample: 40000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 655.2x280.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ-QNPVTCzjd",
        "colab_type": "text"
      },
      "source": [
        "#Objective 2: MIMO: RSSI seq --> Location seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is_h2rQrJCS3",
        "colab_type": "text"
      },
      "source": [
        "##seq RSSI --> seq Node coordinates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDSKYvDBC6r9",
        "colab_type": "code",
        "outputId": "8638a8e3-cd8c-4462-d3c4-59acf46d4a55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(rssi_seq, coordinate_seq, shuffle = True, test_size=0.3)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70000, 15, 4) (70000, 15, 2)\n",
            "(30000, 15, 4) (30000, 15, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNd-VrizDPGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input\n",
        "from tensorflow.python.keras.layers import LSTM\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Concatenate\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "encoder_input_shape = (15,4)\n",
        "decoder_input_shape = (15,2)\n",
        "output_dim = 2\n",
        "latent_dim = 8\n",
        "lstm_dim = 8\n",
        "\n",
        "#encoder\n",
        "encoder_inputs = Input(shape=encoder_input_shape)\n",
        "encoder = LSTM(lstm_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "states = Concatenate(axis=-1)([state_h, state_c])\n",
        "neck = Dense(latent_dim, activation=\"linear\")\n",
        "neck_outputs = neck(states)\n",
        "\n",
        "#decoder\n",
        "decode_h = Dense(lstm_dim, activation=\"relu\")\n",
        "decode_c = Dense(lstm_dim, activation=\"relu\")\n",
        "state_h_decoded =  decode_h(neck_outputs)\n",
        "state_c_decoded =  decode_c(neck_outputs)\n",
        "encoder_states = [state_h_decoded, state_c_decoded]\n",
        "\n",
        "decoder_inputs = Input(shape=decoder_input_shape)\n",
        "decoder_lstm = LSTM(lstm_dim,\n",
        "                    return_sequences=True)\n",
        "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_outputs = Dense(output_dim, activation='relu')(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "#print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlFRDB2gE7-p",
        "colab_type": "code",
        "outputId": "91896cfe-5715-4bc8-9d9c-f05987b4133d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.python.keras.callbacks import History, ReduceLROnPlateau\n",
        "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=5, min_lr=0.0000001, verbose=1, epsilon=1e-5)\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzLyZbcGFQ0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit([X_train, y_train], y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=48,\n",
        "                    shuffle=True,\n",
        "                    callbacks=[rlr],\n",
        "                    validation_data=[[X_test, y_test], y_test ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNQdCQXnFk7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict([X_test[0:1], y_test[0:1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek1MFkAjGxja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test[0:1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A71TpbhCJJM2",
        "colab_type": "text"
      },
      "source": [
        "##seq RSSI --> seq oneHotEncoded node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeqO1z1yGzgf",
        "colab_type": "code",
        "outputId": "5b18aa49-3f21-44e0-9fea-1ddfda91465a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "label_nodes = [[coordinates_to_node(c) for c in seq] for seq in coordinate_seq]\n",
        "one_hot_encoded_nodes = [np.asarray([np.eye(len(nodes)+1)[nodes.index(n)] for n in seq]) for seq in label_nodes]\n",
        "\n",
        "sos = np.eye(len(nodes)+1)[-1].reshape(1,len(nodes)+1)\n",
        "\n",
        "SOS_encoded_nodes = np.asarray([np.vstack((sos, seq)) for seq in one_hot_encoded_nodes])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(rssi_seq, SOS_encoded_nodes, shuffle = True, test_size=0.20)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 7, 4) (40000, 8, 52)\n",
            "(10000, 7, 4) (10000, 8, 52)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYVp-wPNOIWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_real = y_train[:,1:,:]\n",
        "y_test_real = y_test[:,1:,:]\n",
        "\n",
        "y_train_padded = y_train[:,:-1,:]\n",
        "y_test_padded = y_test[:,:-1,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k07nU3BnJRxM",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input\n",
        "from tensorflow.python.keras.layers import LSTM\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Concatenate\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "encoder_input_shape = (X_train.shape[1],4)\n",
        "decoder_input_shape = (None,y_train.shape[-1]) #timesteps: SOS+15 = 17 || vocab: SOS+51+EOS = 53\n",
        "output_dim = decoder_input_shape[1]\n",
        "latent_dim = 8\n",
        "lstm_dim = 8\n",
        "\n",
        "#encoder\n",
        "encoder_inputs = Input(shape=encoder_input_shape)\n",
        "encoder = LSTM(lstm_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "#decoder\n",
        "decoder_inputs = Input(shape=decoder_input_shape)\n",
        "decoder_lstm = LSTM(lstm_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs,_,_ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(output_dim, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "#print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQgiSC0WUnG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inf_encoder = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(lstm_dim,))\n",
        "decoder_state_input_c = Input(shape=(lstm_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "inf_decoder = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8d1cdc84-50d4-4903-8be5-8d05c5a70f6a",
        "id": "twmhiWZMJRx2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.python.keras.callbacks import History, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "filepath = '/content/drive/My Drive/Final_year_project/NLOS_data/seq2seq_weights.h5'\n",
        "\n",
        "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=5, min_lr=0.0000001, verbose=1, epsilon=1e-5)\n",
        "cpt = ModelCheckpoint(filepath=filepath, save_best_only=True, save_weights_only=True, monitor='loss', mode='min')\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3402ac91-ed41-4646-dbc7-1befcaea93be",
        "id": "hUfyFgKCJRyI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit([X_train, y_train_padded], y_train_real,\n",
        "                    epochs=500,\n",
        "                    batch_size=48,\n",
        "                    shuffle=True,\n",
        "                    callbacks=[rlr, cpt],\n",
        "                    validation_data=[[X_test, y_test_padded], y_test_real ])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "834/834 [==============================] - 8s 9ms/step - loss: 2.8792 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.9801 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.7605 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.6502 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.5864 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.5462 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.5166 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.4930 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.4740 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.4580 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "834/834 [==============================] - 7s 9ms/step - loss: 1.4444 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.4326 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.4227 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.4133 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.4047 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.3965 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.3887 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.3811 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.3735 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.3659 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.3589 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.3516 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.3448 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.3382 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.3317 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.3259 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.3204 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.3153 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.3106 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.3061 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.3017 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2975 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2936 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2891 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2855 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2817 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2781 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2750 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2714 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2685 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2656 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2626 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2598 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2569 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2543 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2521 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2491 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2466 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2442 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2416 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2397 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2370 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2352 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2336 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2311 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2289 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2275 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2256 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "834/834 [==============================] - 7s 9ms/step - loss: 1.2234 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2219 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2198 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2181 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2165 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2144 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2128 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2108 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.2089 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2072 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2053 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.2033 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.2014 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1990 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1976 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1955 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1940 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1921 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1901 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.1880 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1864 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1846 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1832 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1815 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1796 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1786 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1768 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1756 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1742 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1726 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1715 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1698 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1684 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1669 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1658 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1644 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1631 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1616 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1603 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1593 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1577 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1571 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.1558 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1542 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1530 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1519 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1505 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1495 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1482 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.1472 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1458 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.1446 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1435 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1424 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1411 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1398 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1387 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1380 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1366 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1357 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1343 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1333 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1321 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1309 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1303 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1291 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1281 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1271 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1263 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1249 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1241 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1231 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1219 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1214 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1203 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1191 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1189 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1175 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1168 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1161 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1152 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1142 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "834/834 [==============================] - 7s 9ms/step - loss: 1.1134 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1125 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.1119 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1108 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1102 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1101 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1087 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1078 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1074 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1066 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1061 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.1051 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1050 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1039 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1031 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1026 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1020 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1014 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1008 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.1001 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0992 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0990 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0981 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0976 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0968 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0965 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0963 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0956 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0952 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0945 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0940 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0933 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0928 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0923 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0919 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0911 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0902 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0902 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0900 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0898 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0885 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 182/500\n",
            "834/834 [==============================] - 12s 14ms/step - loss: 1.0880 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 183/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0879 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 184/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0873 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 185/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0870 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 186/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0865 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 187/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0857 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 188/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0854 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 189/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0850 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 190/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0845 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 191/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0839 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 192/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0834 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 193/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0830 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 194/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0829 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 195/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0824 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 196/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0818 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 197/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0810 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 198/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0811 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 199/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0801 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 200/500\n",
            "834/834 [==============================] - 7s 9ms/step - loss: 1.0801 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 201/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0797 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 202/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0796 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 203/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0789 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 204/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0783 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 205/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0780 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 206/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0776 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 207/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0768 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 208/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0768 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 209/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0765 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 210/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0762 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 211/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0760 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 212/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0759 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 213/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0750 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 214/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0743 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 215/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0739 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 216/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0737 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 217/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0733 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 218/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0731 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 219/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0724 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 220/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0724 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 221/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0716 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 222/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0715 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 223/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0707 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 224/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0710 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 225/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0703 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 226/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0696 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 227/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0696 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 228/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0695 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 229/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0688 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 230/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0683 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 231/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0685 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 232/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0676 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 233/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0674 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 234/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0677 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 235/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0665 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 236/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0663 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 237/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0660 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 238/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0659 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 239/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0652 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 240/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0651 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 241/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0645 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 242/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0641 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 243/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0643 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 244/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0637 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 245/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0633 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 246/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0630 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 247/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0625 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 248/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0625 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 249/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0622 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 250/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0617 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 251/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0612 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 252/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0610 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 253/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0606 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 254/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0603 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 255/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0603 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 256/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0598 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 257/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0592 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 258/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0592 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 259/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0588 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 260/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0588 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 261/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0581 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 262/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0582 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 263/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0575 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 264/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0572 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 265/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0572 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 266/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0565 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 267/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0563 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 268/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0556 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 269/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0553 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 270/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0550 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 271/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0549 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 272/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0549 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 273/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0545 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 274/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0539 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 275/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0538 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 276/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0538 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 277/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0535 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 278/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0533 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 279/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0529 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 280/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0524 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 281/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0518 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 282/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0519 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 283/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0516 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 284/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0510 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 285/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0511 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 286/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0508 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 287/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0503 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 288/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0497 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 289/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0498 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 290/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0494 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 291/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0491 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 292/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0489 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 293/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0482 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 294/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0488 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 295/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0480 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 296/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0481 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 297/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0481 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 298/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0467 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 299/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0469 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 300/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0467 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 301/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0462 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 302/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0462 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 303/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0454 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 304/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0450 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 305/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0452 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 306/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0449 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 307/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0450 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 308/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0443 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 309/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0443 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 310/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0442 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 311/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0434 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 312/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0433 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 313/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0430 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 314/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0429 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 315/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0426 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 316/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0420 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 317/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0422 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 318/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0420 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 319/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0408 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 320/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0413 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 321/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0409 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 322/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0404 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 323/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0405 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 324/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0401 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 325/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0400 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 326/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0396 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 327/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0391 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 328/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0386 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 329/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0389 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 330/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0386 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 331/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0383 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 332/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0376 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 333/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0374 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 334/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0372 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 335/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0370 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 336/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0364 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 337/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0359 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 338/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0366 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 339/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0354 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 340/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0364 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 341/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0357 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 342/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0351 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 343/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0349 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 344/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0347 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 345/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0345 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 346/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0338 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 347/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0340 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 348/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0335 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 349/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0331 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 350/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0330 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 351/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0325 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 352/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0328 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 353/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0323 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 354/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0317 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 355/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0320 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 356/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0312 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 357/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0312 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 358/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0309 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 359/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0310 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 360/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0306 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 361/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0297 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 362/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0301 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 363/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0295 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 364/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0293 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 365/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0287 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 366/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0289 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 367/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0290 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 368/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0288 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 369/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0282 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 370/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0279 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 371/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0277 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 372/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0273 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 373/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0269 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 374/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0271 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 375/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0266 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 376/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0262 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 377/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0265 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 378/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0260 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 379/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0253 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 380/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0254 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 381/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0256 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 382/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0250 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 383/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0247 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 384/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0247 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 385/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0237 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 386/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0237 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 387/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0239 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 388/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0236 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 389/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0234 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 390/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0226 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 391/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0225 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 392/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0221 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 393/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0223 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 394/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0222 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 395/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0216 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 396/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0216 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 397/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0217 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 398/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0210 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 399/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0209 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 400/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0205 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 401/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0201 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 402/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0200 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 403/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0200 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 404/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0194 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 405/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0195 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 406/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0189 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 407/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0191 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 408/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0188 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 409/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0181 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 410/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0180 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 411/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0180 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 412/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0176 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 413/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0173 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 414/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0170 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 415/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0164 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 416/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0164 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 417/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0162 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 418/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0163 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 419/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0158 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 420/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0152 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 421/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0153 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 422/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0146 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 423/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0149 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 424/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0143 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 425/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0144 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 426/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0139 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 427/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0133 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 428/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0139 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 429/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0132 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 430/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0134 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 431/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0130 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 432/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0125 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 433/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0122 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 434/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0125 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 435/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0119 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 436/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0120 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 437/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0112 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 438/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0111 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 439/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0107 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 440/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0105 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 441/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0102 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 442/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0096 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 443/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0101 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 444/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0098 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 445/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0096 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 446/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0097 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 447/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0088 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 448/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0088 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 449/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0086 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 450/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0085 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 451/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0083 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 452/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0081 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 453/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0077 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 454/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0079 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 455/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0073 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 456/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0072 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 457/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0070 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 458/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0068 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 459/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0068 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 460/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0064 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 461/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0064 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 462/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0062 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 463/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0060 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 464/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0056 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 465/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0059 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 466/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0052 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 467/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0048 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 468/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0049 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 469/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0048 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 470/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0049 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 471/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0040 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 472/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0036 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 473/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0044 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 474/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0041 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 475/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0041 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 476/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0032 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 477/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0028 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 478/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0030 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 479/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 1.0025 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 480/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0029 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 481/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0023 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 482/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0013 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 483/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0022 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 484/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0018 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 485/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0018 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 486/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0013 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 487/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0013 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 488/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0009 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 489/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0010 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 490/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0005 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 491/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0006 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 492/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 1.0003 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 493/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0003 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 494/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 0.9997 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 495/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 0.9994 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 496/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 1.0001 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 497/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 0.9995 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 498/500\n",
            "834/834 [==============================] - 7s 8ms/step - loss: 0.9992 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 499/500\n",
            "834/834 [==============================] - 6s 7ms/step - loss: 0.9988 - val_loss: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 500/500\n",
            "834/834 [==============================] - 6s 8ms/step - loss: 0.9992 - val_loss: 0.0000e+00 - lr: 0.0010\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f80007b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eePITqJKvZ8",
        "colab_type": "code",
        "outputId": "36a3355d-d32e-4c00-958c-2769955ac65c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        }
      },
      "source": [
        "for i, seq in enumerate(X_train[10:15]):\n",
        "    state = inf_encoder.predict(seq.reshape(1,seq.shape[0], seq.shape[1]))\n",
        "\n",
        "    sample_vec = sos.reshape(1, sos.shape[0], sos.shape[1])\n",
        "    output = []\n",
        "\n",
        "    for _ in range(X_train.shape[1]):\n",
        "        y_hat, h, c = inf_decoder.predict([sample_vec] + state)\n",
        "        idx = np.argmax(y_hat[0,0,:])\n",
        "        y = np.zeros((y_hat.shape[-1],))\n",
        "        y[idx] = 1\n",
        "        output.append(y)\n",
        "        state = [h,c]\n",
        "        sample_vec = y_hat\n",
        "    output = np.asarray(output)\n",
        "\n",
        "    print(i)\n",
        "    plt.imshow(y_train_real[i], cmap='gray')\n",
        "    plt.show()\n",
        "    plt.imshow(output, cmap='gray')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAABNCAYAAACPBi7ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAGL0lEQVR4nO3dTYhdZx3H8e/PaYOiQm3VIkm0FQOShUYioWIXbaAStRgXIi0KXRSyUaigSHTjC3QhiC8LN0GDXai1qNHgQg01oKuaGVvpm9VYIk2oHYoW200l9e/iniE3cWZykzvn3idzvh8I955n7r3nOU94fvPMc55zT6oKSVK7XjXvCkiS1mdQS1LjDGpJapxBLUmNM6glqXEGtSQ1bqKgTrIvyVNJTiY52HelJEnn5GLrqJMsAH8BbgNOAyeAO6vqiXXe4+JsSVec3bt3r1q+tLQ0k/1XVVYrv2qC9+4BTlbV0wBJ7gf2A2sGtSRdiRYXF1ctT1bNz5mZZOpjK/DM2PbprkySNAOTjKgnkuQAcGCjPk+SNDJJUJ8Bto9tb+vKzlNVh4BD4By1JG2kSaY+TgA7ktyYZAtwB3C032pJklZcdERdVWeTfBr4NbAAHK6qx3uvmSRNYa0VbeudGJz3ScO1XHR53mV9qFMfkubscoJ63tZanueViZLUOINakhpnUEtS4wxqSWrchl3wIkktmdVJw1mctHRELUmNM6glqXEGtSQ1zqCWpMYZ1JLUOINakhrn8jxJmsJay/DW+x6lS12654hakhpnUEtS4wxqSWqcQS1JjTOoJalxrvqQNCgbuRpjPX4pkyQNiEEtSY0zqCWpcQa1JDXOoJakxk206iPJKeBF4BXgbFW9t89KSZLOuZTlebdW1fO91USSNtAs7mU4K059SFLjJg3qAn6TZCnJgT4rJEk636RTHzdX1ZkkbwaOJflzVf1u/AVdgBvikrTBst7llKu+Ifky8FJVfX2d11zah0rSBrsS56iratXKXXTqI8lrk7x+5TnwAeCxja2eJGktk0x9XA8c6X4LXQX8sKp+1WutJGlKsxo5z2LkfslTHxN9qFMfkgZiI4P6sqc+JEnzZVBLUuMMaklqnEEtSY0zqCWpcX3dM/F54O/d8zd220M19OMH28Dj38THP+Hqjkna4G1r7qOP5Xnn7SBZHPLXog79+ME28PiHffwwfRs49SFJjTOoJalxswjqQzPYR8uGfvxgG3j8mqoNep+jliRNx6kPSWpcb0GdZF+Sp5KcTHKwr/20JMnhJMtJHhsruzbJsSR/7R7fMM869inJ9iTHkzyR5PEk93Tlg2iDJK9O8ockf+qO/ytd+Y1JHur6wo+TbJl3XfuWZCHJw0l+2W0Ppg2SnEryaJJHkix2ZVP1gV6COskC8B3gg8BO4M4kO/vYV2O+D+y7oOwg8GBV7QAe7LY3q7PAZ6tqJ3AT8Knu/30obfAysLeq3g3sAvYluQn4GvDNqnoH8C/g7jnWcVbuAZ4c2x5aG9xaVbvGluRN1Qf6GlHvAU5W1dNV9R/gfmB/T/tqRnd7sn9eULwfuK97fh/w0ZlWaoaq6tmq+mP3/EVGHXUrA2mDGnmp27y6+1fAXuAnXfmmPf4VSbYBHwa+222HgbXBKqbqA30F9VbgmbHt013ZEF1fVc92z//B6EYMm16SG4D3AA8xoDbo/uR/BFgGjgF/A16oqrPdS4bQF74FfB74b7d9HcNqg9VuBj5VH+jrEnKtoqpqCDdVSPI64KfAZ6rq3+OX2G72NqiqV4BdSa4BjgDvnHOVZirJ7cByVS0luWXe9ZmT/7sZ+PgPL6cP9DWiPgNsH9ve1pUN0XNJ3gLQPS7PuT69SnI1o5D+QVX9rCseVBsAVNULwHHgfcA1SVYGRZu9L7wf+EiSU4ymPPcC32ZAbVBVZ7rHZUa/rPcwZR/oK6hPADu6M71bgDuAoz3tq3VHgbu653cBv5hjXXrVzUV+D3iyqr4x9qNBtEGSN3UjaZK8BriN0Tz9ceBj3cs27fEDVNUXqmpbVd3AqN//tqo+wUDaYJ2bgU/VB3q74CXJhxjNVS0Ah6vq3l521JAkPwJuYfRNWc8BXwJ+DjwAvJXRNwp+vKouPOG4KSS5Gfg98Cjn5ie/yGieetO3QZJ3MTpRtMBoEPRAVX01ydsZjS6vBR4GPllVL8+vprPRTX18rqpuH0obdMd5pNtcuRn4vUmuY4o+4JWJktQ4r0yUpMYZ1JLUOINakhpnUEtS4wxqSWqcQS1JjTOoJalxBrUkNe5/kqv52NLPU/EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKxfUHJ7ITpY",
        "colab_type": "text"
      },
      "source": [
        "##Seq of RSSI --> OneHot Encoded final node vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmL2rLyuInAm",
        "colab_type": "code",
        "outputId": "e494161d-ecae-47b0-e589-99a89b734d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "coordinates = coordinate_seq[:,-1,:]\n",
        "label_nodes = [coordinates_to_node(c) for c in coordinates]\n",
        "one_hot_encoded_nodes = np.asarray([np.eye(len(nodes))[nodes.index(n)] for n in label_nodes])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(rssi_seq, one_hot_encoded_nodes, shuffle = True, test_size=0.25)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "#Standarize\n",
        "mean = np.mean(X_train)\n",
        "std = np.mean(X_test)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(75000, 15, 4) (75000, 51)\n",
            "(25000, 15, 4) (25000, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WcD7BSoSKx2S",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
        "x = LSTM(4)(inp)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(51, activation = 'softmax')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ESuf1OjpKx26",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=48, epochs=20, validation_data=(X_test, y_test), callbacks=[lr])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU-mgH_CLe41",
        "colab_type": "code",
        "outputId": "c4507cdf-cc9a-4e0b-fec6-4ad9d7a8d8e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_gru = []\n",
        "\n",
        "for e in range(y_pred.shape[0]):\n",
        "    pred_coordinates = node_to_coordinates[nodes[np.argmax(y_pred[e])]]\n",
        "    true_coordinates = node_to_coordinates[nodes[np.argmax(y_test[e])]]\n",
        "\n",
        "    error_gru.append(np.sqrt((pred_coordinates[0] - true_coordinates[0])**2 + (pred_coordinates[1] - true_coordinates[1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_gru))\n",
        "print('Std of SE error:',np.std(error_gru))\n",
        "\n",
        "plt.hist(np.asarray(error_gru), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 0.5380597512611865\n",
            "Std of SE error: 0.7662973992339723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqQJyvFxkbAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFVyQxktka--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIkDbREeka7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F13tkevzka4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkL5zVtoka1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmm4E_HJkaxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-1ZOtaYkagh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HBb6udyhT9u",
        "colab_type": "text"
      },
      "source": [
        "#Time Series prediction : Node prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxmORtt2hZdH",
        "colab_type": "code",
        "outputId": "9a97415c-3087-4509-d199-a24783b6ea46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "node_seq_data = np.asarray([[coordinates_to_node(c) for c in seq] for seq in coordinate_seq])\n",
        "node_seq_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZHqfZ6rj1DW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = node_seq_data[:,:-1]\n",
        "Y = node_seq_data[:, -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1npOlSfZkocL",
        "colab_type": "code",
        "outputId": "e9617b7d-a677-4efe-bcdd-178b47aede47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, shuffle = True, test_size=0.1)\n",
        "\n",
        "en_X_train = np.asarray([np.asarray([nodes.index(n) for n in seq]) for seq in X_train])\n",
        "en_X_test = np.asarray([np.asarray([nodes.index(n) for n in seq]) for seq in X_test])\n",
        "\n",
        "en_y_train = np.asarray([np.eye(len(nodes))[nodes.index(n)] for n in y_train])\n",
        "en_y_test = np.asarray([np.eye(len(nodes))[nodes.index(n)] for n in y_test])\n",
        "\n",
        "print(en_X_train.shape, en_y_train.shape)\n",
        "print(en_X_test.shape, en_y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 6) (45000, 51)\n",
            "(5000, 6) (5000, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L9i1mf_Pmsxh",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(X_train.shape[1],))\n",
        "x = Embedding(51, 16, input_length=X_train.shape[1])(inp)\n",
        "x = LSTM(16, return_sequences=True)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = LSTM(16)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(51, activation = 'softmax')(x)\n",
        "\n",
        "series_model = Model(inp, x)\n",
        "series_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iXPXNLOomsyG",
        "outputId": "f653caf5-8853-475a-cb6e-4b59ccdbea0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "source": [
        "history = series_model.fit(en_X_train, en_y_train, batch_size=48, epochs=20, validation_data=(en_X_test, en_y_test), callbacks=[lr])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 2.9050 - val_loss: 2.2495 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 2.1290 - val_loss: 1.9002 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.8994 - val_loss: 1.7402 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.7908 - val_loss: 1.6579 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.7344 - val_loss: 1.6174 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.7025 - val_loss: 1.5881 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 1.6869 - val_loss: 1.5766 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.6684 - val_loss: 1.5618 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.6555 - val_loss: 1.5562 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.6508 - val_loss: 1.5470 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.6385 - val_loss: 1.5417 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 1.6307 - val_loss: 1.5338 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.6272 - val_loss: 1.5357 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.6226 - val_loss: 1.5264 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 1.6174 - val_loss: 1.5204 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 1.6122 - val_loss: 1.5201 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 1.6096 - val_loss: 1.5111 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 1.6025 - val_loss: 1.5100 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.5981 - val_loss: 1.5041 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 1.5960 - val_loss: 1.5044 - lr: 0.0010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zcF4VUSm5ZW",
        "colab_type": "code",
        "outputId": "12603c54-a079-446b-b3c4-184ceca55599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "s = 60\n",
        "plt.stem(en_y_train[s], use_line_collection=True)\n",
        "plt.show()\n",
        "plt.stem(series_model.predict(en_X_train[s:s+1])[0], use_line_collection=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<StemContainer object of 3 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTbXXUM2ycBV",
        "colab_type": "code",
        "outputId": "01d128d4-e075-4432-dd22-9f03da8af51a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "'''\n",
        "rssi_X = rssi_seq[:,-1,:]\n",
        "rssi_Y = coordinate_seq[:,-1,:]\n",
        "rssi_Y_nodes = [coordinates_to_node(c) for c in rssi_Y]\n",
        "en_rssi_Y = np.asarray([np.eye(len(nodes))[nodes.index(n)] for n in rssi_Y_nodes])\n",
        "'''\n",
        "rssi_X = y_train\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 4) (45000, 51)\n",
            "(5000, 4) (5000, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P1mVhio83OdX",
        "colab": {}
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "inp = Input(shape=(4,))\n",
        "x = Dense(128, activation='relu')(inp)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(51, activation = 'softmax')(x)\n",
        "\n",
        "model = Model(inp, x)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "lr = ReduceLROnPlateau(patience = 10, monitor='val_loss', factor = 0.75, min_lr = 1e-6, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b464f47b-e29d-4b2a-fa0f-56f8dc5a58df",
        "id": "bcGog0WM3Odk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=16, epochs=50, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/50\n",
            "45000/45000 [==============================] - 13s 291us/step - loss: 1.1949 - val_loss: 0.9578\n",
            "Epoch 2/50\n",
            "45000/45000 [==============================] - 13s 288us/step - loss: 0.7778 - val_loss: 0.7183\n",
            "Epoch 3/50\n",
            "45000/45000 [==============================] - 13s 289us/step - loss: 0.7340 - val_loss: 0.7126\n",
            "Epoch 4/50\n",
            "45000/45000 [==============================] - 13s 288us/step - loss: 0.7102 - val_loss: 0.6198\n",
            "Epoch 5/50\n",
            "45000/45000 [==============================] - 13s 291us/step - loss: 0.7014 - val_loss: 0.7092\n",
            "Epoch 6/50\n",
            "45000/45000 [==============================] - 13s 288us/step - loss: 0.6847 - val_loss: 0.6509\n",
            "Epoch 7/50\n",
            "45000/45000 [==============================] - 13s 289us/step - loss: 0.6781 - val_loss: 0.6552\n",
            "Epoch 8/50\n",
            "45000/45000 [==============================] - 13s 290us/step - loss: 0.6677 - val_loss: 0.6747\n",
            "Epoch 9/50\n",
            "45000/45000 [==============================] - 13s 287us/step - loss: 0.6621 - val_loss: 0.6483\n",
            "Epoch 10/50\n",
            "45000/45000 [==============================] - 13s 285us/step - loss: 0.6476 - val_loss: 0.6192\n",
            "Epoch 11/50\n",
            "45000/45000 [==============================] - 13s 287us/step - loss: 0.6467 - val_loss: 0.6620\n",
            "Epoch 12/50\n",
            "45000/45000 [==============================] - 13s 287us/step - loss: 0.6407 - val_loss: 0.6111\n",
            "Epoch 13/50\n",
            "45000/45000 [==============================] - 14s 303us/step - loss: 0.6341 - val_loss: 0.6226\n",
            "Epoch 14/50\n",
            "45000/45000 [==============================] - 13s 296us/step - loss: 0.6285 - val_loss: 0.6375\n",
            "Epoch 15/50\n",
            "45000/45000 [==============================] - 13s 283us/step - loss: 0.6188 - val_loss: 0.6955\n",
            "Epoch 16/50\n",
            "45000/45000 [==============================] - 13s 282us/step - loss: 0.6192 - val_loss: 0.6250\n",
            "Epoch 17/50\n",
            "45000/45000 [==============================] - 13s 288us/step - loss: 0.6101 - val_loss: 0.5918\n",
            "Epoch 18/50\n",
            "45000/45000 [==============================] - 13s 286us/step - loss: 0.6056 - val_loss: 0.5976\n",
            "Epoch 19/50\n",
            "45000/45000 [==============================] - 13s 287us/step - loss: 0.6045 - val_loss: 0.5972\n",
            "Epoch 20/50\n",
            "45000/45000 [==============================] - 13s 289us/step - loss: 0.5958 - val_loss: 0.6086\n",
            "Epoch 21/50\n",
            "45000/45000 [==============================] - 13s 289us/step - loss: 0.5899 - val_loss: 0.5919\n",
            "Epoch 22/50\n",
            "10080/45000 [=====>........................] - ETA: 9s - loss: 0.5818"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-ebbb9eaa9a27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0416f86d-fa7c-4cee-bf09-105e02358e48",
        "id": "bZhovvGU3Odx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "#With added gaussian noise and categorical output\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "error_nn = []\n",
        "for e in range(y_pred.shape[0]):\n",
        "    error_nn.append(np.sqrt((y_pred[e][0] - y_test[e][0])**2 + (y_pred[e][1] - y_test[e][1])**2))\n",
        "\n",
        "print('Mean error:', np.mean(error_nn))\n",
        "print('Std of SE error:',np.std(error_nn))\n",
        "\n",
        "plt.hist(np.asarray(error_nn), bins=100)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean error: 0.006076994534169057\n",
            "Std of SE error: 0.06285789521772076\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}